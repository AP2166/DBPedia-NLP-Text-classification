{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db1ca9ca",
   "metadata": {},
   "source": [
    "## We will only test XGBoost on Word2Vec because of memory issues with the Tf-Idf matrix for XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f8d9fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "import pandas\n",
    "import pickle\n",
    "import numpy\n",
    "\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "from time import time\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1703b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'max_depth': 6,\n",
    "    'objective': 'multi:softmax',  # error evaluation for multiclass training\n",
    "    'num_class': 14,\n",
    "    # Set number of GPUs if available   \n",
    "    'tree_method': 'gpu_hist',\n",
    "    'gpu_id': 0\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42f0b5d",
   "metadata": {},
   "source": [
    "### full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9ca2c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train, _, x_train = pickle.load(open(\"dbpedia_csv/\" + \"/train_lem_preprocessed_vectorized.pickle\", \"rb\"))\n",
    "#_, _, x_valid = pickle.load(open(paths.data + \"/validation_preprocessed.pickle\", \"rb\"))\n",
    "y_test, _, x_test = pickle.load(open(\"dbpedia_csv/\" + \"/test_lem_preprocessed_vectorized.pickle\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22293110",
   "metadata": {},
   "outputs": [],
   "source": [
    "## LABELS MUST BE FROM 0 TO 13 BECAUSE OF XGBOOST IMPLEMENTATION\n",
    "\n",
    "y_test = [y-1 for y in y_test]\n",
    "y_train = [y-1 for y in y_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d28e2b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix(data=x_train, label=y_train)\n",
    "dtest = xgb.DMatrix(data=x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7046a64f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### delete data to free space\n",
    "\n",
    "import sys \n",
    "import gc\n",
    "del x_train\n",
    "del x_test\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a390836b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train time: 81.498s\n",
      "test time:  0.163s\n"
     ]
    }
   ],
   "source": [
    "t = time()\n",
    "bst = xgb.train(params, dtrain)\n",
    "\n",
    "training_time = time() - t\n",
    "print(\"train time: %0.3fs\" % training_time)\n",
    "\n",
    "\n",
    "t = time()\n",
    "pred = bst.predict(dtest)\n",
    "\n",
    "test_time = time() - t\n",
    "print(\"test time:  %0.3fs\" % test_time)\n",
    "\n",
    "# print(classification_report(y_test, pred))\n",
    "# cm = confusion_matrix(y_test, pred)\n",
    "# cm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c6a0bb9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:   0.930\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.84      0.86      5000\n",
      "           1       0.94      0.95      0.95      5000\n",
      "           2       0.83      0.80      0.81      5000\n",
      "           3       0.97      0.97      0.97      5000\n",
      "           4       0.93      0.93      0.93      5000\n",
      "           5       0.93      0.95      0.94      5000\n",
      "           6       0.92      0.92      0.92      5000\n",
      "           7       0.96      0.97      0.97      5000\n",
      "           8       0.99      0.98      0.99      5000\n",
      "           9       0.97      0.97      0.97      5000\n",
      "          10       0.97      0.97      0.97      5000\n",
      "          11       0.93      0.96      0.94      5000\n",
      "          12       0.94      0.95      0.94      5000\n",
      "          13       0.85      0.86      0.86      5000\n",
      "\n",
      "    accuracy                           0.93     70000\n",
      "   macro avg       0.93      0.93      0.93     70000\n",
      "weighted avg       0.93      0.93      0.93     70000\n",
      "\n",
      "confusion matrix:\n",
      "[[4219   45   90   21   52  175   92   15    3    7   11   61   40  169]\n",
      " [  61 4729   33   14   43   11   61   12    4    1    1    0    5   25]\n",
      " [  98   34 3983   32  109    6   24    3    0    3    6  212  109  381]\n",
      " [  10   11   58 4859   22   16    2    1    0    8    0    3    2    8]\n",
      " [  41   63  108   31 4656   27   33    3    0    1    0    1    4   32]\n",
      " [ 155    3    3    2   12 4766   28   12    0    5    0    2    6    6]\n",
      " [  84   82   31    6   45   38 4617   59   11    3    1    3    4   16]\n",
      " [   8    6    5    3    7   16   61 4869    7   10    5    1    1    1]\n",
      " [   0    5    2    2   10    2   41   44 4888    2    1    0    0    3]\n",
      " [   4    0    6    8    7    3    2   12    1 4837  114    0    1    5]\n",
      " [  16    1    5    0    0    5    4   10    0  103 4853    1    0    2]\n",
      " [  14    0  128    3    4    3    2    1    0    2    1 4787   32   23]\n",
      " [  17    2   73    8    6    5    5    1    0    5    2   36 4734  106]\n",
      " [ 125   25  264   29   41   26   21    6    0    4    4   25  113 4317]]\n"
     ]
    }
   ],
   "source": [
    "score1 = metrics.accuracy_score(y_test, pred)\n",
    "print(\"accuracy:   %0.3f\" % score1)\n",
    "\n",
    "print(metrics.classification_report(y_test, pred, target_names=[str(x) for x in range(14)]))\n",
    "\n",
    "print(\"confusion matrix:\")\n",
    "print(metrics.confusion_matrix(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ee234e",
   "metadata": {},
   "source": [
    "### half of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1aa94d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train, _, x_train = pickle.load(open(\"dbpedia_csv/\" + \"/train_lem_preprocessed_vectorized.pickle\", \"rb\"))\n",
    "#_, _, x_valid = pickle.load(open(paths.data + \"/validation_preprocessed.pickle\", \"rb\"))\n",
    "y_test, _, x_test = pickle.load(open(\"dbpedia_csv/\" + \"/test_lem_preprocessed_vectorized.pickle\", \"rb\"))\n",
    "\n",
    "y_train = y_train[slice(0, len(y_train), 2)]\n",
    "x_train = x_train[slice(0, len(x_train), 2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6dc12b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "## LABELS MUST BE FROM 0 TO 13 BECAUSE OF XGBOOST IMPLEMENTATION\n",
    "\n",
    "y_test = [y-1 for y in y_test]\n",
    "y_train = [y-1 for y in y_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f8b6e525",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix(data=x_train, label=y_train)\n",
    "dtest = xgb.DMatrix(data=x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6d07d88f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### delete data to free space\n",
    "\n",
    "import sys \n",
    "import gc\n",
    "del x_train\n",
    "del x_test\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6f96add8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train time: 45.361s\n",
      "test time:  0.188s\n"
     ]
    }
   ],
   "source": [
    "t = time()\n",
    "bst = xgb.train(params, dtrain)\n",
    "\n",
    "training_time = time() - t\n",
    "print(\"train time: %0.3fs\" % training_time)\n",
    "\n",
    "\n",
    "t = time()\n",
    "pred = bst.predict(dtest)\n",
    "\n",
    "test_time = time() - t\n",
    "print(\"test time:  %0.3fs\" % test_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1b403ce0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:   0.930\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.84      0.86      5000\n",
      "           1       0.94      0.95      0.94      5000\n",
      "           2       0.83      0.79      0.81      5000\n",
      "           3       0.97      0.97      0.97      5000\n",
      "           4       0.93      0.93      0.93      5000\n",
      "           5       0.94      0.95      0.94      5000\n",
      "           6       0.93      0.92      0.93      5000\n",
      "           7       0.96      0.98      0.97      5000\n",
      "           8       0.99      0.98      0.99      5000\n",
      "           9       0.97      0.97      0.97      5000\n",
      "          10       0.97      0.97      0.97      5000\n",
      "          11       0.93      0.96      0.95      5000\n",
      "          12       0.93      0.94      0.94      5000\n",
      "          13       0.84      0.86      0.85      5000\n",
      "\n",
      "    accuracy                           0.93     70000\n",
      "   macro avg       0.93      0.93      0.93     70000\n",
      "weighted avg       0.93      0.93      0.93     70000\n",
      "\n",
      "confusion matrix:\n",
      "[[4199   48   94   22   60  182   84   21    1    5   11   64   43  166]\n",
      " [  62 4739   26   12   43    5   66   12    3    2    1    0    6   23]\n",
      " [  89   36 3971   31   90   10   20    2    0    2    4  212  124  409]\n",
      " [  17   10   56 4846   26   14    5    3    0    7    0    3    5    8]\n",
      " [  36   72  120   26 4656   28   20    2    0    2    0    0    4   34]\n",
      " [ 153    4    6    6   14 4761   25   11    0    8    0    1    4    7]\n",
      " [  84   88   30    6   47   40 4606   61   11    1    2    1    8   15]\n",
      " [   6    6    6    2    9   13   51 4878    9    7    7    0    2    4]\n",
      " [   3    4    3    2   10    0   45   47 4880    3    1    0    0    2]\n",
      " [   4    0    8    8    4    3    6   18    0 4839  105    0    1    4]\n",
      " [  12    1    4    0    1    4    2   13    1  102 4854    2    0    4]\n",
      " [  19    0  100    5    2    2    3    0    0    2    2 4808   34   23]\n",
      " [  19    2   72    9    8    6    5    2    0    4    0   38 4715  120]\n",
      " [ 108   28  264   29   54   18   18    6    2   10    2   29  113 4319]]\n"
     ]
    }
   ],
   "source": [
    "score1 = metrics.accuracy_score(y_test, pred)\n",
    "print(\"accuracy:   %0.3f\" % score1)\n",
    "\n",
    "print(metrics.classification_report(y_test, pred, target_names=[str(x) for x in range(14)]))\n",
    "\n",
    "print(\"confusion matrix:\")\n",
    "print(metrics.confusion_matrix(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e443c121",
   "metadata": {},
   "source": [
    "### fourth of the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6b7f6a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train, _, x_train = pickle.load(open(\"dbpedia_csv/\" + \"/train_lem_preprocessed_vectorized.pickle\", \"rb\"))\n",
    "#_, _, x_valid = pickle.load(open(paths.data + \"/validation_preprocessed.pickle\", \"rb\"))\n",
    "y_test, _, x_test = pickle.load(open(\"dbpedia_csv/\" + \"/test_lem_preprocessed_vectorized.pickle\", \"rb\"))\n",
    "\n",
    "y_train = y_train[slice(0, len(y_train), 4)]\n",
    "x_train = x_train[slice(0, len(x_train), 4)]\n",
    "\n",
    "\n",
    "## LABELS MUST BE FROM 0 TO 13 BECAUSE OF XGBOOST IMPLEMENTATION\n",
    "\n",
    "y_test = [y-1 for y in y_test]\n",
    "y_train = [y-1 for y in y_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b9a976ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix(data=x_train, label=y_train)\n",
    "dtest = xgb.DMatrix(data=x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a1d615c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### delete data to free space\n",
    "\n",
    "import sys \n",
    "import gc\n",
    "del x_train\n",
    "del x_test\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2718410a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train time: 26.628s\n",
      "test time:  0.163s\n"
     ]
    }
   ],
   "source": [
    "t = time()\n",
    "bst = xgb.train(params, dtrain)\n",
    "\n",
    "training_time = time() - t\n",
    "print(\"train time: %0.3fs\" % training_time)\n",
    "\n",
    "\n",
    "t = time()\n",
    "pred = bst.predict(dtest)\n",
    "\n",
    "test_time = time() - t\n",
    "print(\"test time:  %0.3fs\" % test_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a5e157d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:   0.928\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.84      0.85      5000\n",
      "           1       0.94      0.94      0.94      5000\n",
      "           2       0.82      0.79      0.81      5000\n",
      "           3       0.97      0.97      0.97      5000\n",
      "           4       0.92      0.93      0.93      5000\n",
      "           5       0.93      0.95      0.94      5000\n",
      "           6       0.93      0.92      0.92      5000\n",
      "           7       0.96      0.97      0.97      5000\n",
      "           8       0.99      0.98      0.98      5000\n",
      "           9       0.97      0.97      0.97      5000\n",
      "          10       0.97      0.97      0.97      5000\n",
      "          11       0.93      0.96      0.94      5000\n",
      "          12       0.93      0.94      0.94      5000\n",
      "          13       0.85      0.86      0.85      5000\n",
      "\n",
      "    accuracy                           0.93     70000\n",
      "   macro avg       0.93      0.93      0.93     70000\n",
      "weighted avg       0.93      0.93      0.93     70000\n",
      "\n",
      "confusion matrix:\n",
      "[[4214   45   90   24   52  168   88   20    2    8   14   65   43  167]\n",
      " [  60 4714   39   13   51    9   69    9    3    2    3    0    6   22]\n",
      " [  95   37 3964   28  105    7   30    3    0    3    3  221  125  379]\n",
      " [  14    8   55 4849   24   23    4    1    0    7    1    3    7    4]\n",
      " [  37   66  119   34 4643   25   25    3    1    5    1    1   10   30]\n",
      " [ 157    4    6    3   17 4760   18   11    0    6    0    1    6   11]\n",
      " [  90   85   33    7   46   44 4591   64   10    6    0    1    2   21]\n",
      " [  10    7    5    2    7   16   57 4860   13    8    9    0    3    3]\n",
      " [   2    3    5    1   11    1   44   45 4879    5    1    0    0    3]\n",
      " [   7    0    9    8    6    4    1   14    0 4839  108    0    1    3]\n",
      " [  15    1    6    0    1    6    3   12    0   92 4859    1    0    4]\n",
      " [  13    0  126    3    3    5    2    1    0    3    2 4788   35   19]\n",
      " [  17    1   84    6    6    8    1    2    0    3    1   33 4724  114]\n",
      " [ 142   24  273   27   48   22   21    6    0   15    7   22  113 4280]]\n"
     ]
    }
   ],
   "source": [
    "score1 = metrics.accuracy_score(y_test, pred)\n",
    "print(\"accuracy:   %0.3f\" % score1)\n",
    "\n",
    "print(metrics.classification_report(y_test, pred, target_names=[str(x) for x in range(14)]))\n",
    "\n",
    "print(\"confusion matrix:\")\n",
    "print(metrics.confusion_matrix(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a4bad1",
   "metadata": {},
   "source": [
    "### tf-idf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033aafa7",
   "metadata": {},
   "source": [
    "### one fourth of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33f9ff91",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to extract features from training data : 2.241900 seconds\n",
      "n_samples: 93334, n_features: 215944\n",
      "Time taken to extract features from test data : 1.360001 seconds\n",
      "n_samples: 70000, n_features: 215944\n"
     ]
    },
    {
     "ename": "XGBoostError",
     "evalue": "[01:23:04] C:/buildkite-agent/builds/buildkite-windows-cpu-autoscaling-group-i-030221e36e1a46bfb-1/xgboost/xgboost-ci-windows/src/tree/updater_gpu_hist.cu:784: Exception in gpu_hist: [01:23:04] c:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-030221e36e1a46bfb-1\\xgboost\\xgboost-ci-windows\\src\\data\\../common/device_helpers.cuh:431: Memory allocation error on worker 0: bad allocation: cudaErrorMemoryAllocation: out of memory\n- Free memory: 1103390311\n- Requested memory: 2426767616\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mXGBoostError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 50>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m## TRAIN AND FIT CLASSIFIER\u001b[39;00m\n\u001b[0;32m     49\u001b[0m t \u001b[38;5;241m=\u001b[39m time()\n\u001b[1;32m---> 50\u001b[0m bst \u001b[38;5;241m=\u001b[39m \u001b[43mxgb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     52\u001b[0m training_time \u001b[38;5;241m=\u001b[39m time() \u001b[38;5;241m-\u001b[39m t\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain time: \u001b[39m\u001b[38;5;132;01m%0.3f\u001b[39;00m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m training_time)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\xgboost\\core.py:620\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    618\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[0;32m    619\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[1;32m--> 620\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\xgboost\\training.py:185\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[0;32m    183\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mbefore_iteration(bst, i, dtrain, evals):\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m \u001b[43mbst\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mafter_iteration(bst, i, dtrain, evals):\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\xgboost\\core.py:1918\u001b[0m, in \u001b[0;36mBooster.update\u001b[1;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[0;32m   1915\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_dmatrix_features(dtrain)\n\u001b[0;32m   1917\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1918\u001b[0m     \u001b[43m_check_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_LIB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mXGBoosterUpdateOneIter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1919\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43miteration\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1920\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mdtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1921\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1922\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(dtrain, output_margin\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\xgboost\\core.py:279\u001b[0m, in \u001b[0;36m_check_call\u001b[1;34m(ret)\u001b[0m\n\u001b[0;32m    268\u001b[0m \u001b[38;5;124;03m\"\"\"Check the return value of C API call\u001b[39;00m\n\u001b[0;32m    269\u001b[0m \n\u001b[0;32m    270\u001b[0m \u001b[38;5;124;03mThis function will raise exception when error occurs.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    276\u001b[0m \u001b[38;5;124;03m    return value from API calls\u001b[39;00m\n\u001b[0;32m    277\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    278\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 279\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m XGBoostError(py_str(_LIB\u001b[38;5;241m.\u001b[39mXGBGetLastError()))\n",
      "\u001b[1;31mXGBoostError\u001b[0m: [01:23:04] C:/buildkite-agent/builds/buildkite-windows-cpu-autoscaling-group-i-030221e36e1a46bfb-1/xgboost/xgboost-ci-windows/src/tree/updater_gpu_hist.cu:784: Exception in gpu_hist: [01:23:04] c:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-030221e36e1a46bfb-1\\xgboost\\xgboost-ci-windows\\src\\data\\../common/device_helpers.cuh:431: Memory allocation error on worker 0: bad allocation: cudaErrorMemoryAllocation: out of memory\n- Free memory: 1103390311\n- Requested memory: 2426767616\n\n"
     ]
    }
   ],
   "source": [
    "y_train, _, x_train = pickle.load(open(\"dbpedia_csv/\" + \"/train_preprocessed.pickle\", \"rb\"))\n",
    "#_, _, x_valid = pickle.load(open(paths.data + \"/validation_preprocessed.pickle\", \"rb\"))\n",
    "y_test, _, x_test = pickle.load(open(\"dbpedia_csv/\" + \"/test_preprocessed.pickle\", \"rb\"))\n",
    "\n",
    "\n",
    "y_train = y_train[slice(0, len(y_train), 6)]\n",
    "x_train = x_train[slice(0, len(x_train), 6)]\n",
    "\n",
    "y_train = [y-1 for y in y_train]\n",
    "y_test = [y-1 for y in y_test]\n",
    "\n",
    "\n",
    "## TFIDF VECTORIZATION\n",
    "\n",
    "t = time()  # not compulsory\n",
    "\n",
    "# loading CountVectorizer\n",
    "tf_vectorizer = TfidfVectorizer() # or term frequency\n",
    "\n",
    "x_train = tf_vectorizer.fit_transform(x_train)\n",
    "\n",
    "duration = time() - t\n",
    "print(\"Time taken to extract features from training data : %f seconds\" % (duration))\n",
    "print(\"n_samples: %d, n_features: %d\" % x_train.shape)\n",
    "\n",
    "t = time()\n",
    "x_test = tf_vectorizer.transform(x_test)\n",
    "\n",
    "duration = time() - t\n",
    "print(\"Time taken to extract features from test data : %f seconds\" % (duration))\n",
    "print(\"n_samples: %d, n_features: %d\" % x_test.shape)\n",
    "\n",
    "\n",
    "\n",
    "dtrain = xgb.DMatrix(data=x_train, label=y_train)\n",
    "dtest = xgb.DMatrix(data=x_test)\n",
    "\n",
    "\n",
    "import gc\n",
    "import sys\n",
    "del x_train\n",
    "del x_test\n",
    "del tf_vectorizer\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "## TRAIN AND FIT CLASSIFIER\n",
    "\n",
    "t = time()\n",
    "bst = xgb.train(params, dtrain)\n",
    "\n",
    "training_time = time() - t\n",
    "print(\"train time: %0.3fs\" % training_time)\n",
    "\n",
    "\n",
    "t = time()\n",
    "y_pred = bst.predict(dtest)\n",
    "\n",
    "test_time = time() - t\n",
    "print(\"test time:  %0.3fs\" % test_time)\n",
    "\n",
    "# compute the performance measures\n",
    "score1 = metrics.accuracy_score(y_test, y_pred)\n",
    "print(\"accuracy:   %0.3f\" % score1)\n",
    "\n",
    "print(metrics.classification_report(y_test, y_pred, target_names=[str(x) for x in range(14)]))\n",
    "\n",
    "print(\"confusion matrix:\")\n",
    "print(metrics.confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print('------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d24890",
   "metadata": {},
   "source": [
    "### half of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f786c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train, _, x_train = pickle.load(open(\"dbpedia_csv/\" + \"/train_preprocessed.pickle\", \"rb\"))\n",
    "#_, _, x_valid = pickle.load(open(paths.data + \"/validation_preprocessed.pickle\", \"rb\"))\n",
    "y_test, _, x_test = pickle.load(open(\"dbpedia_csv/\" + \"/test_preprocessed.pickle\", \"rb\"))\n",
    "\n",
    "\n",
    "y_train = y_train[slice(0, len(y_train), 2)]\n",
    "x_train = x_train[slice(0, len(x_train), 2)]\n",
    "\n",
    "y_train = [y-1 for y in y_train]\n",
    "y_test = [y-1 for y in y_test]\n",
    "\n",
    "\n",
    "## TFIDF VECTORIZATION\n",
    "\n",
    "t = time()  # not compulsory\n",
    "\n",
    "# loading CountVectorizer\n",
    "tf_vectorizer = TfidfVectorizer() # or term frequency\n",
    "\n",
    "x_train = tf_vectorizer.fit_transform(x_train)\n",
    "\n",
    "duration = time() - t\n",
    "print(\"Time taken to extract features from training data : %f seconds\" % (duration))\n",
    "print(\"n_samples: %d, n_features: %d\" % x_train.shape)\n",
    "\n",
    "t = time()\n",
    "x_test = tf_vectorizer.transform(x_test)\n",
    "\n",
    "duration = time() - t\n",
    "print(\"Time taken to extract features from test data : %f seconds\" % (duration))\n",
    "print(\"n_samples: %d, n_features: %d\" % x_test.shape)\n",
    "\n",
    "\n",
    "\n",
    "dtrain = xgb.DMatrix(data=x_train, label=y_train)\n",
    "dtest = xgb.DMatrix(data=x_test)\n",
    "\n",
    "\n",
    "import gc\n",
    "import sys\n",
    "del x_train\n",
    "del x_test\n",
    "del tf_vectorizer\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "## TRAIN AND FIT CLASSIFIER\n",
    "\n",
    "t = time()\n",
    "bst = xgb.train(params, dtrain)\n",
    "\n",
    "training_time = time() - t\n",
    "print(\"train time: %0.3fs\" % training_time)\n",
    "\n",
    "\n",
    "t = time()\n",
    "y_pred = bst.predict(dtest)\n",
    "\n",
    "test_time = time() - t\n",
    "print(\"test time:  %0.3fs\" % test_time)\n",
    "\n",
    "# compute the performance measures\n",
    "score1 = metrics.accuracy_score(y_test, y_pred)\n",
    "print(\"accuracy:   %0.3f\" % score1)\n",
    "\n",
    "print(metrics.classification_report(y_test, y_pred, target_names=[str(x) for x in range(14)]))\n",
    "\n",
    "print(\"confusion matrix:\")\n",
    "print(metrics.confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print('------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431c76dd",
   "metadata": {},
   "source": [
    "### full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3569dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train, _, x_train = pickle.load(open(\"dbpedia_csv/\" + \"/train_preprocessed.pickle\", \"rb\"))\n",
    "#_, _, x_valid = pickle.load(open(paths.data + \"/validation_preprocessed.pickle\", \"rb\"))\n",
    "y_test, _, x_test = pickle.load(open(\"dbpedia_csv/\" + \"/test_preprocessed.pickle\", \"rb\"))\n",
    "\n",
    "y_train = [y-1 for y in y_train]\n",
    "y_test = [y-1 for y in y_test]\n",
    "\n",
    "\n",
    "## TFIDF VECTORIZATION\n",
    "\n",
    "t = time()  # not compulsory\n",
    "\n",
    "# loading CountVectorizer\n",
    "tf_vectorizer = TfidfVectorizer() # or term frequency\n",
    "\n",
    "x_train = tf_vectorizer.fit_transform(x_train)\n",
    "\n",
    "duration = time() - t\n",
    "print(\"Time taken to extract features from training data : %f seconds\" % (duration))\n",
    "print(\"n_samples: %d, n_features: %d\" % x_train.shape)\n",
    "\n",
    "t = time()\n",
    "x_test = tf_vectorizer.transform(x_test)\n",
    "\n",
    "duration = time() - t\n",
    "print(\"Time taken to extract features from test data : %f seconds\" % (duration))\n",
    "print(\"n_samples: %d, n_features: %d\" % x_test.shape)\n",
    "\n",
    "\n",
    "\n",
    "dtrain = xgb.DMatrix(data=x_train, label=y_train)\n",
    "dtest = xgb.DMatrix(data=x_test)\n",
    "\n",
    "\n",
    "import gc\n",
    "import sys\n",
    "del x_train\n",
    "del x_test\n",
    "del tf_vectorizer\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "## TRAIN AND FIT CLASSIFIER\n",
    "\n",
    "t = time()\n",
    "bst = xgb.train(params, dtrain)\n",
    "\n",
    "training_time = time() - t\n",
    "print(\"train time: %0.3fs\" % training_time)\n",
    "\n",
    "\n",
    "t = time()\n",
    "y_pred = bst.predict(dtest)\n",
    "\n",
    "test_time = time() - t\n",
    "print(\"test time:  %0.3fs\" % test_time)\n",
    "\n",
    "# compute the performance measures\n",
    "score1 = metrics.accuracy_score(y_test, y_pred)\n",
    "print(\"accuracy:   %0.3f\" % score1)\n",
    "\n",
    "print(metrics.classification_report(y_test, y_pred, target_names=[str(x) for x in range(14)]))\n",
    "\n",
    "print(\"confusion matrix:\")\n",
    "print(metrics.confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print('------------------------------')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
