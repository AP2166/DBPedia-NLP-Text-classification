{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6d1d59c",
   "metadata": {},
   "source": [
    "### neural network implemented with word2vec only because tf-idf is a sparse matrix and we can't use it to train the nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b97f8008",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import string\n",
    "from re import sub\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# from keras.preprocessing.text import Tokenizer\n",
    "# from keras.preprocessing.sequence import pad_sequences\n",
    "# from keras.utils.np_utils import to_categorical\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "\n",
    "from keras.layers import Input\n",
    "\n",
    "import pickle\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import pandas\n",
    "import numpy\n",
    "\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "from time import time\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5b31873",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device mapping:\n",
      "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: NVIDIA GeForce GTX 1050 with Max-Q Design, pci bus id: 0000:02:00.0, compute capability: 6.1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(log_device_placement=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973f7517",
   "metadata": {},
   "source": [
    "### one fourth of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb1a569d",
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = EarlyStopping(monitor='val_loss',patience=2, min_delta=0.0001)\n",
    "\n",
    "# checkpoint = ModelCheckpoint(\"best_model_conv\", monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "reduce = ReduceLROnPlateau(monitor='val_loss', patience=2, verbose=1, factor=0.5, min_lr=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b551847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14000, 14)\n",
      "(70000, 14)\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_3 (Dense)             (None, 128)               128128    \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 32)                4128      \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 14)                462       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 132,718\n",
      "Trainable params: 132,718\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "3938/3938 [==============================] - 82s 19ms/step - loss: 0.7082 - accuracy: 0.7700 - val_loss: 0.6665 - val_accuracy: 0.9184 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "3938/3938 [==============================] - 63s 16ms/step - loss: 0.3306 - accuracy: 0.8945 - val_loss: 0.4134 - val_accuracy: 0.9284 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "3938/3938 [==============================] - 72s 18ms/step - loss: 0.2919 - accuracy: 0.9075 - val_loss: 0.3547 - val_accuracy: 0.9289 - lr: 0.0010\n",
      "Epoch 4/10\n",
      "3938/3938 [==============================] - 66s 17ms/step - loss: 0.2684 - accuracy: 0.9167 - val_loss: 0.3450 - val_accuracy: 0.9307 - lr: 0.0010\n",
      "Epoch 5/10\n",
      "3938/3938 [==============================] - 71s 18ms/step - loss: 0.2661 - accuracy: 0.9175 - val_loss: 0.3080 - val_accuracy: 0.9250 - lr: 0.0010\n",
      "Epoch 6/10\n",
      "3938/3938 [==============================] - 72s 18ms/step - loss: 0.2557 - accuracy: 0.9204 - val_loss: 0.3306 - val_accuracy: 0.9201 - lr: 0.0010\n",
      "Epoch 7/10\n",
      "3935/3938 [============================>.] - ETA: 0s - loss: 0.2396 - accuracy: 0.9265\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "3938/3938 [==============================] - 68s 17ms/step - loss: 0.2396 - accuracy: 0.9265 - val_loss: 0.3335 - val_accuracy: 0.9172 - lr: 0.0010\n",
      "train time: 496.406s\n",
      "2188/2188 [==============================] - 19s 6ms/step\n",
      "test time:  22.324s\n",
      "[[9.9280763e-01 3.7009260e-04 1.0119907e-03 ... 3.6819212e-05\n",
      "  1.2456052e-04 4.3124342e-03]\n",
      " [7.2190106e-02 9.1181295e-07 3.3047929e-02 ... 1.6238921e-03\n",
      "  7.2703087e-01 1.6514537e-01]\n",
      " [9.9681348e-01 6.1168321e-05 1.9919590e-04 ... 3.7105030e-05\n",
      "  1.6964332e-05 4.7906695e-04]\n",
      " ...\n",
      " [8.6746295e-05 2.0494208e-07 1.4306508e-02 ... 1.1727888e-04\n",
      "  9.4183677e-05 9.8538220e-01]\n",
      " [9.8416058e-07 4.5150453e-11 3.8091055e-04 ... 1.4733722e-07\n",
      "  3.4016590e-08 9.9961793e-01]\n",
      " [3.3920023e-04 2.3374250e-08 4.0654294e-02 ... 4.5716544e-04\n",
      "  6.2733947e-04 9.5791364e-01]]\n",
      "accuracy:   0.950\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.88      0.90      5000\n",
      "           1       0.94      0.97      0.96      5000\n",
      "           2       0.86      0.89      0.87      5000\n",
      "           3       0.99      0.98      0.98      5000\n",
      "           4       0.99      0.90      0.94      5000\n",
      "           5       0.98      0.95      0.96      5000\n",
      "           6       0.96      0.93      0.95      5000\n",
      "           7       0.94      0.99      0.97      5000\n",
      "           8       0.99      0.97      0.98      5000\n",
      "           9       0.98      0.98      0.98      5000\n",
      "          10       0.98      0.98      0.98      5000\n",
      "          11       0.95      0.97      0.96      5000\n",
      "          12       0.95      0.97      0.96      5000\n",
      "          13       0.87      0.93      0.90      5000\n",
      "\n",
      "    accuracy                           0.95     70000\n",
      "   macro avg       0.95      0.95      0.95     70000\n",
      "weighted avg       0.95      0.95      0.95     70000\n",
      "\n",
      "confusion matrix:\n",
      "[[4397   80   86   11   12   68   68   17    1    6    8   47   41  158]\n",
      " [  36 4865   20    0    4    0   39    5    8    0    2    1    3   17]\n",
      " [  38   14 4428    8   24    0    8    1    0    0    2  142   67  268]\n",
      " [   5    4   59 4890    8    8    1    1    0   15    0    0    1    8]\n",
      " [  27   64  243   23 4501    6   10    9   13    0    0    0   18   86]\n",
      " [ 148    4    0    4    0 4738   16   24    0    9    0    8    4   45]\n",
      " [  57   88   23    0    9   13 4641  131   13    1    0    3    9   12]\n",
      " [   1    1    2    0    0    0   17 4963   10    2    1    0    0    3]\n",
      " [   1   18    3    0    1    0   15   90 4870    0    0    1    0    1]\n",
      " [   2    0    2    3    0    0    0   21    0 4894   74    0    0    4]\n",
      " [  11    1    3    0    0    1    1    7    0   51 4922    0    0    3]\n",
      " [   5    0   75    1    0    1    0    0    0    0    0 4865   35   18]\n",
      " [   2    3   41    1    1    2    0    2    0    0    0   24 4851   73]\n",
      " [  63   11  142    8    5    0    4    1    0    0    3    9   90 4664]]\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "y_train, _, x_train = pickle.load(open(\"dbpedia_csv/\" + \"/train_vec_preprocessed_vectorized.pickle\", \"rb\"))\n",
    "#_, _, x_valid = pickle.load(open(paths.data + \"/validation_preprocessed.pickle\", \"rb\"))\n",
    "y_test, _, x_test = pickle.load(open(\"dbpedia_csv/\" + \"/test_vec_preprocessed_vectorized.pickle\", \"rb\"))\n",
    "\n",
    "\n",
    "y_train = y_train[slice(0, len(y_train), 4)]\n",
    "x_train = x_train[slice(0, len(x_train), 4)]\n",
    "\n",
    "\n",
    "x_train_temp = pd.DataFrame(x_train).to_numpy()\n",
    "y_train_temp = []\n",
    "\n",
    "\n",
    "x_valid = []\n",
    "y_valid = []\n",
    "\n",
    "x_train = []\n",
    "y_train_np = []\n",
    "\n",
    "\n",
    "for i in range(len(y_train)):\n",
    "    if i % 10 == 0:\n",
    "        y_valid.append(y_train[i])\n",
    "        x_valid.append(x_train_temp[i])\n",
    "    else:\n",
    "        y_train_np.append(y_train[i])\n",
    "        x_train.append(x_train_temp[i])\n",
    "        \n",
    "y_train = y_train_np\n",
    "x_train = np.array(x_train)\n",
    "x_valid = np.array(x_valid)\n",
    "    \n",
    "\n",
    "\n",
    "# x_test_1 = x_test[slice(0, len(x_test), 2)]\n",
    "# x_test_2 = x_test[slice(1, len(x_test), 2)]\n",
    "# y_test_1 = y_test[slice(0, len(y_test), 2)]\n",
    "# y_test_2 = y_test[slice(1, len(y_test), 2)]\n",
    "\n",
    "\n",
    "# y_train_valid = y_train[slice(0, len(y_train), 10)]\n",
    "# x_train_valid = x_train[slice(0, len(x_train), 10)]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # list1 = [ elem for i,elem in enumerate(list1) if elem % 2 != 0]\n",
    "# x_train = [e for i, e in enumerate(x_train) if i % 10 != 0]\n",
    "# y_train = [e for i, e in enumerate(y_train) if i % 10 != 0]\n",
    "\n",
    "# x_train_temp = [][]\n",
    "# y_train_temp = []\n",
    "\n",
    "# for i in range(1, 10):\n",
    "#     x_train_temp += x_train[slice(i, len(x_train), 10)]\n",
    "#     y_train_temp.append(y_train[i])\n",
    "        \n",
    "\n",
    "# print(len(x_train))\n",
    "\n",
    "\n",
    "y_train_valid_np = []\n",
    "\n",
    "for y in y_valid:\n",
    "    l = [0 for x in range(14)]\n",
    "    l[y-1] = 1\n",
    "    y_train_valid_np.append(l)\n",
    "    \n",
    "y_valid = np.asarray(y_train_valid_np)\n",
    "print(y_valid.shape)\n",
    "\n",
    "\n",
    "\n",
    "y_test_np = []\n",
    "\n",
    "for y in y_test:\n",
    "    l = [0 for x in range(14)]\n",
    "    l[y-1] = 1\n",
    "    y_test_np.append(l)\n",
    "    \n",
    "y_test = np.asarray(y_test_np)\n",
    "print(y_test.shape)\n",
    "\n",
    "\n",
    "y_train_np = []\n",
    "\n",
    "for y in y_train:\n",
    "    l = [0 for x in range(14)]\n",
    "    l[y-1] = 1\n",
    "    y_train_np.append(l)\n",
    "    \n",
    "y_train = np.asarray(y_train_np)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "PredictorScaler=MinMaxScaler()\n",
    "\n",
    "# Storing the fit object for later reference\n",
    "PredictorScalerFit=PredictorScaler.fit(x_train)\n",
    "\n",
    "\n",
    "x_train = PredictorScalerFit.transform(x_train)\n",
    "x_train_valid = PredictorScalerFit.transform(x_valid)\n",
    "x_test = PredictorScalerFit.transform(x_test)\n",
    "\n",
    "\n",
    "del y_train_np\n",
    "del y_test_np\n",
    "del y_train_valid_np\n",
    "del x_train_temp\n",
    "del PredictorScalerFit\n",
    "del PredictorScaler\n",
    "import sys\n",
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "## TRAIN AND FIT CLASSIFIER\n",
    "\n",
    "t = time()\n",
    "\n",
    "model_l1 = Sequential()\n",
    "# model_l1.add(Embedding(56000, 1000))\n",
    "model_l1.add(Input(shape=(1000,), name='input'))\n",
    "# model_l1.add(Conv1D(128, 5, activation='relu'))\n",
    "# model_l1.add(GlobalMaxPooling1D()) # added\n",
    "model_l1.add(Dense(128, activation='relu'))\n",
    "model_l1.add(Dropout(0.1))\n",
    "model_l1.add(Dense(32, activation='relu'))\n",
    "model_l1.add(Dense(14, activation='softmax')) \n",
    "model_l1.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model_l1.summary()\n",
    "\n",
    "history_l1 = model_l1.fit(x_train, y_train, epochs=10, verbose=1, shuffle=True, use_multiprocessing=True, \n",
    "                   validation_data=(x_valid, y_valid), batch_size=32\n",
    "                   ,callbacks=[reduce,callback])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "training_time = time() - t\n",
    "print(\"train time: %0.3fs\" % training_time)\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "t = time()\n",
    "y_pred = model_l1.predict(x_test)\n",
    "\n",
    "test_time = time() - t\n",
    "print(\"test time:  %0.3fs\" % test_time)\n",
    "\n",
    "print(y_pred)\n",
    "\n",
    "y_pred=np.argmax(y_pred, axis=1)\n",
    "y_test=np.argmax(y_test, axis=1)\n",
    "# cm = confusion_matrix(y_test, y_pred)\n",
    "# print(cm)\n",
    "\n",
    "# compute the performance measures\n",
    "score1 = metrics.accuracy_score(y_test, y_pred)\n",
    "print(\"accuracy:   %0.3f\" % score1)\n",
    "\n",
    "print(metrics.classification_report(y_test, y_pred, target_names=[str(x) for x in range(14)]))\n",
    "\n",
    "print(\"confusion matrix:\")\n",
    "print(metrics.confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print('------------------------------')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05308b18",
   "metadata": {},
   "source": [
    "### half of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c5c2242",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70000, 14)\n",
      "(252000, 14)\n",
      "(28000, 14)\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_9 (Dense)             (None, 128)               128128    \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 32)                4128      \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 14)                462       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 132,718\n",
      "Trainable params: 132,718\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "7875/7875 [==============================] - 130s 16ms/step - loss: 0.6784 - accuracy: 0.7777 - val_loss: 0.2716 - val_accuracy: 0.9170 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "7875/7875 [==============================] - 124s 16ms/step - loss: 0.3245 - accuracy: 0.8967 - val_loss: 0.2290 - val_accuracy: 0.9279 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "7875/7875 [==============================] - 123s 16ms/step - loss: 0.2891 - accuracy: 0.9090 - val_loss: 0.2261 - val_accuracy: 0.9289 - lr: 0.0010\n",
      "Epoch 4/10\n",
      "7875/7875 [==============================] - 116s 15ms/step - loss: 0.2833 - accuracy: 0.9109 - val_loss: 0.2087 - val_accuracy: 0.9349 - lr: 0.0010\n",
      "Epoch 5/10\n",
      "7875/7875 [==============================] - 123s 16ms/step - loss: 0.2677 - accuracy: 0.9164 - val_loss: 0.1745 - val_accuracy: 0.9468 - lr: 0.0010\n",
      "Epoch 6/10\n",
      "7875/7875 [==============================] - 123s 16ms/step - loss: 0.2595 - accuracy: 0.9194 - val_loss: 0.1655 - val_accuracy: 0.9502 - lr: 0.0010\n",
      "Epoch 7/10\n",
      "7875/7875 [==============================] - 120s 15ms/step - loss: 0.2429 - accuracy: 0.9253 - val_loss: 0.1869 - val_accuracy: 0.9428 - lr: 0.0010\n",
      "Epoch 8/10\n",
      "7873/7875 [============================>.] - ETA: 0s - loss: 0.2346 - accuracy: 0.9275\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "7875/7875 [==============================] - 122s 16ms/step - loss: 0.2346 - accuracy: 0.9275 - val_loss: 0.1657 - val_accuracy: 0.9510 - lr: 0.0010\n",
      "train time: 984.861s\n",
      "2188/2188 [==============================] - 22s 7ms/step\n",
      "test time:  25.833s\n",
      "accuracy:   0.951\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.85      0.89      5000\n",
      "           1       0.99      0.89      0.94      5000\n",
      "           2       0.87      0.89      0.88      5000\n",
      "           3       0.99      0.98      0.99      5000\n",
      "           4       0.93      0.97      0.95      5000\n",
      "           5       0.95      0.99      0.97      5000\n",
      "           6       0.91      0.96      0.93      5000\n",
      "           7       0.98      0.98      0.98      5000\n",
      "           8       0.99      0.98      0.98      5000\n",
      "           9       0.98      0.98      0.98      5000\n",
      "          10       0.97      0.99      0.98      5000\n",
      "          11       0.95      0.98      0.96      5000\n",
      "          12       0.95      0.97      0.96      5000\n",
      "          13       0.92      0.90      0.91      5000\n",
      "\n",
      "    accuracy                           0.95     70000\n",
      "   macro avg       0.95      0.95      0.95     70000\n",
      "weighted avg       0.95      0.95      0.95     70000\n",
      "\n",
      "confusion matrix:\n",
      "[[4243   22  110   11   45  175  110   10    1   10   15   54   43  151]\n",
      " [  91 4474   32    7  101    1  236    5   38    1    3    0    1   10]\n",
      " [  22    1 4470   17   77    4   10    0    0    7    4  175   65  148]\n",
      " [   0    1   37 4913   19   12    1    1    0   12    1    0    2    1]\n",
      " [  15    1   98    8 4840    8    8    1    0    9    2    0    2    8]\n",
      " [  28    0    1    1    3 4933   18    2    0    6    1    0    1    6]\n",
      " [  51   24   14    0   37   33 4778   41    8    3    2    0    3    6]\n",
      " [   0    0    1    0    7    2   42 4906   12   12   17    0    0    1]\n",
      " [   1    0    1    0   21    0   28   39 4906    2    0    1    0    1]\n",
      " [   0    0    1    3    0    0    0    1    0 4908   86    0    0    1]\n",
      " [  10    0    1    0    0    2    1    0    0   46 4939    0    0    1]\n",
      " [   1    0   56    2    0    2    2    0    0    0    1 4901   30    5]\n",
      " [   2    0   40    4    7    7    0    0    0    4    1   27 4847   61]\n",
      " [  45    2  249    5   28   12    7    2    0    7    4   24  113 4502]]\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "y_train, _, x_train = pickle.load(open(\"dbpedia_csv/\" + \"/train_vec_preprocessed_vectorized.pickle\", \"rb\"))\n",
    "#_, _, x_valid = pickle.load(open(paths.data + \"/validation_preprocessed.pickle\", \"rb\"))\n",
    "y_test, _, x_test = pickle.load(open(\"dbpedia_csv/\" + \"/test_vec_preprocessed_vectorized.pickle\", \"rb\"))\n",
    "\n",
    "\n",
    "y_train = y_train[slice(0, len(y_train), 2)]\n",
    "x_train = x_train[slice(0, len(x_train), 2)]\n",
    "\n",
    "\n",
    "x_train_temp = pd.DataFrame(x_train).to_numpy()\n",
    "y_train_temp = []\n",
    "\n",
    "\n",
    "x_valid = []\n",
    "y_valid = []\n",
    "\n",
    "x_train = []\n",
    "y_train_np = []\n",
    "\n",
    "\n",
    "for i in range(len(y_train)):\n",
    "    if i % 10 == 0:\n",
    "        y_valid.append(y_train[i])\n",
    "        x_valid.append(x_train_temp[i])\n",
    "    else:\n",
    "        y_train_np.append(y_train[i])\n",
    "        x_train.append(x_train_temp[i])\n",
    "        \n",
    "y_train = y_train_np\n",
    "x_train = np.array(x_train)\n",
    "x_valid = np.array(x_valid)\n",
    "    \n",
    "\n",
    "\n",
    "y_test_np = []\n",
    "\n",
    "for y in y_test:\n",
    "    l = [0 for x in range(14)]\n",
    "    l[y-1] = 1\n",
    "    y_test_np.append(l)\n",
    "    \n",
    "y_test = np.asarray(y_test_np)\n",
    "print(y_test.shape)\n",
    "\n",
    "y_train_np = []\n",
    "\n",
    "for y in y_train:\n",
    "    l = [0 for x in range(14)]\n",
    "    l[y-1] = 1\n",
    "    y_train_np.append(l)\n",
    "    \n",
    "y_train = np.asarray(y_train_np)\n",
    "print(y_train.shape)\n",
    "\n",
    "\n",
    "\n",
    "y_train_valid_np = []\n",
    "\n",
    "for y in y_valid:\n",
    "    l = [0 for x in range(14)]\n",
    "    l[y-1] = 1\n",
    "    y_train_valid_np.append(l)\n",
    "    \n",
    "y_valid = np.asarray(y_train_valid_np)\n",
    "print(y_valid.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "PredictorScaler=MinMaxScaler()\n",
    "\n",
    "# Storing the fit object for later reference\n",
    "PredictorScalerFit=PredictorScaler.fit(x_train)\n",
    "\n",
    "\n",
    "x_train = PredictorScalerFit.transform(x_train)\n",
    "x_test = PredictorScalerFit.transform(x_test)\n",
    "x_valid = PredictorScalerFit.transform(x_valid)\n",
    "\n",
    "\n",
    "del y_train_np\n",
    "del y_test_np\n",
    "del y_train_valid_np\n",
    "del x_train_temp\n",
    "del PredictorScalerFit\n",
    "del PredictorScaler\n",
    "import sys\n",
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "## TRAIN AND FIT CLASSIFIER\n",
    "\n",
    "t = time()\n",
    "\n",
    "model_l1 = Sequential()\n",
    "# model_l1.add(Embedding(56000, 1000))\n",
    "model_l1.add(Input(shape=(1000,), name='input'))\n",
    "# model_l1.add(Conv1D(128, 5, activation='relu'))\n",
    "# model_l1.add(GlobalMaxPooling1D()) # added\n",
    "model_l1.add(Dense(128, activation='relu'))\n",
    "model_l1.add(Dropout(0.1))\n",
    "model_l1.add(Dense(32, activation='relu'))\n",
    "model_l1.add(Dense(14, activation='softmax')) \n",
    "model_l1.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model_l1.summary()\n",
    "\n",
    "history_l1 = model_l1.fit(x_train, y_train, epochs=10, verbose=1, use_multiprocessing=True,\n",
    "                   validation_data=(x_valid, y_valid),\n",
    "                    batch_size=32\n",
    "                   ,callbacks=[reduce,callback])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "training_time = time() - t\n",
    "print(\"train time: %0.3fs\" % training_time)\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "t = time()\n",
    "y_pred = model_l1.predict(x_test)\n",
    "\n",
    "\n",
    "\n",
    "test_time = time() - t\n",
    "print(\"test time:  %0.3fs\" % test_time)\n",
    "\n",
    "y_pred=np.argmax(y_pred, axis=1)\n",
    "y_test=np.argmax(y_test, axis=1)\n",
    "# cm = confusion_matrix(y_test, y_pred)\n",
    "# print(cm)\n",
    "\n",
    "# compute the performance measures\n",
    "score1 = metrics.accuracy_score(y_test, y_pred)\n",
    "print(\"accuracy:   %0.3f\" % score1)\n",
    "\n",
    "print(metrics.classification_report(y_test, y_pred, target_names=[str(x) for x in range(14)]))\n",
    "\n",
    "print(\"confusion matrix:\")\n",
    "print(metrics.confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print('------------------------------')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b541e80e",
   "metadata": {},
   "source": [
    "### full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7861f11d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70000, 14)\n",
      "(504000, 14)\n",
      "(56000, 14)\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 128)               128128    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 32)                4128      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 14)                462       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 132,718\n",
      "Trainable params: 132,718\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "15750/15750 [==============================] - 255s 15ms/step - loss: 0.4784 - accuracy: 0.8444 - val_loss: 0.2052 - val_accuracy: 0.9367 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "15750/15750 [==============================] - 234s 15ms/step - loss: 0.3027 - accuracy: 0.9047 - val_loss: 0.2112 - val_accuracy: 0.9361 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "15750/15750 [==============================] - 238s 15ms/step - loss: 0.2654 - accuracy: 0.9176 - val_loss: 0.1651 - val_accuracy: 0.9488 - lr: 0.0010\n",
      "Epoch 4/10\n",
      "15750/15750 [==============================] - 239s 15ms/step - loss: 0.2540 - accuracy: 0.9216 - val_loss: 0.1946 - val_accuracy: 0.9407 - lr: 0.0010\n",
      "Epoch 5/10\n",
      "15750/15750 [==============================] - 236s 15ms/step - loss: 0.2386 - accuracy: 0.9265 - val_loss: 0.1510 - val_accuracy: 0.9532 - lr: 0.0010\n",
      "Epoch 6/10\n",
      "15750/15750 [==============================] - 231s 15ms/step - loss: 0.2292 - accuracy: 0.9298 - val_loss: 0.1756 - val_accuracy: 0.9476 - lr: 0.0010\n",
      "Epoch 7/10\n",
      "15747/15750 [============================>.] - ETA: 0s - loss: 0.2400 - accuracy: 0.9268\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "15750/15750 [==============================] - 241s 15ms/step - loss: 0.2401 - accuracy: 0.9268 - val_loss: 0.1855 - val_accuracy: 0.9460 - lr: 0.0010\n",
      "train time: 1681.277s\n",
      "2188/2188 [==============================] - 21s 7ms/step\n",
      "test time:  25.557s\n",
      "accuracy:   0.945\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.91      0.88      5000\n",
      "           1       0.90      0.98      0.94      5000\n",
      "           2       0.85      0.91      0.88      5000\n",
      "           3       0.98      0.98      0.98      5000\n",
      "           4       0.97      0.94      0.96      5000\n",
      "           5       0.99      0.91      0.95      5000\n",
      "           6       0.92      0.95      0.94      5000\n",
      "           7       0.96      0.98      0.97      5000\n",
      "           8       0.99      0.98      0.99      5000\n",
      "           9       0.98      0.96      0.97      5000\n",
      "          10       0.97      0.98      0.97      5000\n",
      "          11       0.96      0.97      0.97      5000\n",
      "          12       0.98      0.92      0.95      5000\n",
      "          13       0.94      0.86      0.90      5000\n",
      "\n",
      "    accuracy                           0.95     70000\n",
      "   macro avg       0.95      0.95      0.95     70000\n",
      "weighted avg       0.95      0.95      0.95     70000\n",
      "\n",
      "confusion matrix:\n",
      "[[4561  141   53    9   15   30  104    8    1    5    1   13    8   51]\n",
      " [  30 4896    8    1    7    1   42    4   11    0    0    0    0    0]\n",
      " [  70   55 4554   19   52    0   20    3    1    1    1  104   15  105]\n",
      " [   6   31   37 4884   16   11    1    5    0    6    1    0    0    2]\n",
      " [  16  115   95   14 4711    3   25    7    9    0    0    0    0    5]\n",
      " [ 356    4    0    1    2 4538   79   13    1    2    3    1    0    0]\n",
      " [  48  132    4    0   15    7 4740   41   10    0    1    0    2    0]\n",
      " [   3   19    0    0    1    1   41 4918   17    0    0    0    0    0]\n",
      " [   1   32    1    1    3    0   14   42 4906    0    0    0    0    0]\n",
      " [   1    0    3   13    0    1    1   45    0 4807  128    0    0    1]\n",
      " [  17    0    4    1    0    1    1   11    0   82 4882    0    0    1]\n",
      " [  28    0  108    1    0    1    2    0    0    0    0 4849    7    4]\n",
      " [  32    1  146    2    4    4   49    2    0    1    0   49 4624   86]\n",
      " [ 239   33  331   17   15    1   15    1    0    0    1   16   47 4284]]\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "y_train, _, x_train = pickle.load(open(\"dbpedia_csv/\" + \"/train_vec_preprocessed_vectorized.pickle\", \"rb\"))\n",
    "#_, _, x_valid = pickle.load(open(paths.data + \"/validation_preprocessed.pickle\", \"rb\"))\n",
    "y_test, _, x_test = pickle.load(open(\"dbpedia_csv/\" + \"/test_vec_preprocessed_vectorized.pickle\", \"rb\"))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "x_train_temp = pd.DataFrame(x_train).to_numpy()\n",
    "y_train_temp = []\n",
    "\n",
    "\n",
    "x_valid = []\n",
    "y_valid = []\n",
    "\n",
    "x_train = []\n",
    "y_train_np = []\n",
    "\n",
    "\n",
    "for i in range(len(y_train)):\n",
    "    if i % 10 == 0:\n",
    "        y_valid.append(y_train[i])\n",
    "        x_valid.append(x_train_temp[i])\n",
    "    else:\n",
    "        y_train_np.append(y_train[i])\n",
    "        x_train.append(x_train_temp[i])\n",
    "        \n",
    "y_train = y_train_np\n",
    "x_train = np.array(x_train)\n",
    "x_valid = np.array(x_valid)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "y_test_np = []\n",
    "\n",
    "for y in y_test:\n",
    "    l = [0 for x in range(14)]\n",
    "    l[y-1] = 1\n",
    "    y_test_np.append(l)\n",
    "    \n",
    "y_test = np.asarray(y_test_np)\n",
    "print(y_test.shape)\n",
    "\n",
    "y_train_np = []\n",
    "\n",
    "for y in y_train:\n",
    "    l = [0 for x in range(14)]\n",
    "    l[y-1] = 1\n",
    "    y_train_np.append(l)\n",
    "    \n",
    "y_train = np.asarray(y_train_np)\n",
    "print(y_train.shape)\n",
    "\n",
    "\n",
    "y_train_valid_np = []\n",
    "\n",
    "for y in y_valid:\n",
    "    l = [0 for x in range(14)]\n",
    "    l[y-1] = 1\n",
    "    y_train_valid_np.append(l)\n",
    "    \n",
    "y_valid = np.asarray(y_train_valid_np)\n",
    "print(y_valid.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "PredictorScaler=MinMaxScaler()\n",
    "\n",
    "# Storing the fit object for later reference\n",
    "PredictorScalerFit=PredictorScaler.fit(x_train)\n",
    "\n",
    "\n",
    "x_train = PredictorScalerFit.transform(x_train)\n",
    "x_test = PredictorScalerFit.transform(x_test)\n",
    "x_valid = PredictorScalerFit.transform(x_valid)\n",
    "\n",
    "\n",
    "del y_train_np\n",
    "del y_test_np\n",
    "del y_train_valid_np\n",
    "del x_train_temp\n",
    "del PredictorScalerFit\n",
    "del PredictorScaler\n",
    "import sys\n",
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "## TRAIN AND FIT CLASSIFIER\n",
    "\n",
    "t = time()\n",
    "\n",
    "model_l1 = Sequential()\n",
    "# model_l1.add(Embedding(56000, 1000))\n",
    "model_l1.add(Input(shape=(1000,), name='input'))\n",
    "# model_l1.add(Conv1D(128, 5, activation='relu'))\n",
    "# model_l1.add(GlobalMaxPooling1D()) # added\n",
    "model_l1.add(Dense(128, activation='relu'))\n",
    "model_l1.add(Dropout(0.1))\n",
    "model_l1.add(Dense(32, activation='relu'))\n",
    "model_l1.add(Dense(14, activation='softmax')) \n",
    "model_l1.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model_l1.summary()\n",
    "\n",
    "history_l1 = model_l1.fit(x_train, y_train, epochs=10, verbose=1, use_multiprocessing=True,\n",
    "                   validation_data=(x_valid, y_valid),\n",
    "                    batch_size=32\n",
    "                   ,callbacks=[reduce,callback])\n",
    "\n",
    "\n",
    "\n",
    "training_time = time() - t\n",
    "print(\"train time: %0.3fs\" % training_time)\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "t = time()\n",
    "y_pred = model_l1.predict(x_test)\n",
    "\n",
    "test_time = time() - t\n",
    "print(\"test time:  %0.3fs\" % test_time)\n",
    "\n",
    "y_pred=np.argmax(y_pred, axis=1)\n",
    "y_test=np.argmax(y_test, axis=1)\n",
    "# cm = confusion_matrix(y_test, y_pred)\n",
    "# print(cm)\n",
    "\n",
    "# compute the performance measures\n",
    "score1 = metrics.accuracy_score(y_test, y_pred)\n",
    "print(\"accuracy:   %0.3f\" % score1)\n",
    "\n",
    "print(metrics.classification_report(y_test, y_pred, target_names=[str(x) for x in range(14)]))\n",
    "\n",
    "print(\"confusion matrix:\")\n",
    "print(metrics.confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print('------------------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2294af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
