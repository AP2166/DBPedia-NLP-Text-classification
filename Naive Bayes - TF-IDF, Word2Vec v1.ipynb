{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "da7b11f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "import pandas\n",
    "import pickle\n",
    "import numpy\n",
    "\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "from time import time\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759da62c",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ba227a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring = ['accuracy', 'precision', 'recall', 'f1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f72dd71",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train, _, x_train = pickle.load(open(\"dbpedia_csv/\" + \"/train_preprocessed.pickle\", \"rb\"))\n",
    "#_, _, x_valid = pickle.load(open(paths.data + \"/validation_preprocessed.pickle\", \"rb\"))\n",
    "y_test, _, x_test = pickle.load(open(\"dbpedia_csv/\" + \"/test_preprocessed.pickle\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3bb09b04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to extract features from training data : 23.517927 seconds\n",
      "n_samples: 560000, n_features: 698699\n"
     ]
    }
   ],
   "source": [
    "t = time()  # not compulsory\n",
    "\n",
    "# loading CountVectorizer\n",
    "tf_vectorizer = TfidfVectorizer() # or term frequency\n",
    "\n",
    "x_train = tf_vectorizer.fit_transform(x_train)\n",
    "\n",
    "duration = time() - t\n",
    "print(\"Time taken to extract features from training data : %f seconds\" % (duration))\n",
    "print(\"n_samples: %d, n_features: %d\" % x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "53008fb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to extract features from test data : 2.694394 seconds\n",
      "n_samples: 70000, n_features: 698699\n"
     ]
    }
   ],
   "source": [
    "t = time()\n",
    "x_test = tf_vectorizer.transform(x_test)\n",
    "\n",
    "duration = time() - t\n",
    "print(\"Time taken to extract features from test data : %f seconds\" % (duration))\n",
    "print(\"n_samples: %d, n_features: %d\" % x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "459a7914",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "640"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import gc \n",
    "del X_train_tf\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "33ffdecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train time: 1.146s\n"
     ]
    }
   ],
   "source": [
    "t = time()\n",
    "\n",
    "naive_bayes_classifier = MultinomialNB()\n",
    "# naive_bayes_classifier.fit(X_train_tf, y_train)\n",
    "\n",
    "naive_bayes_classifier.fit(x_train, y_train)\n",
    "\n",
    "training_time = time() - t\n",
    "print(\"train time: %0.3fs\" % training_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a1e12462",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test time:  0.204s\n",
      "accuracy:   0.947\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.86      0.89      5000\n",
      "           1       0.94      0.97      0.96      5000\n",
      "           2       0.94      0.77      0.85      5000\n",
      "           3       0.98      0.99      0.99      5000\n",
      "           4       0.96      0.97      0.96      5000\n",
      "           5       0.96      0.99      0.97      5000\n",
      "           6       0.94      0.95      0.95      5000\n",
      "           7       0.98      0.98      0.98      5000\n",
      "           8       1.00      0.95      0.97      5000\n",
      "           9       1.00      0.98      0.99      5000\n",
      "          10       0.99      0.99      0.99      5000\n",
      "          11       0.88      0.99      0.93      5000\n",
      "          12       0.93      0.96      0.95      5000\n",
      "          13       0.86      0.92      0.89      5000\n",
      "\n",
      "    accuracy                           0.95     70000\n",
      "   macro avg       0.95      0.95      0.95     70000\n",
      "weighted avg       0.95      0.95      0.95     70000\n",
      "\n",
      "confusion matrix:\n",
      "[[4276   56   47   12   19  142   58    3    0    0    0  190   42  155]\n",
      " [  47 4862   12    3   22    5   34    1    3    0    0    0    1   10]\n",
      " [  46   20 3850   12   92    2   18    1    1    1    2  421  116  418]\n",
      " [   2    0   24 4939   14    8    3    0    0    0    0    2    7    1]\n",
      " [  23   25   42   15 4831   16   18    0    0    0    0    1    9   20]\n",
      " [  39    1    1    0    1 4942    9    1    0    0    0    0    2    4]\n",
      " [  64  100   10    2   16   21 4748   25    2    1    0    1    1    9]\n",
      " [   6    6    0    0    4    8   74 4886    7    2    3    1    0    3]\n",
      " [   4   53    2    0   10    1   73   86 4768    0    0    0    0    3]\n",
      " [   6    0    6   25    1    1    3    7    0 4885   57    1    3    5]\n",
      " [  14    1    4    0    0    1    4    1    0   16 4957    0    1    1]\n",
      " [   3    0   28    1    0    0    0    0    0    0    0 4940   21    7]\n",
      " [  10    1   24    7    4    8    1    0    0    0    0   46 4794  105]\n",
      " [  97   22   57    9   23    3    5    0    0    1    3   25  148 4607]]\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "t = time()\n",
    "y_pred = naive_bayes_classifier.predict(x_test)\n",
    "\n",
    "test_time = time() - t\n",
    "print(\"test time:  %0.3fs\" % test_time)\n",
    "\n",
    "# compute the performance measures\n",
    "score1 = metrics.accuracy_score(y_test, y_pred)\n",
    "print(\"accuracy:   %0.3f\" % score1)\n",
    "\n",
    "print(metrics.classification_report(y_test, y_pred, target_names=[str(x) for x in range(14)]))\n",
    "\n",
    "print(\"confusion matrix:\")\n",
    "print(metrics.confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print('------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335b3c82",
   "metadata": {},
   "source": [
    "### half of the dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "19289b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train, _, x_train = pickle.load(open(\"dbpedia_csv/\" + \"/train_preprocessed.pickle\", \"rb\"))\n",
    "#_, _, x_valid = pickle.load(open(paths.data + \"/validation_preprocessed.pickle\", \"rb\"))\n",
    "y_test, _, x_test = pickle.load(open(\"dbpedia_csv/\" + \"/test_preprocessed.pickle\", \"rb\"))\n",
    "\n",
    "y_train = y_train[slice(0, len(y_train), 2)]\n",
    "x_train = x_train[slice(0, len(x_train), 2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "db1c3723",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to extract features from training data : 12.984325 seconds\n",
      "n_samples: 280000, n_features: 445084\n",
      "Time taken to extract features from test data : 3.610619 seconds\n",
      "n_samples: 70000, n_features: 445084\n"
     ]
    }
   ],
   "source": [
    "t = time()  # not compulsory\n",
    "\n",
    "# loading CountVectorizer\n",
    "tf_vectorizer = TfidfVectorizer() # or term frequency\n",
    "\n",
    "x_train = tf_vectorizer.fit_transform(x_train)\n",
    "\n",
    "duration = time() - t\n",
    "print(\"Time taken to extract features from training data : %f seconds\" % (duration))\n",
    "print(\"n_samples: %d, n_features: %d\" % x_train.shape)\n",
    "\n",
    "t = time()\n",
    "x_test = tf_vectorizer.transform(x_test)\n",
    "\n",
    "duration = time() - t\n",
    "print(\"Time taken to extract features from test data : %f seconds\" % (duration))\n",
    "print(\"n_samples: %d, n_features: %d\" % x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9c3c2556",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train time: 0.692s\n"
     ]
    }
   ],
   "source": [
    "t = time()\n",
    "\n",
    "naive_bayes_classifier = MultinomialNB()\n",
    "# naive_bayes_classifier.fit(X_train_tf, y_train)\n",
    "\n",
    "naive_bayes_classifier.fit(x_train, y_train)\n",
    "\n",
    "training_time = time() - t\n",
    "print(\"train time: %0.3fs\" % training_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c37a1651",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test time:  0.256s\n",
      "accuracy:   0.947\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.86      0.89      5000\n",
      "           1       0.94      0.97      0.96      5000\n",
      "           2       0.94      0.78      0.85      5000\n",
      "           3       0.98      0.99      0.98      5000\n",
      "           4       0.96      0.97      0.96      5000\n",
      "           5       0.96      0.99      0.97      5000\n",
      "           6       0.94      0.95      0.95      5000\n",
      "           7       0.98      0.98      0.98      5000\n",
      "           8       1.00      0.96      0.98      5000\n",
      "           9       1.00      0.97      0.98      5000\n",
      "          10       0.98      0.99      0.99      5000\n",
      "          11       0.88      0.99      0.93      5000\n",
      "          12       0.94      0.96      0.95      5000\n",
      "          13       0.86      0.92      0.89      5000\n",
      "\n",
      "    accuracy                           0.95     70000\n",
      "   macro avg       0.95      0.95      0.95     70000\n",
      "weighted avg       0.95      0.95      0.95     70000\n",
      "\n",
      "confusion matrix:\n",
      "[[4278   59   49   12   18  148   58    5    0    0    0  180   37  156]\n",
      " [  46 4871   12    5   16    6   32    2    2    0    0    0    0    8]\n",
      " [  43   19 3886   13  102    2   19    1    1    1    2  404  101  406]\n",
      " [   2    0   26 4937   13    8    4    0    0    0    0    1    8    1]\n",
      " [  20   24   36   16 4844   17   17    0    0    0    0    1    9   16]\n",
      " [  39    2    1    0    0 4938   14    2    0    0    0    0    2    2]\n",
      " [  62  106    8    1   17   19 4748   25    2    1    0    2    1    8]\n",
      " [   6    6    0    0    3    9   61 4903    6    1    2    1    0    2]\n",
      " [   4   45    2    1   10    0   77   80 4778    0    0    0    0    3]\n",
      " [   4    0    5   33    2    1    3    8    0 4842   90    1    4    7]\n",
      " [  15    2    3    0    0    1    5    0    0   17 4954    0    1    2]\n",
      " [   3    0   27    1    0    0    0    0    0    0    0 4946   19    4]\n",
      " [  10    1   24    7    5   10    1    0    0    0    0   46 4791  105]\n",
      " [  94   26   57    8   27    3    5    0    0    1    2   27  144 4606]]\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "t = time()\n",
    "y_pred = naive_bayes_classifier.predict(x_test)\n",
    "\n",
    "test_time = time() - t\n",
    "print(\"test time:  %0.3fs\" % test_time)\n",
    "\n",
    "# compute the performance measures\n",
    "score1 = metrics.accuracy_score(y_test, y_pred)\n",
    "print(\"accuracy:   %0.3f\" % score1)\n",
    "\n",
    "print(metrics.classification_report(y_test, y_pred, target_names=[str(x) for x in range(14)]))\n",
    "\n",
    "print(\"confusion matrix:\")\n",
    "print(metrics.confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print('------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684aa827",
   "metadata": {},
   "source": [
    "### one fourth of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fde04a66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to extract features from training data : 5.715026 seconds\n",
      "n_samples: 140000, n_features: 282474\n",
      "Time taken to extract features from test data : 2.271709 seconds\n",
      "n_samples: 70000, n_features: 282474\n",
      "train time: 0.285s\n",
      "test time:  0.061s\n",
      "accuracy:   0.947\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.86      0.89      5000\n",
      "           1       0.94      0.98      0.96      5000\n",
      "           2       0.94      0.77      0.85      5000\n",
      "           3       0.98      0.99      0.98      5000\n",
      "           4       0.96      0.97      0.96      5000\n",
      "           5       0.96      0.99      0.97      5000\n",
      "           6       0.94      0.95      0.95      5000\n",
      "           7       0.97      0.98      0.98      5000\n",
      "           8       1.00      0.96      0.98      5000\n",
      "           9       1.00      0.96      0.98      5000\n",
      "          10       0.97      0.99      0.98      5000\n",
      "          11       0.88      0.99      0.93      5000\n",
      "          12       0.94      0.96      0.95      5000\n",
      "          13       0.87      0.92      0.89      5000\n",
      "\n",
      "    accuracy                           0.95     70000\n",
      "   macro avg       0.95      0.95      0.95     70000\n",
      "weighted avg       0.95      0.95      0.95     70000\n",
      "\n",
      "confusion matrix:\n",
      "[[4277   62   54   14   19  143   57    5    0    0    0  181   38  150]\n",
      " [  39 4883   10    6   13    6   33    1    1    0    0    0    0    8]\n",
      " [  45   22 3860   15  106    2   22    1    1    1    1  412  106  406]\n",
      " [   3    1   30 4938   11    7    4    0    0    0    0    0    5    1]\n",
      " [  22   30   35   19 4837   16   19    0    0    0    0    1    7   14]\n",
      " [  42    2    2    1    0 4932   14    2    0    0    0    1    2    2]\n",
      " [  57  103   10    2   18   19 4754   28    2    1    0    1    1    4]\n",
      " [   6    6    1    0    3    9   58 4906    6    1    2    1    0    1]\n",
      " [   4   37    2    0    9    0   72   83 4791    0    0    0    0    2]\n",
      " [   4    0    5   37    2    2    3   10    0 4803  122    2    3    7]\n",
      " [  15    2    3    0    0    1    5    1    0   19 4950    0    1    3]\n",
      " [   3    0   26    1    0    0    1    0    0    0    0 4947   17    5]\n",
      " [  10    1   21    8    4   10    1    0    0    1    0   46 4799   99]\n",
      " [ 100   23   58   10   28    4    7    0    0    1    2   23  141 4603]]\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "y_train, _, x_train = pickle.load(open(\"dbpedia_csv/\" + \"/train_preprocessed.pickle\", \"rb\"))\n",
    "#_, _, x_valid = pickle.load(open(paths.data + \"/validation_preprocessed.pickle\", \"rb\"))\n",
    "y_test, _, x_test = pickle.load(open(\"dbpedia_csv/\" + \"/test_preprocessed.pickle\", \"rb\"))\n",
    "\n",
    "y_train = y_train[slice(0, len(y_train), 4)]\n",
    "x_train = x_train[slice(0, len(x_train), 4)]\n",
    "\n",
    "\n",
    "## TFIDF VECTORIZATION\n",
    "\n",
    "t = time()  # not compulsory\n",
    "\n",
    "# loading CountVectorizer\n",
    "tf_vectorizer = TfidfVectorizer() # or term frequency\n",
    "\n",
    "x_train = tf_vectorizer.fit_transform(x_train)\n",
    "\n",
    "duration = time() - t\n",
    "print(\"Time taken to extract features from training data : %f seconds\" % (duration))\n",
    "print(\"n_samples: %d, n_features: %d\" % x_train.shape)\n",
    "\n",
    "t = time()\n",
    "x_test = tf_vectorizer.transform(x_test)\n",
    "\n",
    "duration = time() - t\n",
    "print(\"Time taken to extract features from test data : %f seconds\" % (duration))\n",
    "print(\"n_samples: %d, n_features: %d\" % x_test.shape)\n",
    "\n",
    "\n",
    "## TRAIN AND FIT CLASSIFIER\n",
    "\n",
    "t = time()\n",
    "\n",
    "naive_bayes_classifier = MultinomialNB()\n",
    "# naive_bayes_classifier.fit(X_train_tf, y_train)\n",
    "\n",
    "naive_bayes_classifier.fit(x_train, y_train)\n",
    "\n",
    "training_time = time() - t\n",
    "print(\"train time: %0.3fs\" % training_time)\n",
    "\n",
    "\n",
    "t = time()\n",
    "y_pred = naive_bayes_classifier.predict(x_test)\n",
    "\n",
    "test_time = time() - t\n",
    "print(\"test time:  %0.3fs\" % test_time)\n",
    "\n",
    "# compute the performance measures\n",
    "score1 = metrics.accuracy_score(y_test, y_pred)\n",
    "print(\"accuracy:   %0.3f\" % score1)\n",
    "\n",
    "print(metrics.classification_report(y_test, y_pred, target_names=[str(x) for x in range(14)]))\n",
    "\n",
    "print(\"confusion matrix:\")\n",
    "print(metrics.confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print('------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff507dd",
   "metadata": {},
   "source": [
    "## Word2Vec using pretrained model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59fa3701",
   "metadata": {},
   "source": [
    "### full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4978457b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train time: 2.698s\n",
      "test time:  0.397s\n",
      "accuracy:   0.880\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.82      0.80      5000\n",
      "           1       0.94      0.93      0.94      5000\n",
      "           2       0.80      0.72      0.76      5000\n",
      "           3       0.99      0.93      0.96      5000\n",
      "           4       0.95      0.93      0.94      5000\n",
      "           5       0.85      0.93      0.89      5000\n",
      "           6       0.83      0.90      0.86      5000\n",
      "           7       0.96      0.89      0.92      5000\n",
      "           8       0.98      0.96      0.97      5000\n",
      "           9       0.97      0.65      0.78      5000\n",
      "          10       0.81      0.90      0.85      5000\n",
      "          11       0.88      0.97      0.92      5000\n",
      "          12       0.95      0.88      0.92      5000\n",
      "          13       0.73      0.90      0.80      5000\n",
      "\n",
      "    accuracy                           0.88     70000\n",
      "   macro avg       0.89      0.88      0.88     70000\n",
      "weighted avg       0.89      0.88      0.88     70000\n",
      "\n",
      "confusion matrix:\n",
      "[[4115   36  116    4   19  176   79    2    0    0    0  200   28  225]\n",
      " [ 132 4671   20    4   39    9   74    4   31    0    0    0    4   12]\n",
      " [  66   41 3614    7   73    4   24    1    0    0    1  386  107  676]\n",
      " [  16   10  192 4633   31   78    6    0    0    0    0    6    0   28]\n",
      " [  47   80  105    8 4650   33   25    1    6    0    0    3    5   37]\n",
      " [ 286    0    5    1    0 4673   18    1    0    0    0    2    0   14]\n",
      " [ 205  121   32    2   32   50 4486   21   24    0    0    0    3   24]\n",
      " [  19    1    9    0    8   76  408 4435   31    0    0    0    0   13]\n",
      " [   6    7    3    0    8    3  114   31 4819    0    0    0    0    9]\n",
      " [  39    0   70    3    7  289   85   59    0 3261 1067    0    0  120]\n",
      " [ 113    0   18    0    1   62   92   42    0  110 4477    1    1   83]\n",
      " [   5    0   69    1    0    2    1    0    0    0    0 4858   17   47]\n",
      " [   4    5  111    5    3   17    1    1    0    0    0   34 4420  399]\n",
      " [ 216   18  128    8   32   10   21    1    0    0    1   16   68 4481]]\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "y_train, _, x_train = pickle.load(open(\"dbpedia_csv/\" + \"/train_vec_preprocessed_vectorized.pickle\", \"rb\"))\n",
    "#_, _, x_valid = pickle.load(open(paths.data + \"/validation_preprocessed.pickle\", \"rb\"))\n",
    "y_test, _, x_test = pickle.load(open(\"dbpedia_csv/\" + \"/test_vec_preprocessed_vectorized.pickle\", \"rb\"))\n",
    "\n",
    "\n",
    "\n",
    "# Choose either standardization or Normalization\n",
    "# On this data Min Max Normalization is used because we need to fit Naive Bayes\n",
    "\n",
    "# Choose between standardization and MinMAx normalization\n",
    "#PredictorScaler=StandardScaler()\n",
    "PredictorScaler=MinMaxScaler()\n",
    "\n",
    "# Storing the fit object for later reference\n",
    "PredictorScalerFit=PredictorScaler.fit(x_train)\n",
    "\n",
    "\n",
    "x_train = PredictorScalerFit.transform(x_train)\n",
    "x_test = PredictorScalerFit.transform(x_test)\n",
    "\n",
    "\n",
    "## TRAIN AND FIT CLASSIFIER\n",
    "\n",
    "t = time()\n",
    "\n",
    "naive_bayes_classifier = MultinomialNB()\n",
    "# naive_bayes_classifier.fit(X_train_tf, y_train)\n",
    "\n",
    "naive_bayes_classifier.fit(x_train, y_train)\n",
    "\n",
    "training_time = time() - t\n",
    "print(\"train time: %0.3fs\" % training_time)\n",
    "\n",
    "\n",
    "t = time()\n",
    "y_pred = naive_bayes_classifier.predict(x_test)\n",
    "\n",
    "test_time = time() - t\n",
    "print(\"test time:  %0.3fs\" % test_time)\n",
    "\n",
    "# compute the performance measures\n",
    "score1 = metrics.accuracy_score(y_test, y_pred)\n",
    "print(\"accuracy:   %0.3f\" % score1)\n",
    "\n",
    "print(metrics.classification_report(y_test, y_pred, target_names=[str(x) for x in range(14)]))\n",
    "\n",
    "print(\"confusion matrix:\")\n",
    "print(metrics.confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print('------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fcce4dc",
   "metadata": {},
   "source": [
    "### half dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "77af6261",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train time: 1.142s\n",
      "test time:  0.504s\n",
      "accuracy:   0.881\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.82      0.80      5000\n",
      "           1       0.94      0.94      0.94      5000\n",
      "           2       0.81      0.72      0.76      5000\n",
      "           3       0.99      0.93      0.96      5000\n",
      "           4       0.94      0.93      0.94      5000\n",
      "           5       0.86      0.93      0.89      5000\n",
      "           6       0.83      0.90      0.86      5000\n",
      "           7       0.96      0.89      0.93      5000\n",
      "           8       0.98      0.96      0.97      5000\n",
      "           9       0.97      0.66      0.78      5000\n",
      "          10       0.81      0.90      0.85      5000\n",
      "          11       0.88      0.97      0.93      5000\n",
      "          12       0.95      0.88      0.92      5000\n",
      "          13       0.72      0.90      0.80      5000\n",
      "\n",
      "    accuracy                           0.88     70000\n",
      "   macro avg       0.89      0.88      0.88     70000\n",
      "weighted avg       0.89      0.88      0.88     70000\n",
      "\n",
      "confusion matrix:\n",
      "[[4116   36  126    5   19  174   71    2    1    0    0  197   28  225]\n",
      " [ 122 4679   20    4   40    9   71    4   35    0    0    0    4   12]\n",
      " [  63   38 3614    8   83    4   24    1    0    0    1  384  104  676]\n",
      " [  20    7  175 4658   30   73    6    0    0    0    0    6    0   25]\n",
      " [  49   78  105    8 4656   31   24    1    6    0    0    1    5   36]\n",
      " [ 290    0    5    0    0 4671   17    1    0    0    0    2    0   14]\n",
      " [ 197  127   33    2   32   48 4489   22   24    0    0    0    3   23]\n",
      " [  23    1    7    0   11   74  383 4457   30    0    0    0    0   14]\n",
      " [   6    7    3    0    9    3  115   31 4817    0    0    0    0    9]\n",
      " [  41    0   70    5   12  282   97   62    0 3275 1029    0    0  127]\n",
      " [ 116    0   15    0    1   62  100   47    0   88 4479    0    1   91]\n",
      " [   7    0   69    1    0    1    1    0    0    0    0 4854   17   50]\n",
      " [   5    5  112    6    3   18    0    1    0    0    0   31 4415  404]\n",
      " [ 216   18  127    9   32   11   23    1    0    0    0   14   67 4482]]\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "y_train, _, x_train = pickle.load(open(\"dbpedia_csv/\" + \"/train_vec_preprocessed_vectorized.pickle\", \"rb\"))\n",
    "#_, _, x_valid = pickle.load(open(paths.data + \"/validation_preprocessed.pickle\", \"rb\"))\n",
    "y_test, _, x_test = pickle.load(open(\"dbpedia_csv/\" + \"/test_vec_preprocessed_vectorized.pickle\", \"rb\"))\n",
    "\n",
    "\n",
    "y_train = y_train[slice(0, len(y_train), 2)]\n",
    "x_train = x_train[slice(0, len(x_train), 2)]\n",
    "\n",
    "\n",
    "# Choose either standardization or Normalization\n",
    "# On this data Min Max Normalization is used because we need to fit Naive Bayes\n",
    "\n",
    "# Choose between standardization and MinMAx normalization\n",
    "#PredictorScaler=StandardScaler()\n",
    "PredictorScaler=MinMaxScaler()\n",
    "\n",
    "# Storing the fit object for later reference\n",
    "PredictorScalerFit=PredictorScaler.fit(x_train)\n",
    "\n",
    "\n",
    "x_train = PredictorScalerFit.transform(x_train)\n",
    "x_test = PredictorScalerFit.transform(x_test)\n",
    "\n",
    "\n",
    "## TRAIN AND FIT CLASSIFIER\n",
    "\n",
    "t = time()\n",
    "\n",
    "naive_bayes_classifier = MultinomialNB()\n",
    "# naive_bayes_classifier.fit(X_train_tf, y_train)\n",
    "\n",
    "naive_bayes_classifier.fit(x_train, y_train)\n",
    "\n",
    "training_time = time() - t\n",
    "print(\"train time: %0.3fs\" % training_time)\n",
    "\n",
    "\n",
    "t = time()\n",
    "y_pred = naive_bayes_classifier.predict(x_test)\n",
    "\n",
    "test_time = time() - t\n",
    "print(\"test time:  %0.3fs\" % test_time)\n",
    "\n",
    "# compute the performance measures\n",
    "score1 = metrics.accuracy_score(y_test, y_pred)\n",
    "print(\"accuracy:   %0.3f\" % score1)\n",
    "\n",
    "print(metrics.classification_report(y_test, y_pred, target_names=[str(x) for x in range(14)]))\n",
    "\n",
    "print(\"confusion matrix:\")\n",
    "print(metrics.confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print('------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f509f0d9",
   "metadata": {},
   "source": [
    "### one fourth of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b2c8d020",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train time: 0.621s\n",
      "test time:  0.325s\n",
      "accuracy:   0.882\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.82      0.80      5000\n",
      "           1       0.94      0.94      0.94      5000\n",
      "           2       0.81      0.72      0.76      5000\n",
      "           3       0.99      0.93      0.96      5000\n",
      "           4       0.94      0.93      0.94      5000\n",
      "           5       0.86      0.94      0.90      5000\n",
      "           6       0.82      0.90      0.86      5000\n",
      "           7       0.96      0.89      0.92      5000\n",
      "           8       0.98      0.96      0.97      5000\n",
      "           9       0.98      0.66      0.79      5000\n",
      "          10       0.82      0.90      0.86      5000\n",
      "          11       0.88      0.97      0.93      5000\n",
      "          12       0.95      0.88      0.92      5000\n",
      "          13       0.73      0.90      0.80      5000\n",
      "\n",
      "    accuracy                           0.88     70000\n",
      "   macro avg       0.89      0.88      0.88     70000\n",
      "weighted avg       0.89      0.88      0.88     70000\n",
      "\n",
      "confusion matrix:\n",
      "[[4105   37  121    6   21  180   74    3    1    0    0  199   29  224]\n",
      " [ 127 4675   20    3   40    9   72    4   35    0    0    0    4   11]\n",
      " [  63   37 3613    8   79    6   25    1    0    0    1  384  108  675]\n",
      " [  15    8  172 4666   32   73    5    0    0    0    0    6    0   23]\n",
      " [  45   80   99    8 4664   32   25    1    6    0    0    1    5   34]\n",
      " [ 284    0    6    1    0 4678   16    1    0    0    0    2    0   12]\n",
      " [ 193  126   30    2   34   45 4493   22   25    0    0    0    3   27]\n",
      " [  21    1    8    0   13   72  401 4441   31    0    0    0    0   12]\n",
      " [   6    7    3    0    9    3  117   31 4817    0    0    0    0    7]\n",
      " [  44    0   64    5   19  268  118   59    0 3295 1009    0    0  119]\n",
      " [ 118    0   16    0    1   56  111   49    0   64 4497    0    1   87]\n",
      " [   5    0   69    1    0    1    1    0    0    0    0 4859   17   47]\n",
      " [   6    4  113    6    3   17    1    1    0    0    0   33 4421  395]\n",
      " [ 212   18  127    8   33   11   24    1    0    0    1   14   69 4482]]\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "y_train, _, x_train = pickle.load(open(\"dbpedia_csv/\" + \"/train_vec_preprocessed_vectorized.pickle\", \"rb\"))\n",
    "#_, _, x_valid = pickle.load(open(paths.data + \"/validation_preprocessed.pickle\", \"rb\"))\n",
    "y_test, _, x_test = pickle.load(open(\"dbpedia_csv/\" + \"/test_vec_preprocessed_vectorized.pickle\", \"rb\"))\n",
    "\n",
    "\n",
    "y_train = y_train[slice(0, len(y_train), 4)]\n",
    "x_train = x_train[slice(0, len(x_train), 4)]\n",
    "\n",
    "\n",
    "# Choose either standardization or Normalization\n",
    "# On this data Min Max Normalization is used because we need to fit Naive Bayes\n",
    "\n",
    "# Choose between standardization and MinMAx normalization\n",
    "#PredictorScaler=StandardScaler()\n",
    "PredictorScaler=MinMaxScaler()\n",
    "\n",
    "# Storing the fit object for later reference\n",
    "PredictorScalerFit=PredictorScaler.fit(x_train)\n",
    "\n",
    "\n",
    "x_train = PredictorScalerFit.transform(x_train)\n",
    "x_test = PredictorScalerFit.transform(x_test)\n",
    "\n",
    "\n",
    "## TRAIN AND FIT CLASSIFIER\n",
    "\n",
    "t = time()\n",
    "\n",
    "naive_bayes_classifier = MultinomialNB()\n",
    "# naive_bayes_classifier.fit(X_train_tf, y_train)\n",
    "\n",
    "naive_bayes_classifier.fit(x_train, y_train)\n",
    "\n",
    "training_time = time() - t\n",
    "print(\"train time: %0.3fs\" % training_time)\n",
    "\n",
    "\n",
    "t = time()\n",
    "y_pred = naive_bayes_classifier.predict(x_test)\n",
    "\n",
    "test_time = time() - t\n",
    "print(\"test time:  %0.3fs\" % test_time)\n",
    "\n",
    "# compute the performance measures\n",
    "score1 = metrics.accuracy_score(y_test, y_pred)\n",
    "print(\"accuracy:   %0.3f\" % score1)\n",
    "\n",
    "print(metrics.classification_report(y_test, y_pred, target_names=[str(x) for x in range(14)]))\n",
    "\n",
    "print(\"confusion matrix:\")\n",
    "print(metrics.confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print('------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504575e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
