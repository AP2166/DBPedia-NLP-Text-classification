{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "130b4421",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from time import time\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62935947",
   "metadata": {},
   "source": [
    "## word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13abec37",
   "metadata": {},
   "source": [
    "### full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "572f972d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  56 tasks      | elapsed: 13.0min\n",
      "[Parallel(n_jobs=-1)]: Done 200 out of 200 | elapsed: 44.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train time: 2681.743s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  56 tasks      | elapsed:    0.7s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test time:  3.112s\n",
      "accuracy:   0.952\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.87      0.89      5000\n",
      "           1       0.96      0.97      0.96      5000\n",
      "           2       0.89      0.85      0.87      5000\n",
      "           3       0.98      0.98      0.98      5000\n",
      "           4       0.95      0.96      0.96      5000\n",
      "           5       0.96      0.98      0.97      5000\n",
      "           6       0.96      0.94      0.95      5000\n",
      "           7       0.97      0.99      0.98      5000\n",
      "           8       1.00      0.98      0.99      5000\n",
      "           9       0.99      0.99      0.99      5000\n",
      "          10       0.99      0.99      0.99      5000\n",
      "          11       0.95      0.97      0.96      5000\n",
      "          12       0.96      0.96      0.96      5000\n",
      "          13       0.88      0.91      0.89      5000\n",
      "\n",
      "    accuracy                           0.95     70000\n",
      "   macro avg       0.95      0.95      0.95     70000\n",
      "weighted avg       0.95      0.95      0.95     70000\n",
      "\n",
      "confusion matrix:\n",
      "[[4366   37   80   19   34  136   58    6    1    2    7   48   42  164]\n",
      " [  47 4832   18    5   33    4   41    5    2    1    0    0    2   10]\n",
      " [  54   24 4239   16   88    3   12    1    0    1    4  172   71  315]\n",
      " [   3    3   37 4925   12    8    4    0    0    3    0    0    1    4]\n",
      " [  29   48   66   15 4801   12   13    3    0    0    0    0    2   11]\n",
      " [  85    1    1    2    4 4879   12    5    0    5    0    0    1    5]\n",
      " [  75   65   21    4   28   26 4715   48    4    2    0    0    4    8]\n",
      " [   4    1    3    0    2    7   32 4933    9    5    2    0    0    2]\n",
      " [   4    1    3    0   10    0   35   48 4897    1    0    0    0    1]\n",
      " [   1    0    2    6    0    2    1    8    0 4931   47    0    0    2]\n",
      " [  15    0    4    0    0    2    0    1    0   42 4931    1    1    3]\n",
      " [   6    0   90    1    0    2    2    0    0    0    1 4872   16   10]\n",
      " [   5    0   53   10    4   11    0    2    0    2    1   26 4783  103]\n",
      " [  84   17  158   16   38   11    8    6    0    6    4   15   77 4560]]\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Done 200 out of 200 | elapsed:    2.7s finished\n"
     ]
    }
   ],
   "source": [
    "y_train, _, x_train = pickle.load(open(\"dbpedia_csv/\" + \"/train_lem_preprocessed_vectorized.pickle\", \"rb\"))\n",
    "#_, _, x_valid = pickle.load(open(paths.data + \"/validation_preprocessed.pickle\", \"rb\"))\n",
    "y_test, _, x_test = pickle.load(open(\"dbpedia_csv/\" + \"/test_lem_preprocessed_vectorized.pickle\", \"rb\"))\n",
    "\n",
    "\n",
    "## TRAIN AND FIT CLASSIFIER\n",
    "\n",
    "t = time()\n",
    "\n",
    "\n",
    "forest = RandomForestClassifier(bootstrap = False, n_estimators = 200, verbose=-1, n_jobs=-1, max_depth=40,  min_samples_split= 2, min_samples_leaf=1)\n",
    "forest = forest.fit(x_train, y_train)\n",
    "\n",
    "training_time = time() - t\n",
    "print(\"train time: %0.3fs\" % training_time)\n",
    "\n",
    "\n",
    "t = time()\n",
    "y_pred = forest.predict(x_test)\n",
    "\n",
    "test_time = time() - t\n",
    "print(\"test time:  %0.3fs\" % test_time)\n",
    "\n",
    "# compute the performance measures\n",
    "score1 = metrics.accuracy_score(y_test, y_pred)\n",
    "print(\"accuracy:   %0.3f\" % score1)\n",
    "\n",
    "print(metrics.classification_report(y_test, y_pred, target_names=[str(x) for x in range(14)]))\n",
    "\n",
    "print(\"confusion matrix:\")\n",
    "print(metrics.confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print('------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffed4a56",
   "metadata": {},
   "source": [
    "### half of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6229807a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  56 tasks      | elapsed:  5.6min\n",
      "[Parallel(n_jobs=-1)]: Done 200 out of 200 | elapsed: 19.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train time: 1159.352s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  56 tasks      | elapsed:    0.7s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test time:  2.761s\n",
      "accuracy:   0.950\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.87      0.89      5000\n",
      "           1       0.96      0.96      0.96      5000\n",
      "           2       0.89      0.84      0.86      5000\n",
      "           3       0.98      0.99      0.98      5000\n",
      "           4       0.95      0.96      0.95      5000\n",
      "           5       0.96      0.98      0.97      5000\n",
      "           6       0.95      0.94      0.95      5000\n",
      "           7       0.97      0.99      0.98      5000\n",
      "           8       1.00      0.98      0.99      5000\n",
      "           9       0.98      0.98      0.98      5000\n",
      "          10       0.99      0.98      0.98      5000\n",
      "          11       0.94      0.97      0.96      5000\n",
      "          12       0.96      0.95      0.96      5000\n",
      "          13       0.87      0.91      0.89      5000\n",
      "\n",
      "    accuracy                           0.95     70000\n",
      "   macro avg       0.95      0.95      0.95     70000\n",
      "weighted avg       0.95      0.95      0.95     70000\n",
      "\n",
      "confusion matrix:\n",
      "[[4338   44   73   21   35  139   61    7    2    1    7   59   42  171]\n",
      " [  48 4814   23    7   36    2   44    7    2    2    0    0    3   12]\n",
      " [  57   28 4205   17   81    3   13    1    0    5    3  183   71  333]\n",
      " [   2    3   31 4926   15   10    3    1    0    5    0    0    1    3]\n",
      " [  28   41   72   17 4794   12   15    3    0    0    1    0    2   15]\n",
      " [  82    1    1    2    2 4881   15    6    0    4    0    0    1    5]\n",
      " [  79   63   25    4   34   28 4703   51    4    1    0    0    2    6]\n",
      " [   6    1    5    0    5    5   32 4926   10    5    3    0    0    2]\n",
      " [   5    1    2    0    9    0   41   49 4891    1    0    0    0    1]\n",
      " [   2    0    5    6    0    2    1   13    0 4917   54    0    0    0]\n",
      " [  16    0    5    0    1    1    2    2    0   55 4916    1    0    1]\n",
      " [  10    0   88    1    1    2    2    0    0    0    1 4866   18   11]\n",
      " [   6    1   57    9    3   11    0    2    0    0    1   31 4774  105]\n",
      " [  95   21  158   17   42    9   10    6    0    5    4   15   74 4544]]\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Done 200 out of 200 | elapsed:    2.5s finished\n"
     ]
    }
   ],
   "source": [
    "y_train, _, x_train = pickle.load(open(\"dbpedia_csv/\" + \"/train_lem_preprocessed_vectorized.pickle\", \"rb\"))\n",
    "#_, _, x_valid = pickle.load(open(paths.data + \"/validation_preprocessed.pickle\", \"rb\"))\n",
    "y_test, _, x_test = pickle.load(open(\"dbpedia_csv/\" + \"/test_lem_preprocessed_vectorized.pickle\", \"rb\"))\n",
    "\n",
    "\n",
    "y_train = y_train[slice(0, len(y_train), 2)]\n",
    "x_train = x_train[slice(0, len(x_train), 2)]\n",
    "\n",
    "\n",
    "\n",
    "## TRAIN AND FIT CLASSIFIER\n",
    "\n",
    "t = time()\n",
    "\n",
    "forest = RandomForestClassifier(bootstrap = False, n_estimators = 200, verbose=-1, n_jobs=-1, max_depth=40,  min_samples_split= 2, min_samples_leaf=1)\n",
    "\n",
    "forest = forest.fit(x_train, y_train)\n",
    "\n",
    "training_time = time() - t\n",
    "print(\"train time: %0.3fs\" % training_time)\n",
    "\n",
    "\n",
    "t = time()\n",
    "y_pred = forest.predict(x_test)\n",
    "\n",
    "test_time = time() - t\n",
    "print(\"test time:  %0.3fs\" % test_time)\n",
    "\n",
    "# compute the performance measures\n",
    "score1 = metrics.accuracy_score(y_test, y_pred)\n",
    "print(\"accuracy:   %0.3f\" % score1)\n",
    "\n",
    "print(metrics.classification_report(y_test, y_pred, target_names=[str(x) for x in range(14)]))\n",
    "\n",
    "print(\"confusion matrix:\")\n",
    "print(metrics.confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print('------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0d0826",
   "metadata": {},
   "source": [
    "### one fourth of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba6f831b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  56 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=-1)]: Done 200 out of 200 | elapsed:  8.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train time: 499.013s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  56 tasks      | elapsed:    0.6s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test time:  2.561s\n",
      "accuracy:   0.947\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.86      0.88      5000\n",
      "           1       0.95      0.96      0.96      5000\n",
      "           2       0.88      0.83      0.85      5000\n",
      "           3       0.98      0.98      0.98      5000\n",
      "           4       0.95      0.95      0.95      5000\n",
      "           5       0.95      0.97      0.96      5000\n",
      "           6       0.95      0.94      0.94      5000\n",
      "           7       0.97      0.98      0.98      5000\n",
      "           8       1.00      0.98      0.99      5000\n",
      "           9       0.98      0.98      0.98      5000\n",
      "          10       0.98      0.98      0.98      5000\n",
      "          11       0.94      0.97      0.96      5000\n",
      "          12       0.95      0.95      0.95      5000\n",
      "          13       0.87      0.91      0.89      5000\n",
      "\n",
      "    accuracy                           0.95     70000\n",
      "   macro avg       0.95      0.95      0.95     70000\n",
      "weighted avg       0.95      0.95      0.95     70000\n",
      "\n",
      "confusion matrix:\n",
      "[[4312   44   92   19   38  151   57    7    1    4    7   58   44  166]\n",
      " [  52 4808   23    9   40    4   41    5    2    2    0    0    3   11]\n",
      " [  57   28 4163   14   93    1   11    1    0    4    5  193   79  351]\n",
      " [   4    2   36 4921   15   11    4    0    0    5    0    0    1    1]\n",
      " [  28   51   73   18 4775   19   13    1    0    1    1    0    3   17]\n",
      " [  95    1    2    4    3 4866   12    6    0    5    0    0    1    5]\n",
      " [  79   71   23    4   36   29 4690   52    5    1    0    0    2    8]\n",
      " [   4    3    3    0    5    6   43 4917    9    5    3    0    1    1]\n",
      " [   4    5    1    0    7    0   46   52 4882    2    1    0    0    0]\n",
      " [   3    0    5    7    0    2    1   12    0 4917   51    0    0    2]\n",
      " [  20    0    7    0    1    3    1    6    0   58 4903    0    0    1]\n",
      " [  11    0  100    1    0    1    2    0    0    0    1 4849   20   15]\n",
      " [   7    2   58   11    5   10    0    1    0    1    1   30 4771  103]\n",
      " [ 101   21  164   15   33   10   15    6    0    4    6   15   83 4527]]\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Done 200 out of 200 | elapsed:    2.2s finished\n"
     ]
    }
   ],
   "source": [
    "y_train, _, x_train = pickle.load(open(\"dbpedia_csv/\" + \"/train_lem_preprocessed_vectorized.pickle\", \"rb\"))\n",
    "#_, _, x_valid = pickle.load(open(paths.data + \"/validation_preprocessed.pickle\", \"rb\"))\n",
    "y_test, _, x_test = pickle.load(open(\"dbpedia_csv/\" + \"/test_lem_preprocessed_vectorized.pickle\", \"rb\"))\n",
    "\n",
    "y_train = y_train[slice(0, len(y_train), 4)]\n",
    "x_train = x_train[slice(0, len(x_train), 4)]\n",
    "\n",
    "\n",
    "\n",
    "## TRAIN AND FIT CLASSIFIER\n",
    "\n",
    "t = time()\n",
    "\n",
    "forest = RandomForestClassifier(bootstrap = False, n_estimators = 200, verbose=-1, n_jobs=-1, max_depth=40,  min_samples_split= 2, min_samples_leaf=1)\n",
    "\n",
    "forest = forest.fit(x_train, y_train)\n",
    "\n",
    "training_time = time() - t\n",
    "print(\"train time: %0.3fs\" % training_time)\n",
    "\n",
    "\n",
    "t = time()\n",
    "y_pred = forest.predict(x_test)\n",
    "\n",
    "test_time = time() - t\n",
    "print(\"test time:  %0.3fs\" % test_time)\n",
    "\n",
    "# compute the performance measures\n",
    "score1 = metrics.accuracy_score(y_test, y_pred)\n",
    "print(\"accuracy:   %0.3f\" % score1)\n",
    "\n",
    "print(metrics.classification_report(y_test, y_pred, target_names=[str(x) for x in range(14)]))\n",
    "\n",
    "print(\"confusion matrix:\")\n",
    "print(metrics.confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print('------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20fdadc5",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeca3f34",
   "metadata": {},
   "source": [
    "### full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd5a3373",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to extract features from training data : 14.677125 seconds\n",
      "n_samples: 560000, n_features: 698699\n",
      "Time taken to extract features from test data : 1.621698 seconds\n",
      "n_samples: 70000, n_features: 698699\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  56 tasks      | elapsed:    5.1s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:    8.7s finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train time: 9.826s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Done  56 tasks      | elapsed:    0.4s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test time:  0.848s\n",
      "accuracy:   0.885\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.74      0.80      5000\n",
      "           1       0.84      0.91      0.88      5000\n",
      "           2       0.89      0.58      0.70      5000\n",
      "           3       0.91      0.95      0.93      5000\n",
      "           4       0.91      0.90      0.91      5000\n",
      "           5       0.89      0.95      0.92      5000\n",
      "           6       0.91      0.83      0.87      5000\n",
      "           7       0.92      0.95      0.93      5000\n",
      "           8       0.92      0.99      0.95      5000\n",
      "           9       0.85      0.83      0.84      5000\n",
      "          10       0.86      0.93      0.89      5000\n",
      "          11       0.87      0.98      0.92      5000\n",
      "          12       0.90      0.96      0.93      5000\n",
      "          13       0.84      0.89      0.86      5000\n",
      "\n",
      "    accuracy                           0.89     70000\n",
      "   macro avg       0.89      0.89      0.88     70000\n",
      "weighted avg       0.89      0.89      0.88     70000\n",
      "\n",
      "confusion matrix:\n",
      "[[3690  152   92   77   67  225   94   49   44   62   16  201   92  139]\n",
      " [  80 4569   20   30   38   29   98   29   76   14    4    3    4    6]\n",
      " [  67  157 2913  178  162   48   57   22   19  150   11  452  238  526]\n",
      " [   6   16   42 4775   21   35    2   15    4   65    4    7    3    5]\n",
      " [  45  132   53   52 4501   70   24   17   15   58    9    5    6   13]\n",
      " [  79   19    7    8   17 4747   25   25    5   26    4    5   16   17]\n",
      " [ 159  258   19   16   57   82 4126  109   99   32   11    5   11   16]\n",
      " [   2   31    5    5    7   15   54 4734  117   10   12    2    0    6]\n",
      " [   3    8    4    1    4    1   12   27 4935    2    1    0    2    0]\n",
      " [   5    4   26   25    6   30    7   61   15 4168  640    3    1    9]\n",
      " [   9   10    2    2    5   11    7   33   10  264 4636    2    2    7]\n",
      " [   3    0   22    6    3    6    0    1    0    4    0 4917   26   12]\n",
      " [   6    5   16   31    6    5    0    2    5   11    2   33 4793   85]\n",
      " [  67   51   59   48   49   38   17    6   20   31   12   40  115 4447]]\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Done 100 out of 100 | elapsed:    0.7s finished\n"
     ]
    }
   ],
   "source": [
    "y_train, _, x_train = pickle.load(open(\"dbpedia_csv/\" + \"/train_preprocessed.pickle\", \"rb\"))\n",
    "#_, _, x_valid = pickle.load(open(paths.data + \"/validation_preprocessed.pickle\", \"rb\"))\n",
    "y_test, _, x_test = pickle.load(open(\"dbpedia_csv/\" + \"/test_preprocessed.pickle\", \"rb\"))\n",
    "\n",
    "\n",
    "## TFIDF VECTORIZATION\n",
    "\n",
    "t = time()  # not compulsory\n",
    "\n",
    "# loading CountVectorizer\n",
    "tf_vectorizer = TfidfVectorizer() # or term frequency\n",
    "\n",
    "x_train = tf_vectorizer.fit_transform(x_train)\n",
    "\n",
    "duration = time() - t\n",
    "print(\"Time taken to extract features from training data : %f seconds\" % (duration))\n",
    "print(\"n_samples: %d, n_features: %d\" % x_train.shape)\n",
    "\n",
    "t = time()\n",
    "x_test = tf_vectorizer.transform(x_test)\n",
    "\n",
    "duration = time() - t\n",
    "print(\"Time taken to extract features from test data : %f seconds\" % (duration))\n",
    "print(\"n_samples: %d, n_features: %d\" % x_test.shape)\n",
    "\n",
    "\n",
    "## TRAIN AND FIT CLASSIFIER\n",
    "\n",
    "t = time()\n",
    "\n",
    "\n",
    "forest = RandomForestClassifier(bootstrap = False, n_estimators = 100, verbose=-1, n_jobs=-1, max_depth=15)\n",
    "forest = forest.fit(x_train, y_train)\n",
    "\n",
    "training_time = time() - t\n",
    "print(\"train time: %0.3fs\" % training_time)\n",
    "\n",
    "\n",
    "t = time()\n",
    "y_pred = forest.predict(x_test)\n",
    "\n",
    "test_time = time() - t\n",
    "print(\"test time:  %0.3fs\" % test_time)\n",
    "\n",
    "# compute the performance measures\n",
    "score1 = metrics.accuracy_score(y_test, y_pred)\n",
    "print(\"accuracy:   %0.3f\" % score1)\n",
    "\n",
    "print(metrics.classification_report(y_test, y_pred, target_names=[str(x) for x in range(14)]))\n",
    "\n",
    "print(\"confusion matrix:\")\n",
    "print(metrics.confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print('------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9623ebe4",
   "metadata": {},
   "source": [
    "### half of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "607d48bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to extract features from training data : 7.142597 seconds\n",
      "n_samples: 280000, n_features: 445084\n",
      "Time taken to extract features from test data : 1.608977 seconds\n",
      "n_samples: 70000, n_features: 445084\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  56 tasks      | elapsed:    2.8s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:    4.7s finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train time: 5.132s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Done  56 tasks      | elapsed:    0.3s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test time:  0.722s\n",
      "accuracy:   0.880\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.82      0.83      5000\n",
      "           1       0.88      0.92      0.90      5000\n",
      "           2       0.87      0.62      0.72      5000\n",
      "           3       0.91      0.96      0.94      5000\n",
      "           4       0.91      0.91      0.91      5000\n",
      "           5       0.90      0.90      0.90      5000\n",
      "           6       0.92      0.83      0.87      5000\n",
      "           7       0.85      0.93      0.89      5000\n",
      "           8       0.92      0.96      0.94      5000\n",
      "           9       0.87      0.83      0.85      5000\n",
      "          10       0.86      0.88      0.87      5000\n",
      "          11       0.85      0.99      0.91      5000\n",
      "          12       0.89      0.97      0.93      5000\n",
      "          13       0.85      0.82      0.83      5000\n",
      "\n",
      "    accuracy                           0.88     70000\n",
      "   macro avg       0.88      0.88      0.88     70000\n",
      "weighted avg       0.88      0.88      0.88     70000\n",
      "\n",
      "confusion matrix:\n",
      "[[4088  110   74   73   47  114   97   48   24   13   18  130   50  114]\n",
      " [ 115 4581   10    8   46   28   80   49   46    5    2    0    6   24]\n",
      " [  74   96 3105  139  151   39   47   79   11   23   15  574  212  435]\n",
      " [   8   17   50 4808   26   12    2   44    1    7    5    7    2   11]\n",
      " [  31   80   58   56 4571   62   15   56   14    9    3    3   14   28]\n",
      " [ 221   13   35   35   23 4479   24   48    9   51   12   12   11   27]\n",
      " [ 129  183   12   22   59   81 4154  196  108   17    7    2   16   14]\n",
      " [  14   26    6    3    8   17   43 4647  189   23   17    3    0    4]\n",
      " [   4   17    2    1   11   15   27  132 4780    8    2    0    0    1]\n",
      " [  20    2   21   63    7   16    6   81    8 4131  622    8    0   15]\n",
      " [  19    7   15    8    4    9    7   57    6  414 4423    6    5   20]\n",
      " [   3    2   18    4    2    4    3    1    1    1    1 4934   26    0]\n",
      " [   9   21   21   14   10    7    4    6    0    5    1   38 4834   30]\n",
      " [ 128   62  140   38   71   73   22   29   21   19   20   70  227 4080]]\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Done 100 out of 100 | elapsed:    0.6s finished\n"
     ]
    }
   ],
   "source": [
    "y_train, _, x_train = pickle.load(open(\"dbpedia_csv/\" + \"/train_preprocessed.pickle\", \"rb\"))\n",
    "#_, _, x_valid = pickle.load(open(paths.data + \"/validation_preprocessed.pickle\", \"rb\"))\n",
    "y_test, _, x_test = pickle.load(open(\"dbpedia_csv/\" + \"/test_preprocessed.pickle\", \"rb\"))\n",
    "\n",
    "\n",
    "y_train = y_train[slice(0, len(y_train), 2)]\n",
    "x_train = x_train[slice(0, len(x_train), 2)]\n",
    "\n",
    "\n",
    "## TFIDF VECTORIZATION\n",
    "\n",
    "t = time()  # not compulsory\n",
    "\n",
    "# loading CountVectorizer\n",
    "tf_vectorizer = TfidfVectorizer() # or term frequency\n",
    "\n",
    "x_train = tf_vectorizer.fit_transform(x_train)\n",
    "\n",
    "duration = time() - t\n",
    "print(\"Time taken to extract features from training data : %f seconds\" % (duration))\n",
    "print(\"n_samples: %d, n_features: %d\" % x_train.shape)\n",
    "\n",
    "t = time()\n",
    "x_test = tf_vectorizer.transform(x_test)\n",
    "\n",
    "duration = time() - t\n",
    "print(\"Time taken to extract features from test data : %f seconds\" % (duration))\n",
    "print(\"n_samples: %d, n_features: %d\" % x_test.shape)\n",
    "\n",
    "\n",
    "## TRAIN AND FIT CLASSIFIER\n",
    "\n",
    "t = time()\n",
    "\n",
    "\n",
    "forest = RandomForestClassifier(bootstrap = False, n_estimators = 100, verbose=-1, n_jobs=-1, max_depth=15)\n",
    "forest = forest.fit(x_train, y_train)\n",
    "\n",
    "training_time = time() - t\n",
    "print(\"train time: %0.3fs\" % training_time)\n",
    "\n",
    "\n",
    "t = time()\n",
    "y_pred = forest.predict(x_test)\n",
    "\n",
    "test_time = time() - t\n",
    "print(\"test time:  %0.3fs\" % test_time)\n",
    "\n",
    "# compute the performance measures\n",
    "score1 = metrics.accuracy_score(y_test, y_pred)\n",
    "print(\"accuracy:   %0.3f\" % score1)\n",
    "\n",
    "print(metrics.classification_report(y_test, y_pred, target_names=[str(x) for x in range(14)]))\n",
    "\n",
    "print(\"confusion matrix:\")\n",
    "print(metrics.confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print('------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341098b7",
   "metadata": {},
   "source": [
    "### one fourth of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "301c9b95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to extract features from training data : 3.604377 seconds\n",
      "n_samples: 140000, n_features: 282474\n",
      "Time taken to extract features from test data : 1.477304 seconds\n",
      "n_samples: 70000, n_features: 282474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  56 tasks      | elapsed:    1.2s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:    2.2s finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train time: 2.388s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Done  56 tasks      | elapsed:    0.3s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test time:  0.623s\n",
      "accuracy:   0.880\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.76      0.81      5000\n",
      "           1       0.85      0.92      0.88      5000\n",
      "           2       0.86      0.56      0.68      5000\n",
      "           3       0.94      0.96      0.95      5000\n",
      "           4       0.91      0.90      0.91      5000\n",
      "           5       0.90      0.91      0.90      5000\n",
      "           6       0.89      0.82      0.85      5000\n",
      "           7       0.87      0.95      0.91      5000\n",
      "           8       0.92      0.98      0.95      5000\n",
      "           9       0.94      0.78      0.85      5000\n",
      "          10       0.83      0.95      0.88      5000\n",
      "          11       0.83      0.98      0.90      5000\n",
      "          12       0.88      0.98      0.93      5000\n",
      "          13       0.85      0.87      0.86      5000\n",
      "\n",
      "    accuracy                           0.88     70000\n",
      "   macro avg       0.88      0.88      0.88     70000\n",
      "weighted avg       0.88      0.88      0.88     70000\n",
      "\n",
      "confusion matrix:\n",
      "[[3799  149   72   47   51  184  120   54   31   17   19  212  100  145]\n",
      " [  39 4595    9   22   30   10  120   38  115    5    3    1    8    5]\n",
      " [  69  129 2804  116  187   46   65  101   23   21   26  650  289  474]\n",
      " [   5   11   63 4803   19   17   16   39    2    4    5    3    2   11]\n",
      " [  32  100   80   39 4524   60   16   56   25   10    6   11    8   33]\n",
      " [ 182   15   19   12   25 4543   46   54   11   29   10    8   14   32]\n",
      " [ 110  268   23   10   59   87 4123  193   77   12    5    5   12   16]\n",
      " [   8   36    6    5   12    6   80 4734   70   22   12    3    2    4]\n",
      " [   5   17    3    1    6    7   18   48 4892    1    2    0    0    0]\n",
      " [  18    1   36   36    6   13   11   68   14 3888  892    6    1   10]\n",
      " [  13   14   20    3    1    4    9   28   14  125 4742   12    2   13]\n",
      " [   2    0    8    4    3    5    1    2    0    1    0 4925   41    8]\n",
      " [   5    4    8    7    5    8    2    4    4    5    1   32 4883   32]\n",
      " [  65   67   91   31   47   66   28   17   15    8   10   41  187 4327]]\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Done 100 out of 100 | elapsed:    0.5s finished\n"
     ]
    }
   ],
   "source": [
    "y_train, _, x_train = pickle.load(open(\"dbpedia_csv/\" + \"/train_preprocessed.pickle\", \"rb\"))\n",
    "#_, _, x_valid = pickle.load(open(paths.data + \"/validation_preprocessed.pickle\", \"rb\"))\n",
    "y_test, _, x_test = pickle.load(open(\"dbpedia_csv/\" + \"/test_preprocessed.pickle\", \"rb\"))\n",
    "\n",
    "\n",
    "y_train = y_train[slice(0, len(y_train), 4)]\n",
    "x_train = x_train[slice(0, len(x_train), 4)]\n",
    "\n",
    "\n",
    "## TFIDF VECTORIZATION\n",
    "\n",
    "t = time()  # not compulsory\n",
    "\n",
    "# loading CountVectorizer\n",
    "tf_vectorizer = TfidfVectorizer() # or term frequency\n",
    "\n",
    "x_train = tf_vectorizer.fit_transform(x_train)\n",
    "\n",
    "duration = time() - t\n",
    "print(\"Time taken to extract features from training data : %f seconds\" % (duration))\n",
    "print(\"n_samples: %d, n_features: %d\" % x_train.shape)\n",
    "\n",
    "t = time()\n",
    "x_test = tf_vectorizer.transform(x_test)\n",
    "\n",
    "duration = time() - t\n",
    "print(\"Time taken to extract features from test data : %f seconds\" % (duration))\n",
    "print(\"n_samples: %d, n_features: %d\" % x_test.shape)\n",
    "\n",
    "\n",
    "## TRAIN AND FIT CLASSIFIER\n",
    "\n",
    "t = time()\n",
    "\n",
    "\n",
    "forest = RandomForestClassifier(bootstrap = False, n_estimators = 100, verbose=-1, n_jobs=-1, max_depth=15)\n",
    "forest = forest.fit(x_train, y_train)\n",
    "\n",
    "training_time = time() - t\n",
    "print(\"train time: %0.3fs\" % training_time)\n",
    "\n",
    "\n",
    "t = time()\n",
    "y_pred = forest.predict(x_test)\n",
    "\n",
    "test_time = time() - t\n",
    "print(\"test time:  %0.3fs\" % test_time)\n",
    "\n",
    "# compute the performance measures\n",
    "score1 = metrics.accuracy_score(y_test, y_pred)\n",
    "print(\"accuracy:   %0.3f\" % score1)\n",
    "\n",
    "print(metrics.classification_report(y_test, y_pred, target_names=[str(x) for x in range(14)]))\n",
    "\n",
    "print(\"confusion matrix:\")\n",
    "print(metrics.confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print('------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5185ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
