{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6d1d59c",
   "metadata": {},
   "source": [
    "### neural network implemented with word2vec only because tf-idf is a sparse matrix and we can't use it to train the nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b97f8008",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import string\n",
    "from re import sub\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# from keras.preprocessing.text import Tokenizer\n",
    "# from keras.preprocessing.sequence import pad_sequences\n",
    "# from keras.utils.np_utils import to_categorical\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "\n",
    "from keras.layers import Input\n",
    "\n",
    "import pickle\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import pandas\n",
    "import numpy\n",
    "\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "from time import time\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5b31873",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device mapping:\n",
      "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: NVIDIA GeForce GTX 1050 with Max-Q Design, pci bus id: 0000:02:00.0, compute capability: 6.1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(log_device_placement=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973f7517",
   "metadata": {},
   "source": [
    "### one fourth of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb1a569d",
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = EarlyStopping(monitor='val_loss',patience=2, min_delta=0.0001)\n",
    "\n",
    "# checkpoint = ModelCheckpoint(\"best_model_conv\", monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "reduce = ReduceLROnPlateau(monitor='val_loss', patience=2, verbose=1, factor=0.5, min_lr=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b551847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14000, 14)\n",
      "(70000, 14)\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 128)               128128    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 32)                4128      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 14)                462       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 132,718\n",
      "Trainable params: 132,718\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "3938/3938 [==============================] - 25s 6ms/step - loss: 0.6872 - accuracy: 0.7786 - val_loss: 0.6140 - val_accuracy: 0.8979 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "3938/3938 [==============================] - 21s 5ms/step - loss: 0.3239 - accuracy: 0.8969 - val_loss: 0.3910 - val_accuracy: 0.9058 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "3938/3938 [==============================] - 21s 5ms/step - loss: 0.3068 - accuracy: 0.9031 - val_loss: 0.3110 - val_accuracy: 0.9129 - lr: 0.0010\n",
      "Epoch 4/10\n",
      "3938/3938 [==============================] - 22s 6ms/step - loss: 0.2921 - accuracy: 0.9090 - val_loss: 0.2995 - val_accuracy: 0.9073 - lr: 0.0010\n",
      "Epoch 5/10\n",
      "3938/3938 [==============================] - 21s 5ms/step - loss: 0.2776 - accuracy: 0.9128 - val_loss: 0.3101 - val_accuracy: 0.9020 - lr: 0.0010\n",
      "Epoch 6/10\n",
      "3938/3938 [==============================] - 21s 5ms/step - loss: 0.2654 - accuracy: 0.9174 - val_loss: 0.2767 - val_accuracy: 0.9124 - lr: 0.0010\n",
      "Epoch 7/10\n",
      "3938/3938 [==============================] - 21s 5ms/step - loss: 0.2507 - accuracy: 0.9229 - val_loss: 0.2941 - val_accuracy: 0.9054 - lr: 0.0010\n",
      "Epoch 8/10\n",
      "3931/3938 [============================>.] - ETA: 0s - loss: 0.2509 - accuracy: 0.9226\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "3938/3938 [==============================] - 21s 5ms/step - loss: 0.2510 - accuracy: 0.9226 - val_loss: 0.3606 - val_accuracy: 0.8811 - lr: 0.0010\n",
      "train time: 175.463s\n",
      "2188/2188 [==============================] - 4s 2ms/step\n",
      "test time:  6.232s\n",
      "[[9.9562114e-01 3.5017813e-04 3.3307378e-04 ... 3.2391511e-06\n",
      "  7.3256915e-06 4.3162928e-04]\n",
      " [1.4647403e-01 2.6889900e-06 5.7735141e-02 ... 7.1805869e-03\n",
      "  7.2363192e-01 6.4416356e-02]\n",
      " [9.9577254e-01 5.8625988e-04 5.8772483e-05 ... 2.2601230e-07\n",
      "  1.3048837e-06 2.7732109e-04]\n",
      " ...\n",
      " [1.2450762e-03 4.9089003e-06 2.3141608e-02 ... 2.7669150e-05\n",
      "  2.0467272e-05 9.7505730e-01]\n",
      " [1.0962795e-05 2.9004635e-10 3.2348032e-06 ... 8.4638359e-11\n",
      "  5.3307103e-10 9.9998569e-01]\n",
      " [6.1533144e-03 6.9378652e-06 2.2701532e-01 ... 3.8663347e-03\n",
      "  1.8125579e-03 7.5998956e-01]]\n",
      "accuracy:   0.940\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.83      0.88      5000\n",
      "           1       0.98      0.95      0.96      5000\n",
      "           2       0.74      0.93      0.83      5000\n",
      "           3       0.98      0.98      0.98      5000\n",
      "           4       0.94      0.95      0.95      5000\n",
      "           5       0.96      0.98      0.97      5000\n",
      "           6       0.92      0.97      0.94      5000\n",
      "           7       0.99      0.96      0.97      5000\n",
      "           8       1.00      0.96      0.98      5000\n",
      "           9       0.95      0.99      0.97      5000\n",
      "          10       0.99      0.96      0.97      5000\n",
      "          11       0.96      0.95      0.95      5000\n",
      "          12       0.97      0.93      0.95      5000\n",
      "          13       0.91      0.83      0.87      5000\n",
      "\n",
      "    accuracy                           0.94     70000\n",
      "   macro avg       0.94      0.94      0.94     70000\n",
      "weighted avg       0.94      0.94      0.94     70000\n",
      "\n",
      "confusion matrix:\n",
      "[[4173   41  189   19   50  152  119    2    0    2    5   48   31  169]\n",
      " [  53 4729   50    9   79    2   66    3    1    0    1    0    0    7]\n",
      " [  21    3 4652   11   55    1    6    0    0    0    2  109   26  114]\n",
      " [   0    1   67 4898   21    7    2    0    0    3    0    0    0    1]\n",
      " [  15   12  184   10 4747   12   11    1    0    1    0    0    0    7]\n",
      " [  51    0    7    4    7 4898   15    2    0    8    0    0    2    6]\n",
      " [  30   41   30    1   20   23 4828   13    2    3    0    0    3    6]\n",
      " [   5    1   14    0    2   16  100 4780    8   64    8    0    0    2]\n",
      " [   2   15    2    0   12    0  101   43 4823    1    0    0    0    1]\n",
      " [   0    0   11    9    0    0    0    0    0 4943   33    0    0    4]\n",
      " [  12    0    8    0    0    1    1    4    0  184 4783    1    0    6]\n",
      " [   2    0  219    1    0    1    0    0    0    0    0 4756   16    5]\n",
      " [   6    0  192    4   11   11    1    1    0    1    0   38 4665   70]\n",
      " [  68    6  645   15   21    4    7    0    0    2    3   14   56 4159]]\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "y_train, _, x_train = pickle.load(open(\"dbpedia_csv/\" + \"/train_lem_preprocessed_vectorized.pickle\", \"rb\"))\n",
    "#_, _, x_valid = pickle.load(open(paths.data + \"/validation_preprocessed.pickle\", \"rb\"))\n",
    "y_test, _, x_test = pickle.load(open(\"dbpedia_csv/\" + \"/test_lem_preprocessed_vectorized.pickle\", \"rb\"))\n",
    "\n",
    "\n",
    "y_train = y_train[slice(0, len(y_train), 4)]\n",
    "x_train = x_train[slice(0, len(x_train), 4)]\n",
    "\n",
    "\n",
    "x_train_temp = pd.DataFrame(x_train).to_numpy()\n",
    "y_train_temp = []\n",
    "\n",
    "\n",
    "x_valid = []\n",
    "y_valid = []\n",
    "\n",
    "x_train = []\n",
    "y_train_np = []\n",
    "\n",
    "\n",
    "for i in range(len(y_train)):\n",
    "    if i % 10 == 0:\n",
    "        y_valid.append(y_train[i])\n",
    "        x_valid.append(x_train_temp[i])\n",
    "    else:\n",
    "        y_train_np.append(y_train[i])\n",
    "        x_train.append(x_train_temp[i])\n",
    "        \n",
    "y_train = y_train_np\n",
    "x_train = np.array(x_train)\n",
    "x_valid = np.array(x_valid)\n",
    "    \n",
    "\n",
    "\n",
    "# x_test_1 = x_test[slice(0, len(x_test), 2)]\n",
    "# x_test_2 = x_test[slice(1, len(x_test), 2)]\n",
    "# y_test_1 = y_test[slice(0, len(y_test), 2)]\n",
    "# y_test_2 = y_test[slice(1, len(y_test), 2)]\n",
    "\n",
    "\n",
    "# y_train_valid = y_train[slice(0, len(y_train), 10)]\n",
    "# x_train_valid = x_train[slice(0, len(x_train), 10)]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # list1 = [ elem for i,elem in enumerate(list1) if elem % 2 != 0]\n",
    "# x_train = [e for i, e in enumerate(x_train) if i % 10 != 0]\n",
    "# y_train = [e for i, e in enumerate(y_train) if i % 10 != 0]\n",
    "\n",
    "# x_train_temp = [][]\n",
    "# y_train_temp = []\n",
    "\n",
    "# for i in range(1, 10):\n",
    "#     x_train_temp += x_train[slice(i, len(x_train), 10)]\n",
    "#     y_train_temp.append(y_train[i])\n",
    "        \n",
    "\n",
    "# print(len(x_train))\n",
    "\n",
    "\n",
    "y_train_valid_np = []\n",
    "\n",
    "for y in y_valid:\n",
    "    l = [0 for x in range(14)]\n",
    "    l[y-1] = 1\n",
    "    y_train_valid_np.append(l)\n",
    "    \n",
    "y_valid = np.asarray(y_train_valid_np)\n",
    "print(y_valid.shape)\n",
    "\n",
    "\n",
    "\n",
    "y_test_np = []\n",
    "\n",
    "for y in y_test:\n",
    "    l = [0 for x in range(14)]\n",
    "    l[y-1] = 1\n",
    "    y_test_np.append(l)\n",
    "    \n",
    "y_test = np.asarray(y_test_np)\n",
    "print(y_test.shape)\n",
    "\n",
    "\n",
    "y_train_np = []\n",
    "\n",
    "for y in y_train:\n",
    "    l = [0 for x in range(14)]\n",
    "    l[y-1] = 1\n",
    "    y_train_np.append(l)\n",
    "    \n",
    "y_train = np.asarray(y_train_np)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "PredictorScaler=MinMaxScaler()\n",
    "\n",
    "# Storing the fit object for later reference\n",
    "PredictorScalerFit=PredictorScaler.fit(x_train)\n",
    "\n",
    "\n",
    "x_train = PredictorScalerFit.transform(x_train)\n",
    "x_train_valid = PredictorScalerFit.transform(x_valid)\n",
    "x_test = PredictorScalerFit.transform(x_test)\n",
    "\n",
    "\n",
    "del y_train_np\n",
    "del y_test_np\n",
    "del y_train_valid_np\n",
    "del x_train_temp\n",
    "del PredictorScalerFit\n",
    "del PredictorScaler\n",
    "import sys\n",
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "## TRAIN AND FIT CLASSIFIER\n",
    "\n",
    "t = time()\n",
    "\n",
    "model_l1 = Sequential()\n",
    "# model_l1.add(Embedding(56000, 1000))\n",
    "model_l1.add(Input(shape=(1000,), name='input'))\n",
    "# model_l1.add(Conv1D(128, 5, activation='relu'))\n",
    "# model_l1.add(GlobalMaxPooling1D()) # added\n",
    "model_l1.add(Dense(128, activation='relu'))\n",
    "model_l1.add(Dropout(0.1))\n",
    "model_l1.add(Dense(32, activation='relu'))\n",
    "model_l1.add(Dense(14, activation='softmax')) \n",
    "model_l1.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model_l1.summary()\n",
    "\n",
    "history_l1 = model_l1.fit(x_train, y_train, epochs=10, verbose=1, shuffle=True, use_multiprocessing=True, \n",
    "                   validation_data=(x_valid, y_valid), batch_size=32\n",
    "                   ,callbacks=[reduce,callback])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "training_time = time() - t\n",
    "print(\"train time: %0.3fs\" % training_time)\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "t = time()\n",
    "y_pred = model_l1.predict(x_test)\n",
    "\n",
    "test_time = time() - t\n",
    "print(\"test time:  %0.3fs\" % test_time)\n",
    "\n",
    "print(y_pred)\n",
    "\n",
    "y_pred=np.argmax(y_pred, axis=1)\n",
    "y_test=np.argmax(y_test, axis=1)\n",
    "# cm = confusion_matrix(y_test, y_pred)\n",
    "# print(cm)\n",
    "\n",
    "# compute the performance measures\n",
    "score1 = metrics.accuracy_score(y_test, y_pred)\n",
    "print(\"accuracy:   %0.3f\" % score1)\n",
    "\n",
    "print(metrics.classification_report(y_test, y_pred, target_names=[str(x) for x in range(14)]))\n",
    "\n",
    "print(\"confusion matrix:\")\n",
    "print(metrics.confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print('------------------------------')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05308b18",
   "metadata": {},
   "source": [
    "### half of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c5c2242",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70000, 14)\n",
      "(252000, 14)\n",
      "(28000, 14)\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_3 (Dense)             (None, 128)               128128    \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 32)                4128      \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 14)                462       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 132,718\n",
      "Trainable params: 132,718\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "7875/7875 [==============================] - 42s 5ms/step - loss: 0.7235 - accuracy: 0.7563 - val_loss: 0.2967 - val_accuracy: 0.9103 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "7875/7875 [==============================] - 40s 5ms/step - loss: 0.3695 - accuracy: 0.8813 - val_loss: 0.2276 - val_accuracy: 0.9310 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "7875/7875 [==============================] - 40s 5ms/step - loss: 0.3198 - accuracy: 0.8978 - val_loss: 0.1986 - val_accuracy: 0.9403 - lr: 0.0010\n",
      "Epoch 4/10\n",
      "7875/7875 [==============================] - 40s 5ms/step - loss: 0.3146 - accuracy: 0.9006 - val_loss: 0.2788 - val_accuracy: 0.9134 - lr: 0.0010\n",
      "Epoch 5/10\n",
      "7865/7875 [============================>.] - ETA: 0s - loss: 0.2810 - accuracy: 0.9116\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "7875/7875 [==============================] - 41s 5ms/step - loss: 0.2810 - accuracy: 0.9116 - val_loss: 0.2866 - val_accuracy: 0.9112 - lr: 0.0010\n",
      "train time: 205.640s\n",
      "2188/2188 [==============================] - 3s 1ms/step\n",
      "test time:  4.956s\n",
      "accuracy:   0.911\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.77      0.84      5000\n",
      "           1       0.88      0.98      0.92      5000\n",
      "           2       0.80      0.78      0.79      5000\n",
      "           3       0.97      0.95      0.96      5000\n",
      "           4       0.80      0.98      0.88      5000\n",
      "           5       0.90      0.98      0.94      5000\n",
      "           6       0.90      0.91      0.91      5000\n",
      "           7       0.95      0.96      0.95      5000\n",
      "           8       0.98      0.98      0.98      5000\n",
      "           9       0.87      0.97      0.92      5000\n",
      "          10       1.00      0.82      0.90      5000\n",
      "          11       0.98      0.88      0.93      5000\n",
      "          12       0.96      0.92      0.94      5000\n",
      "          13       0.88      0.87      0.87      5000\n",
      "\n",
      "    accuracy                           0.91     70000\n",
      "   macro avg       0.91      0.91      0.91     70000\n",
      "weighted avg       0.91      0.91      0.91     70000\n",
      "\n",
      "confusion matrix:\n",
      "[[3831  305  141   14   99  266  139    4    3    1    0    9   31  157]\n",
      " [  15 4884    8    0   73    4   15    0    0    0    0    0    0    1]\n",
      " [  40   53 3917   46  512    8   11    2    0    0    0   72   42  297]\n",
      " [   0    6   11 4755  210   12    1    1    3    1    0    0    0    0]\n",
      " [   5   50   23   10 4894    8    9    1    0    0    0    0    0    0]\n",
      " [  27    0    0    2   25 4921   18    3    0    3    0    0    0    1]\n",
      " [  18  197    9    0  121   42 4555   31   20    0    0    0    2    5]\n",
      " [   0    2    0    0   33   23   98 4790   53    1    0    0    0    0]\n",
      " [   1   26    1    0   25    1   30   18 4898    0    0    0    0    0]\n",
      " [   1    0    1   17   11   32    0   58    9 4854   17    0    0    0]\n",
      " [  27    1   15    1    3    8    1  137    1  699 4105    0    1    1]\n",
      " [   5    0  453    2    0   20   58    0    0    0    0 4394   48   20]\n",
      " [  21    2  119   11   12   22   64   16    0    3    0   13 4610  107]\n",
      " [  96   38  175   23  107   84   66    3    0    1    0    4   68 4335]]\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "y_train, _, x_train = pickle.load(open(\"dbpedia_csv/\" + \"/train_lem_preprocessed_vectorized.pickle\", \"rb\"))\n",
    "#_, _, x_valid = pickle.load(open(paths.data + \"/validation_preprocessed.pickle\", \"rb\"))\n",
    "y_test, _, x_test = pickle.load(open(\"dbpedia_csv/\" + \"/test_lem_preprocessed_vectorized.pickle\", \"rb\"))\n",
    "\n",
    "y_train = y_train[slice(0, len(y_train), 2)]\n",
    "x_train = x_train[slice(0, len(x_train), 2)]\n",
    "\n",
    "\n",
    "x_train_temp = pd.DataFrame(x_train).to_numpy()\n",
    "y_train_temp = []\n",
    "\n",
    "\n",
    "x_valid = []\n",
    "y_valid = []\n",
    "\n",
    "x_train = []\n",
    "y_train_np = []\n",
    "\n",
    "\n",
    "for i in range(len(y_train)):\n",
    "    if i % 10 == 0:\n",
    "        y_valid.append(y_train[i])\n",
    "        x_valid.append(x_train_temp[i])\n",
    "    else:\n",
    "        y_train_np.append(y_train[i])\n",
    "        x_train.append(x_train_temp[i])\n",
    "        \n",
    "y_train = y_train_np\n",
    "x_train = np.array(x_train)\n",
    "x_valid = np.array(x_valid)\n",
    "    \n",
    "\n",
    "\n",
    "y_test_np = []\n",
    "\n",
    "for y in y_test:\n",
    "    l = [0 for x in range(14)]\n",
    "    l[y-1] = 1\n",
    "    y_test_np.append(l)\n",
    "    \n",
    "y_test = np.asarray(y_test_np)\n",
    "print(y_test.shape)\n",
    "\n",
    "y_train_np = []\n",
    "\n",
    "for y in y_train:\n",
    "    l = [0 for x in range(14)]\n",
    "    l[y-1] = 1\n",
    "    y_train_np.append(l)\n",
    "    \n",
    "y_train = np.asarray(y_train_np)\n",
    "print(y_train.shape)\n",
    "\n",
    "\n",
    "\n",
    "y_train_valid_np = []\n",
    "\n",
    "for y in y_valid:\n",
    "    l = [0 for x in range(14)]\n",
    "    l[y-1] = 1\n",
    "    y_train_valid_np.append(l)\n",
    "    \n",
    "y_valid = np.asarray(y_train_valid_np)\n",
    "print(y_valid.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "PredictorScaler=MinMaxScaler()\n",
    "\n",
    "# Storing the fit object for later reference\n",
    "PredictorScalerFit=PredictorScaler.fit(x_train)\n",
    "\n",
    "\n",
    "x_train = PredictorScalerFit.transform(x_train)\n",
    "x_test = PredictorScalerFit.transform(x_test)\n",
    "x_valid = PredictorScalerFit.transform(x_valid)\n",
    "\n",
    "\n",
    "del y_train_np\n",
    "del y_test_np\n",
    "del y_train_valid_np\n",
    "del x_train_temp\n",
    "del PredictorScalerFit\n",
    "del PredictorScaler\n",
    "import sys\n",
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "## TRAIN AND FIT CLASSIFIER\n",
    "\n",
    "t = time()\n",
    "\n",
    "model_l1 = Sequential()\n",
    "# model_l1.add(Embedding(56000, 1000))\n",
    "model_l1.add(Input(shape=(1000,), name='input'))\n",
    "# model_l1.add(Conv1D(128, 5, activation='relu'))\n",
    "# model_l1.add(GlobalMaxPooling1D()) # added\n",
    "model_l1.add(Dense(128, activation='relu'))\n",
    "model_l1.add(Dropout(0.1))\n",
    "model_l1.add(Dense(32, activation='relu'))\n",
    "model_l1.add(Dense(14, activation='softmax')) \n",
    "model_l1.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model_l1.summary()\n",
    "\n",
    "history_l1 = model_l1.fit(x_train, y_train, epochs=10, verbose=1, use_multiprocessing=True,\n",
    "                   validation_data=(x_valid, y_valid),\n",
    "                    batch_size=32\n",
    "                   ,callbacks=[reduce,callback])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "training_time = time() - t\n",
    "print(\"train time: %0.3fs\" % training_time)\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "t = time()\n",
    "y_pred = model_l1.predict(x_test)\n",
    "\n",
    "\n",
    "\n",
    "test_time = time() - t\n",
    "print(\"test time:  %0.3fs\" % test_time)\n",
    "\n",
    "y_pred=np.argmax(y_pred, axis=1)\n",
    "y_test=np.argmax(y_test, axis=1)\n",
    "# cm = confusion_matrix(y_test, y_pred)\n",
    "# print(cm)\n",
    "\n",
    "# compute the performance measures\n",
    "score1 = metrics.accuracy_score(y_test, y_pred)\n",
    "print(\"accuracy:   %0.3f\" % score1)\n",
    "\n",
    "print(metrics.classification_report(y_test, y_pred, target_names=[str(x) for x in range(14)]))\n",
    "\n",
    "print(\"confusion matrix:\")\n",
    "print(metrics.confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print('------------------------------')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b541e80e",
   "metadata": {},
   "source": [
    "### full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7861f11d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70000, 14)\n",
      "(504000, 14)\n",
      "(56000, 14)\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_6 (Dense)             (None, 128)               128128    \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 32)                4128      \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 14)                462       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 132,718\n",
      "Trainable params: 132,718\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "15750/15750 [==============================] - 87s 5ms/step - loss: 0.5086 - accuracy: 0.8333 - val_loss: 0.2989 - val_accuracy: 0.9063 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "15750/15750 [==============================] - 81s 5ms/step - loss: 0.3079 - accuracy: 0.9033 - val_loss: 0.2168 - val_accuracy: 0.9343 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "15750/15750 [==============================] - 82s 5ms/step - loss: 0.2846 - accuracy: 0.9114 - val_loss: 0.1908 - val_accuracy: 0.9422 - lr: 0.0010\n",
      "Epoch 4/10\n",
      "15750/15750 [==============================] - 82s 5ms/step - loss: 0.2650 - accuracy: 0.9178 - val_loss: 0.1676 - val_accuracy: 0.9485 - lr: 0.0010\n",
      "Epoch 5/10\n",
      "15750/15750 [==============================] - 82s 5ms/step - loss: 0.2508 - accuracy: 0.9224 - val_loss: 0.2794 - val_accuracy: 0.9176 - lr: 0.0010\n",
      "Epoch 6/10\n",
      "15744/15750 [============================>.] - ETA: 0s - loss: 0.2415 - accuracy: 0.9256\n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "15750/15750 [==============================] - 81s 5ms/step - loss: 0.2415 - accuracy: 0.9256 - val_loss: 0.2109 - val_accuracy: 0.9346 - lr: 0.0010\n",
      "train time: 497.130s\n",
      "2188/2188 [==============================] - 7s 2ms/step\n",
      "test time:  8.555s\n",
      "accuracy:   0.934\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.91      0.87      5000\n",
      "           1       0.94      0.97      0.96      5000\n",
      "           2       0.95      0.72      0.82      5000\n",
      "           3       0.99      0.98      0.98      5000\n",
      "           4       0.94      0.97      0.95      5000\n",
      "           5       0.95      0.97      0.96      5000\n",
      "           6       0.90      0.91      0.90      5000\n",
      "           7       0.99      0.91      0.95      5000\n",
      "           8       1.00      0.95      0.98      5000\n",
      "           9       0.99      0.97      0.98      5000\n",
      "          10       0.99      0.97      0.98      5000\n",
      "          11       0.93      0.98      0.96      5000\n",
      "          12       0.97      0.91      0.94      5000\n",
      "          13       0.78      0.96      0.86      5000\n",
      "\n",
      "    accuracy                           0.93     70000\n",
      "   macro avg       0.94      0.93      0.93     70000\n",
      "weighted avg       0.94      0.93      0.93     70000\n",
      "\n",
      "confusion matrix:\n",
      "[[4565   44   17    6   35   92   31    1    0    0    1   29   13  166]\n",
      " [  71 4856    8    2   40    3   16    0    0    0    0    0    0    4]\n",
      " [ 131   50 3617   24  127    0    7    0    0    0    1  258   77  708]\n",
      " [  26    9   15 4899   25   19    0    0    0    3    0    0    1    3]\n",
      " [  32   44   24    9 4842    9    4    0    0    0    0    0    2   34]\n",
      " [ 131    0    0    1    1 4858    2    0    0    0    0    0    0    7]\n",
      " [ 259  125    7    0   24   30 4529    5    4    1    0    0    2   14]\n",
      " [ 155    5    0    0    3   21  254 4544    7    5    2    0    0    4]\n",
      " [   1   15    1    0   23    0  178    9 4769    0    2    0    0    2]\n",
      " [  13    0   11    6    0   44    5   14    0 4830   59    0    0   18]\n",
      " [  30    1    1    0    1   13   15    5    0   54 4850    0    0   30]\n",
      " [  19    0   25    1    0    0    0    0    0    0    0 4913    7   35]\n",
      " [  24    3   27   15    4    6    0    1    0    0    0   64 4548  308]\n",
      " [  74    4   49    5   33    3    5    0    0    0    1   14   28 4784]]\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "y_train, _, x_train = pickle.load(open(\"dbpedia_csv/\" + \"/train_lem_preprocessed_vectorized.pickle\", \"rb\"))\n",
    "#_, _, x_valid = pickle.load(open(paths.data + \"/validation_preprocessed.pickle\", \"rb\"))\n",
    "y_test, _, x_test = pickle.load(open(\"dbpedia_csv/\" + \"/test_lem_preprocessed_vectorized.pickle\", \"rb\"))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "x_train_temp = pd.DataFrame(x_train).to_numpy()\n",
    "y_train_temp = []\n",
    "\n",
    "\n",
    "x_valid = []\n",
    "y_valid = []\n",
    "\n",
    "x_train = []\n",
    "y_train_np = []\n",
    "\n",
    "\n",
    "for i in range(len(y_train)):\n",
    "    if i % 10 == 0:\n",
    "        y_valid.append(y_train[i])\n",
    "        x_valid.append(x_train_temp[i])\n",
    "    else:\n",
    "        y_train_np.append(y_train[i])\n",
    "        x_train.append(x_train_temp[i])\n",
    "        \n",
    "y_train = y_train_np\n",
    "x_train = np.array(x_train)\n",
    "x_valid = np.array(x_valid)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "y_test_np = []\n",
    "\n",
    "for y in y_test:\n",
    "    l = [0 for x in range(14)]\n",
    "    l[y-1] = 1\n",
    "    y_test_np.append(l)\n",
    "    \n",
    "y_test = np.asarray(y_test_np)\n",
    "print(y_test.shape)\n",
    "\n",
    "y_train_np = []\n",
    "\n",
    "for y in y_train:\n",
    "    l = [0 for x in range(14)]\n",
    "    l[y-1] = 1\n",
    "    y_train_np.append(l)\n",
    "    \n",
    "y_train = np.asarray(y_train_np)\n",
    "print(y_train.shape)\n",
    "\n",
    "\n",
    "y_train_valid_np = []\n",
    "\n",
    "for y in y_valid:\n",
    "    l = [0 for x in range(14)]\n",
    "    l[y-1] = 1\n",
    "    y_train_valid_np.append(l)\n",
    "    \n",
    "y_valid = np.asarray(y_train_valid_np)\n",
    "print(y_valid.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "PredictorScaler=MinMaxScaler()\n",
    "\n",
    "# Storing the fit object for later reference\n",
    "PredictorScalerFit=PredictorScaler.fit(x_train)\n",
    "\n",
    "\n",
    "x_train = PredictorScalerFit.transform(x_train)\n",
    "x_test = PredictorScalerFit.transform(x_test)\n",
    "x_valid = PredictorScalerFit.transform(x_valid)\n",
    "\n",
    "\n",
    "del y_train_np\n",
    "del y_test_np\n",
    "del y_train_valid_np\n",
    "del x_train_temp\n",
    "del PredictorScalerFit\n",
    "del PredictorScaler\n",
    "import sys\n",
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "## TRAIN AND FIT CLASSIFIER\n",
    "\n",
    "t = time()\n",
    "\n",
    "model_l1 = Sequential()\n",
    "# model_l1.add(Embedding(56000, 1000))\n",
    "model_l1.add(Input(shape=(1000,), name='input'))\n",
    "# model_l1.add(Conv1D(128, 5, activation='relu'))\n",
    "# model_l1.add(GlobalMaxPooling1D()) # added\n",
    "model_l1.add(Dense(128, activation='relu'))\n",
    "model_l1.add(Dropout(0.1))\n",
    "model_l1.add(Dense(32, activation='relu'))\n",
    "model_l1.add(Dense(14, activation='softmax')) \n",
    "model_l1.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model_l1.summary()\n",
    "\n",
    "history_l1 = model_l1.fit(x_train, y_train, epochs=10, verbose=1, use_multiprocessing=True,\n",
    "                   validation_data=(x_valid, y_valid),\n",
    "                    batch_size=32\n",
    "                   ,callbacks=[reduce,callback])\n",
    "\n",
    "\n",
    "\n",
    "training_time = time() - t\n",
    "print(\"train time: %0.3fs\" % training_time)\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "t = time()\n",
    "y_pred = model_l1.predict(x_test)\n",
    "\n",
    "test_time = time() - t\n",
    "print(\"test time:  %0.3fs\" % test_time)\n",
    "\n",
    "y_pred=np.argmax(y_pred, axis=1)\n",
    "y_test=np.argmax(y_test, axis=1)\n",
    "# cm = confusion_matrix(y_test, y_pred)\n",
    "# print(cm)\n",
    "\n",
    "# compute the performance measures\n",
    "score1 = metrics.accuracy_score(y_test, y_pred)\n",
    "print(\"accuracy:   %0.3f\" % score1)\n",
    "\n",
    "print(metrics.classification_report(y_test, y_pred, target_names=[str(x) for x in range(14)]))\n",
    "\n",
    "print(\"confusion matrix:\")\n",
    "print(metrics.confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print('------------------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2294af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12582762",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
