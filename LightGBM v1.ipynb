{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74627c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "import pandas\n",
    "import pickle\n",
    "import numpy\n",
    "\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "from time import time\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f33c001a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train, _, x_train = pickle.load(open(\"dbpedia_csv/\" + \"/train_vec_preprocessed_vectorized.pickle\", \"rb\"))\n",
    "#_, _, x_valid = pickle.load(open(paths.data + \"/validation_preprocessed.pickle\", \"rb\"))\n",
    "y_test, _, x_test = pickle.load(open(\"dbpedia_csv/\" + \"/test_vec_preprocessed_vectorized.pickle\", \"rb\"))\n",
    "\n",
    "\n",
    "y_train = [y-1 for y in y_train]\n",
    "y_test = [y-1 for y in y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2fb75c63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train time: 3529.790s\n",
      "test time:  13.925s\n"
     ]
    }
   ],
   "source": [
    "d_train=lgb.Dataset(x_train, label=y_train)\n",
    "params={}\n",
    "params['learning_rate']=0.03\n",
    "params['boosting_type']='gbdt' #GradientBoostingDecisionTree\n",
    "params['objective']='multiclass' #Multi-class target feature\n",
    "params['metric']='multi_logloss' #metric for multi-class\n",
    "params['max_depth']=10\n",
    "params['num_leaves'] = 1100\n",
    "params['verbose'] = -1\n",
    "params['num_class']= 14 #no.of unique values in the target class not inclusive of the end value\n",
    "params['device'] = 'gpu'\n",
    "t = time()\n",
    "clf=lgb.train(params,d_train,100)\n",
    "training_time = time() - t\n",
    "print(\"train time: %0.3fs\" % training_time)\n",
    "\n",
    "\n",
    "t = time()\n",
    "y_pred=clf.predict(x_test)\n",
    "\n",
    "test_time = time() - t\n",
    "print(\"test time:  %0.3fs\" % test_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e989ec72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:   0.957\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.91      0.91      5000\n",
      "           1       0.97      0.96      0.96      5000\n",
      "           2       0.90      0.88      0.89      5000\n",
      "           3       0.98      0.98      0.98      5000\n",
      "           4       0.95      0.96      0.95      5000\n",
      "           5       0.96      0.97      0.97      5000\n",
      "           6       0.95      0.95      0.95      5000\n",
      "           7       0.98      0.98      0.98      5000\n",
      "           8       1.00      0.98      0.99      5000\n",
      "           9       0.98      0.98      0.98      5000\n",
      "          10       0.98      0.98      0.98      5000\n",
      "          11       0.96      0.97      0.97      5000\n",
      "          12       0.97      0.96      0.96      5000\n",
      "          13       0.90      0.92      0.91      5000\n",
      "\n",
      "    accuracy                           0.96     70000\n",
      "   macro avg       0.96      0.96      0.96     70000\n",
      "weighted avg       0.96      0.96      0.96     70000\n",
      "\n",
      "confusion matrix:\n",
      "[[4530   36   64   14   26  100   63    5    2    5    3   20   20  112]\n",
      " [  55 4819   15    5   31    3   47    6    3    0    1    0    2   13]\n",
      " [  62   15 4413   14   80    5   15    1    0    2    4  122   45  222]\n",
      " [  10    2   39 4917   15    7    4    0    0    3    0    0    1    2]\n",
      " [  33   36   83   15 4776   15   15    2    0    2    0    0    0   23]\n",
      " [  89    1    4    5    6 4867   15    4    0    2    0    0    2    5]\n",
      " [  75   61   16    3   29   24 4735   38    5    2    0    0    5    7]\n",
      " [   6    1    2    0    5    8   43 4911   14    5    4    0    0    1]\n",
      " [   3    2    2    0    8    1   30   32 4919    1    1    0    0    1]\n",
      " [   3    0    4    7    2    0    2    6    0 4916   57    0    0    3]\n",
      " [  16    0    3    0    2    3    2    4    0   50 4918    0    0    2]\n",
      " [  14    0   98    1    0    1    2    1    0    1    1 4851   19   11]\n",
      " [   9    1   39    7    1    5    0    1    0    3    1   27 4815   91]\n",
      " [  80   17  144   17   30   10   11    4    0    2    5   17   77 4586]]\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "# compute the performance measures\n",
    "\n",
    "y_pred = [numpy.argmax(line) for line in y_pred]#printing the predictions\n",
    "\n",
    "score1 = metrics.accuracy_score(y_test, y_pred)\n",
    "print(\"accuracy:   %0.3f\" % score1)\n",
    "\n",
    "print(metrics.classification_report(y_test, y_pred, target_names=[str(x) for x in range(14)]))\n",
    "\n",
    "print(\"confusion matrix:\")\n",
    "print(metrics.confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print('------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59a3505",
   "metadata": {},
   "source": [
    "### half dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b5c2a82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train time: 1762.957s\n",
      "test time:  10.705s\n",
      "accuracy:   0.954\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.90      0.90      5000\n",
      "           1       0.96      0.96      0.96      5000\n",
      "           2       0.89      0.88      0.88      5000\n",
      "           3       0.98      0.98      0.98      5000\n",
      "           4       0.95      0.95      0.95      5000\n",
      "           5       0.96      0.97      0.96      5000\n",
      "           6       0.95      0.94      0.95      5000\n",
      "           7       0.98      0.98      0.98      5000\n",
      "           8       1.00      0.98      0.99      5000\n",
      "           9       0.98      0.98      0.98      5000\n",
      "          10       0.98      0.98      0.98      5000\n",
      "          11       0.96      0.97      0.96      5000\n",
      "          12       0.96      0.96      0.96      5000\n",
      "          13       0.90      0.91      0.91      5000\n",
      "\n",
      "    accuracy                           0.95     70000\n",
      "   macro avg       0.95      0.95      0.95     70000\n",
      "weighted avg       0.95      0.95      0.95     70000\n",
      "\n",
      "confusion matrix:\n",
      "[[4482   45   70   16   28  116   59    6    0    4    2   32   20  120]\n",
      " [  59 4803   23    8   34    3   47    7    2    0    1    1    2   10]\n",
      " [  57   17 4396   14   85    3   14    1    0    2    1  134   47  229]\n",
      " [   4    3   49 4905   15   12    4    0    0    4    1    0    1    2]\n",
      " [  31   34   96   17 4764   17   14    3    0    3    0    1    2   18]\n",
      " [  98    2    5    4    3 4855   11    9    1    4    0    1    1    6]\n",
      " [  72   60   14    3   31   30 4725   46    3    2    0    0    3   11]\n",
      " [   5    2    2    0    9    7   40 4908   12    6    4    0    2    3]\n",
      " [   4    1    2    0    5    3   32   28 4922    1    1    0    0    1]\n",
      " [   2    1    4    4    2    4    1    4    0 4916   58    0    0    4]\n",
      " [  14    0    4    0    1    4    1    4    0   63 4907    1    0    1]\n",
      " [   9    0   90    2    1    1    4    1    0    1    2 4853   22   14]\n",
      " [   9    1   43    7    3    6    0    1    0    2    1   29 4801   97]\n",
      " [  90   17  155   17   37   15    8    3    0    2    4   17   76 4559]]\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "y_train, _, x_train = pickle.load(open(\"dbpedia_csv/\" + \"/train_vec_preprocessed_vectorized.pickle\", \"rb\"))\n",
    "#_, _, x_valid = pickle.load(open(paths.data + \"/validation_preprocessed.pickle\", \"rb\"))\n",
    "y_test, _, x_test = pickle.load(open(\"dbpedia_csv/\" + \"/test_vec_preprocessed_vectorized.pickle\", \"rb\"))\n",
    "\n",
    "\n",
    "\n",
    "y_train = y_train[slice(0, len(y_train), 2)]\n",
    "x_train = x_train[slice(0, len(x_train), 2)]\n",
    "\n",
    "y_train = [y-1 for y in y_train]\n",
    "y_test = [y-1 for y in y_test]\n",
    "\n",
    "\n",
    "d_train=lgb.Dataset(x_train, label=y_train)\n",
    "params={}\n",
    "params['learning_rate']=0.03\n",
    "params['boosting_type']='gbdt' #GradientBoostingDecisionTree\n",
    "params['objective']='multiclass' #Multi-class target feature\n",
    "params['metric']='multi_logloss' #metric for multi-class\n",
    "params['max_depth']=10\n",
    "params['num_leaves'] = 1100\n",
    "params['verbose'] = -1\n",
    "params['num_class']= 14 #no.of unique values in the target class not inclusive of the end value\n",
    "params['device'] = 'gpu'\n",
    "t = time()\n",
    "clf=lgb.train(params,d_train,100)\n",
    "training_time = time() - t\n",
    "print(\"train time: %0.3fs\" % training_time)\n",
    "\n",
    "\n",
    "t = time()\n",
    "y_pred=clf.predict(x_test)\n",
    "\n",
    "test_time = time() - t\n",
    "print(\"test time:  %0.3fs\" % test_time)\n",
    "\n",
    "\n",
    "# compute the performance measures\n",
    "y_pred = [numpy.argmax(line) for line in y_pred]#printing the predictions\n",
    "\n",
    "score1 = metrics.accuracy_score(y_test, y_pred)\n",
    "print(\"accuracy:   %0.3f\" % score1)\n",
    "\n",
    "print(metrics.classification_report(y_test, y_pred, target_names=[str(x) for x in range(14)]))\n",
    "\n",
    "print(\"confusion matrix:\")\n",
    "print(metrics.confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print('------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d25aca",
   "metadata": {},
   "source": [
    "### fourth of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ee04093",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train time: 1257.104s\n",
      "test time:  10.863s\n",
      "accuracy:   0.951\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.89      0.89      5000\n",
      "           1       0.96      0.96      0.96      5000\n",
      "           2       0.88      0.87      0.88      5000\n",
      "           3       0.98      0.98      0.98      5000\n",
      "           4       0.95      0.95      0.95      5000\n",
      "           5       0.95      0.97      0.96      5000\n",
      "           6       0.95      0.94      0.94      5000\n",
      "           7       0.97      0.98      0.98      5000\n",
      "           8       0.99      0.98      0.99      5000\n",
      "           9       0.98      0.98      0.98      5000\n",
      "          10       0.98      0.98      0.98      5000\n",
      "          11       0.96      0.97      0.96      5000\n",
      "          12       0.96      0.96      0.96      5000\n",
      "          13       0.90      0.90      0.90      5000\n",
      "\n",
      "    accuracy                           0.95     70000\n",
      "   macro avg       0.95      0.95      0.95     70000\n",
      "weighted avg       0.95      0.95      0.95     70000\n",
      "\n",
      "confusion matrix:\n",
      "[[4444   42   70   22   34  123   62   10    2    4    4   31   24  128]\n",
      " [  57 4788   21    7   36    4   55    6    4    0    1    0    4   17]\n",
      " [  73   16 4357   15   86    6   14    1    0    1    3  141   56  231]\n",
      " [   7    5   47 4900   14   11    5    0    0    7    0    1    1    2]\n",
      " [  36   44   99   18 4746   18   15    2    1    1    1    1    1   17]\n",
      " [ 104    1    6    1    5 4848   16    7    1    3    0    0    2    6]\n",
      " [  82   65   17    5   34   27 4709   43    5    2    0    1    3    7]\n",
      " [   5    3    3    0   11   12   40 4896   15    7    5    0    1    2]\n",
      " [   4    3    1    0    6    2   33   36 4913    0    1    0    0    1]\n",
      " [   4    0    5    7    2    2    5    9    0 4892   71    0    1    2]\n",
      " [  13    0    4    0    2    4    1    6    0   65 4903    1    0    1]\n",
      " [  13    0   94    2    1    1    2    1    0    1    2 4842   24   17]\n",
      " [   7    1   45   11    4    6    2    1    0    3    2   25 4807   86]\n",
      " [  93   18  180   19   33   14   15    5    1    5    3   20   81 4513]]\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "y_train, _, x_train = pickle.load(open(\"dbpedia_csv/\" + \"/train_vec_preprocessed_vectorized.pickle\", \"rb\"))\n",
    "#_, _, x_valid = pickle.load(open(paths.data + \"/validation_preprocessed.pickle\", \"rb\"))\n",
    "y_test, _, x_test = pickle.load(open(\"dbpedia_csv/\" + \"/test_vec_preprocessed_vectorized.pickle\", \"rb\"))\n",
    "\n",
    "\n",
    "\n",
    "y_train = y_train[slice(0, len(y_train), 4)]\n",
    "x_train = x_train[slice(0, len(x_train), 4)]\n",
    "\n",
    "y_train = [y-1 for y in y_train]\n",
    "y_test = [y-1 for y in y_test]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "d_train=lgb.Dataset(x_train, label=y_train)\n",
    "params={}\n",
    "params['learning_rate']=0.03\n",
    "params['boosting_type']='gbdt' #GradientBoostingDecisionTree\n",
    "params['objective']='multiclass' #Multi-class target feature\n",
    "params['metric']='multi_logloss' #metric for multi-class\n",
    "params['max_depth']=10\n",
    "params['num_leaves'] = 800\n",
    "params['verbose'] = -1\n",
    "params['num_class']= 14 #no.of unique values in the target class not inclusive of the end value\n",
    "params['device'] = 'gpu'\n",
    "t = time()\n",
    "clf=lgb.train(params,d_train,100)\n",
    "training_time = time() - t\n",
    "print(\"train time: %0.3fs\" % training_time)\n",
    "\n",
    "\n",
    "t = time()\n",
    "y_pred=clf.predict(x_test)\n",
    "\n",
    "test_time = time() - t\n",
    "print(\"test time:  %0.3fs\" % test_time)\n",
    "\n",
    "\n",
    "# compute the performance measures\n",
    "y_pred = [numpy.argmax(line) for line in y_pred]#printing the predictions\n",
    "\n",
    "score1 = metrics.accuracy_score(y_test, y_pred)\n",
    "print(\"accuracy:   %0.3f\" % score1)\n",
    "\n",
    "print(metrics.classification_report(y_test, y_pred, target_names=[str(x) for x in range(14)]))\n",
    "\n",
    "print(\"confusion matrix:\")\n",
    "print(metrics.confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print('------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a48b062",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4efd9b28",
   "metadata": {},
   "source": [
    "### full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55e0fffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to extract features from training data : 14.589975 seconds\n",
      "n_samples: 560000, n_features: 698699\n",
      "Time taken to extract features from test data : 1.523001 seconds\n",
      "n_samples: 70000, n_features: 698699\n",
      "train time: 700.722s\n",
      "test time:  3.266s\n",
      "accuracy:   0.951\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.90      0.91      5000\n",
      "           1       0.96      0.97      0.97      5000\n",
      "           2       0.85      0.87      0.86      5000\n",
      "           3       0.97      0.97      0.97      5000\n",
      "           4       0.96      0.94      0.95      5000\n",
      "           5       0.95      0.95      0.95      5000\n",
      "           6       0.95      0.93      0.94      5000\n",
      "           7       0.97      0.97      0.97      5000\n",
      "           8       0.99      0.99      0.99      5000\n",
      "           9       0.95      0.97      0.96      5000\n",
      "          10       0.98      0.95      0.96      5000\n",
      "          11       0.97      0.98      0.97      5000\n",
      "          12       0.97      0.98      0.97      5000\n",
      "          13       0.93      0.95      0.94      5000\n",
      "\n",
      "    accuracy                           0.95     70000\n",
      "   macro avg       0.95      0.95      0.95     70000\n",
      "weighted avg       0.95      0.95      0.95     70000\n",
      "\n",
      "confusion matrix:\n",
      "[[4488   60   87   33   27   92   72   10    5    9    5   25   31   56]\n",
      " [  33 4874   19    5    8    6   39    1    3    1    1    0    7    3]\n",
      " [  58   42 4358   48   80   12   11    6    0    7    1  113   61  203]\n",
      " [   6    5  117 4828   14   11    4    4    0    4    1    0    3    3]\n",
      " [  37   40  163   24 4686   14    8    4    3    4    1    1    2   13]\n",
      " [ 113    0   50    4   19 4764   21   13    0    3    1    0    4    8]\n",
      " [  84   57   36    0   28   61 4634   59   17    6    2    2    7    7]\n",
      " [  13    2   29    0    7    4   43 4870   18    9    3    0    0    2]\n",
      " [   3    3    3    0    0    0   15   10 4963    1    0    1    0    1]\n",
      " [   5    0   30    0    1    8    1   14    0 4851   88    0    1    1]\n",
      " [  11    2   21    0    0    6    0    8    0  213 4737    0    1    1]\n",
      " [   7    0   57    3    1    2    0    0    0    0    0 4894   22   14]\n",
      " [   7    3   27    8    3    7    0    2    2    2    0   21 4886   32]\n",
      " [  45   10  108   14   18    7    9    0    1    4    2    9   36 4737]]\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "y_train, _, x_train = pickle.load(open(\"dbpedia_csv/\" + \"/train_preprocessed.pickle\", \"rb\"))\n",
    "#_, _, x_valid = pickle.load(open(paths.data + \"/validation_preprocessed.pickle\", \"rb\"))\n",
    "y_test, _, x_test = pickle.load(open(\"dbpedia_csv/\" + \"/test_preprocessed.pickle\", \"rb\"))\n",
    "\n",
    "y_train = [y-1 for y in y_train]\n",
    "y_test = [y-1 for y in y_test]\n",
    "\n",
    "t = time()  # not compulsory\n",
    "\n",
    "# loading CountVectorizer\n",
    "tf_vectorizer = TfidfVectorizer() # or term frequency\n",
    "\n",
    "x_train = tf_vectorizer.fit_transform(x_train)\n",
    "\n",
    "duration = time() - t\n",
    "print(\"Time taken to extract features from training data : %f seconds\" % (duration))\n",
    "print(\"n_samples: %d, n_features: %d\" % x_train.shape)\n",
    "\n",
    "t = time()\n",
    "x_test = tf_vectorizer.transform(x_test)\n",
    "\n",
    "duration = time() - t\n",
    "print(\"Time taken to extract features from test data : %f seconds\" % (duration))\n",
    "print(\"n_samples: %d, n_features: %d\" % x_test.shape)\n",
    "\n",
    "\n",
    "\n",
    "d_train=lgb.Dataset(x_train, label=y_train)\n",
    "\n",
    "\n",
    "import gc\n",
    "import sys\n",
    "del x_train\n",
    "del tf_vectorizer\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "params={}\n",
    "params['learning_rate']=0.03\n",
    "params['boosting_type']='gbdt' #GradientBoostingDecisionTree\n",
    "params['objective']='multiclass' #Multi-class target feature\n",
    "params['metric']='multi_logloss' #metric for multi-class\n",
    "params['max_depth']=10\n",
    "params['num_leaves'] = 800\n",
    "params['verbose'] = -1\n",
    "params['num_class']= 14 #no.of unique values in the target class not inclusive of the end value\n",
    "params['device'] = 'gpu'\n",
    "t = time()\n",
    "clf=lgb.train(params,d_train,100)\n",
    "training_time = time() - t\n",
    "print(\"train time: %0.3fs\" % training_time)\n",
    "\n",
    "\n",
    "t = time()\n",
    "y_pred=clf.predict(x_test)\n",
    "\n",
    "test_time = time() - t\n",
    "print(\"test time:  %0.3fs\" % test_time)\n",
    "\n",
    "\n",
    "# compute the performance measures\n",
    "y_pred = [numpy.argmax(line) for line in y_pred]#printing the predictions\n",
    "\n",
    "score1 = metrics.accuracy_score(y_test, y_pred)\n",
    "print(\"accuracy:   %0.3f\" % score1)\n",
    "\n",
    "print(metrics.classification_report(y_test, y_pred, target_names=[str(x) for x in range(14)]))\n",
    "\n",
    "print(\"confusion matrix:\")\n",
    "print(metrics.confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print('------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7500618e",
   "metadata": {},
   "source": [
    "## half of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0388801",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to extract features from training data : 6.953000 seconds\n",
      "n_samples: 280000, n_features: 445084\n",
      "Time taken to extract features from test data : 1.486998 seconds\n",
      "n_samples: 70000, n_features: 445084\n",
      "train time: 429.631s\n",
      "test time:  2.819s\n",
      "accuracy:   0.949\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.89      0.90      5000\n",
      "           1       0.95      0.97      0.96      5000\n",
      "           2       0.85      0.86      0.85      5000\n",
      "           3       0.97      0.97      0.97      5000\n",
      "           4       0.96      0.93      0.95      5000\n",
      "           5       0.95      0.95      0.95      5000\n",
      "           6       0.95      0.92      0.94      5000\n",
      "           7       0.97      0.97      0.97      5000\n",
      "           8       0.99      0.99      0.99      5000\n",
      "           9       0.95      0.97      0.96      5000\n",
      "          10       0.98      0.94      0.96      5000\n",
      "          11       0.96      0.98      0.97      5000\n",
      "          12       0.97      0.97      0.97      5000\n",
      "          13       0.93      0.94      0.94      5000\n",
      "\n",
      "    accuracy                           0.95     70000\n",
      "   macro avg       0.95      0.95      0.95     70000\n",
      "weighted avg       0.95      0.95      0.95     70000\n",
      "\n",
      "confusion matrix:\n",
      "[[4473   60   94   34   24   85   78   13    6   10    6   28   30   59]\n",
      " [  34 4864   20    6   10   11   40    3    2    0    1    0    5    4]\n",
      " [  56   49 4311   45   79   12   18    6    0    9    1  134   60  220]\n",
      " [   6    4  108 4837   17   12    3    4    0    4    0    0    3    2]\n",
      " [  37   49  163   26 4672   16    8    5    2    3    1    1    4   13]\n",
      " [ 105    0   58    7   16 4761   24   13    0    2    3    0    4    7]\n",
      " [  86   57   41    1   30   64 4621   58   19    5    1    2    7    8]\n",
      " [   8    1   26    0    8    7   46 4863   24   10    6    0    0    1]\n",
      " [   2    5    7    0    1    1   16   12 4954    1    0    0    0    1]\n",
      " [   5    0   35    0    1    6    1   15    0 4838   97    0    1    1]\n",
      " [  11    2   26    0    0    6    0    7    0  224 4722    0    0    2]\n",
      " [   4    0   57    3    1    3    0    0    0    1    0 4897   21   13]\n",
      " [  12    3   35    7    2    8    0    2    1    1    0   21 4874   34]\n",
      " [  51   11  111   13   17    9    8    1    1    6    2   11   37 4722]]\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "y_train, _, x_train = pickle.load(open(\"dbpedia_csv/\" + \"/train_preprocessed.pickle\", \"rb\"))\n",
    "#_, _, x_valid = pickle.load(open(paths.data + \"/validation_preprocessed.pickle\", \"rb\"))\n",
    "y_test, _, x_test = pickle.load(open(\"dbpedia_csv/\" + \"/test_preprocessed.pickle\", \"rb\"))\n",
    "\n",
    "y_train = y_train[slice(0, len(y_train), 2)]\n",
    "x_train = x_train[slice(0, len(x_train), 2)]\n",
    "\n",
    "y_train = [y-1 for y in y_train]\n",
    "y_test = [y-1 for y in y_test]\n",
    "\n",
    "t = time()  # not compulsory\n",
    "\n",
    "# loading CountVectorizer\n",
    "tf_vectorizer = TfidfVectorizer() # or term frequency\n",
    "\n",
    "x_train = tf_vectorizer.fit_transform(x_train)\n",
    "\n",
    "duration = time() - t\n",
    "print(\"Time taken to extract features from training data : %f seconds\" % (duration))\n",
    "print(\"n_samples: %d, n_features: %d\" % x_train.shape)\n",
    "\n",
    "t = time()\n",
    "x_test = tf_vectorizer.transform(x_test)\n",
    "\n",
    "duration = time() - t\n",
    "print(\"Time taken to extract features from test data : %f seconds\" % (duration))\n",
    "print(\"n_samples: %d, n_features: %d\" % x_test.shape)\n",
    "\n",
    "\n",
    "\n",
    "d_train=lgb.Dataset(x_train, label=y_train)\n",
    "\n",
    "\n",
    "import gc\n",
    "import sys\n",
    "del x_train\n",
    "del tf_vectorizer\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "params={}\n",
    "params['learning_rate']=0.03\n",
    "params['boosting_type']='gbdt' #GradientBoostingDecisionTree\n",
    "params['objective']='multiclass' #Multi-class target feature\n",
    "params['metric']='multi_logloss' #metric for multi-class\n",
    "params['max_depth']=10\n",
    "params['num_leaves'] = 800\n",
    "params['verbose'] = -1\n",
    "params['num_class']= 14 #no.of unique values in the target class not inclusive of the end value\n",
    "params['device'] = 'gpu'\n",
    "t = time()\n",
    "clf=lgb.train(params,d_train,100)\n",
    "training_time = time() - t\n",
    "print(\"train time: %0.3fs\" % training_time)\n",
    "\n",
    "\n",
    "t = time()\n",
    "y_pred=clf.predict(x_test)\n",
    "\n",
    "test_time = time() - t\n",
    "print(\"test time:  %0.3fs\" % test_time)\n",
    "\n",
    "\n",
    "# compute the performance measures\n",
    "y_pred = [numpy.argmax(line) for line in y_pred]#printing the predictions\n",
    "\n",
    "score1 = metrics.accuracy_score(y_test, y_pred)\n",
    "print(\"accuracy:   %0.3f\" % score1)\n",
    "\n",
    "print(metrics.classification_report(y_test, y_pred, target_names=[str(x) for x in range(14)]))\n",
    "\n",
    "print(\"confusion matrix:\")\n",
    "print(metrics.confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print('------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0704883f",
   "metadata": {},
   "source": [
    "### fourth of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c0d98a5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to extract features from training data : 3.361711 seconds\n",
      "n_samples: 140000, n_features: 282474\n",
      "Time taken to extract features from test data : 1.366998 seconds\n",
      "n_samples: 70000, n_features: 282474\n",
      "train time: 230.059s\n",
      "test time:  2.764s\n",
      "accuracy:   0.946\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.89      0.90      5000\n",
      "           1       0.95      0.97      0.96      5000\n",
      "           2       0.84      0.85      0.85      5000\n",
      "           3       0.97      0.96      0.97      5000\n",
      "           4       0.96      0.93      0.95      5000\n",
      "           5       0.95      0.95      0.95      5000\n",
      "           6       0.95      0.92      0.93      5000\n",
      "           7       0.97      0.97      0.97      5000\n",
      "           8       0.99      0.99      0.99      5000\n",
      "           9       0.94      0.96      0.95      5000\n",
      "          10       0.97      0.94      0.96      5000\n",
      "          11       0.96      0.98      0.97      5000\n",
      "          12       0.96      0.97      0.97      5000\n",
      "          13       0.92      0.94      0.93      5000\n",
      "\n",
      "    accuracy                           0.95     70000\n",
      "   macro avg       0.95      0.95      0.95     70000\n",
      "weighted avg       0.95      0.95      0.95     70000\n",
      "\n",
      "confusion matrix:\n",
      "[[4459   61   96   33   23   91   82   12    5   11    6   29   27   65]\n",
      " [  46 4851   21    9    9   10   41    1    1    0    1    0    7    3]\n",
      " [  72   53 4270   52   79   12   16    8    2    6    2  144   62  222]\n",
      " [   6    6  116 4823   16   10    6    4    0    6    1    0    1    5]\n",
      " [  40   50  161   25 4669   17    6    6    3    3    1    1    4   14]\n",
      " [ 117    0   55    5   15 4748   27   13    0    4    2    0    4   10]\n",
      " [  85   66   35    1   29   72 4604   64   19    6    1    2    8    8]\n",
      " [  14    1   29    0    8    9   39 4864   22   11    2    0    0    1]\n",
      " [   2    5    7    0    1    1   17   13 4952    1    0    0    0    1]\n",
      " [   5    0   39    2    1    7    1   17    0 4803  122    0    1    2]\n",
      " [  11    1   26    0    0    6    1    8    0  238 4705    0    1    3]\n",
      " [   5    0   63    3    0    3    0    0    0    1    0 4885   27   13]\n",
      " [  11    6   35    9    3   10    1    2    1    2    0   23 4862   35]\n",
      " [  53   12  127   11   19    9    8    1    1    5    3   11   46 4694]]\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "y_train, _, x_train = pickle.load(open(\"dbpedia_csv/\" + \"/train_preprocessed.pickle\", \"rb\"))\n",
    "#_, _, x_valid = pickle.load(open(paths.data + \"/validation_preprocessed.pickle\", \"rb\"))\n",
    "y_test, _, x_test = pickle.load(open(\"dbpedia_csv/\" + \"/test_preprocessed.pickle\", \"rb\"))\n",
    "\n",
    "\n",
    "y_train = y_train[slice(0, len(y_train), 4)]\n",
    "x_train = x_train[slice(0, len(x_train), 4)]\n",
    "\n",
    "\n",
    "y_train = [y-1 for y in y_train]\n",
    "y_test = [y-1 for y in y_test]\n",
    "\n",
    "t = time()  # not compulsory\n",
    "\n",
    "# loading CountVectorizer\n",
    "tf_vectorizer = TfidfVectorizer() # or term frequency\n",
    "\n",
    "x_train = tf_vectorizer.fit_transform(x_train)\n",
    "\n",
    "duration = time() - t\n",
    "print(\"Time taken to extract features from training data : %f seconds\" % (duration))\n",
    "print(\"n_samples: %d, n_features: %d\" % x_train.shape)\n",
    "\n",
    "t = time()\n",
    "x_test = tf_vectorizer.transform(x_test)\n",
    "\n",
    "duration = time() - t\n",
    "print(\"Time taken to extract features from test data : %f seconds\" % (duration))\n",
    "print(\"n_samples: %d, n_features: %d\" % x_test.shape)\n",
    "\n",
    "\n",
    "\n",
    "d_train=lgb.Dataset(x_train, label=y_train)\n",
    "\n",
    "\n",
    "import gc\n",
    "import sys\n",
    "del x_train\n",
    "del tf_vectorizer\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "params={}\n",
    "params['learning_rate']=0.03\n",
    "params['boosting_type']='gbdt' #GradientBoostingDecisionTree\n",
    "params['objective']='multiclass' #Multi-class target feature\n",
    "params['metric']='multi_logloss' #metric for multi-class\n",
    "params['max_depth']=10\n",
    "params['num_leaves'] = 800\n",
    "params['verbose'] = -1\n",
    "params['num_class']= 14 #no.of unique values in the target class not inclusive of the end value\n",
    "params['device'] = 'gpu'\n",
    "t = time()\n",
    "clf=lgb.train(params,d_train,100)\n",
    "training_time = time() - t\n",
    "print(\"train time: %0.3fs\" % training_time)\n",
    "\n",
    "\n",
    "t = time()\n",
    "y_pred=clf.predict(x_test)\n",
    "\n",
    "test_time = time() - t\n",
    "print(\"test time:  %0.3fs\" % test_time)\n",
    "\n",
    "\n",
    "# compute the performance measures\n",
    "y_pred = [numpy.argmax(line) for line in y_pred]#printing the predictions\n",
    "\n",
    "score1 = metrics.accuracy_score(y_test, y_pred)\n",
    "print(\"accuracy:   %0.3f\" % score1)\n",
    "\n",
    "print(metrics.classification_report(y_test, y_pred, target_names=[str(x) for x in range(14)]))\n",
    "\n",
    "print(\"confusion matrix:\")\n",
    "print(metrics.confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print('------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192f1968",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
