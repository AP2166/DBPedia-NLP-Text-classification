{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74627c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "import pandas\n",
    "import pickle\n",
    "import numpy\n",
    "\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "from time import time\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f33c001a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train, _, x_train = pickle.load(open(\"dbpedia_csv/\" + \"/train_lem_preprocessed_vectorized.pickle\", \"rb\"))\n",
    "#_, _, x_valid = pickle.load(open(paths.data + \"/validation_preprocessed.pickle\", \"rb\"))\n",
    "y_test, _, x_test = pickle.load(open(\"dbpedia_csv/\" + \"/test_lem_preprocessed_vectorized.pickle\", \"rb\"))\n",
    "\n",
    "\n",
    "y_train = [y-1 for y in y_train]\n",
    "y_test = [y-1 for y in y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2fb75c63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train time: 2490.397s\n",
      "test time:  14.675s\n"
     ]
    }
   ],
   "source": [
    "d_train=lgb.Dataset(x_train, label=y_train)\n",
    "params={}\n",
    "params['learning_rate']=0.03\n",
    "params['boosting_type']='gbdt' #GradientBoostingDecisionTree\n",
    "params['objective']='multiclass' #Multi-class target feature\n",
    "params['metric']='multi_logloss' #metric for multi-class\n",
    "params['max_depth']=10\n",
    "params['num_leaves'] = 1100\n",
    "params['verbose'] = -1\n",
    "params['num_class']= 14 #no.of unique values in the target class not inclusive of the end value\n",
    "params['device'] = 'gpu'\n",
    "t = time()\n",
    "clf=lgb.train(params,d_train,100)\n",
    "training_time = time() - t\n",
    "print(\"train time: %0.3fs\" % training_time)\n",
    "\n",
    "\n",
    "t = time()\n",
    "y_pred=clf.predict(x_test)\n",
    "\n",
    "test_time = time() - t\n",
    "print(\"test time:  %0.3fs\" % test_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e989ec72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:   0.954\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.89      0.90      5000\n",
      "           1       0.96      0.97      0.97      5000\n",
      "           2       0.88      0.87      0.87      5000\n",
      "           3       0.98      0.98      0.98      5000\n",
      "           4       0.95      0.96      0.95      5000\n",
      "           5       0.96      0.97      0.97      5000\n",
      "           6       0.95      0.95      0.95      5000\n",
      "           7       0.98      0.98      0.98      5000\n",
      "           8       1.00      0.98      0.99      5000\n",
      "           9       0.99      0.98      0.98      5000\n",
      "          10       0.99      0.99      0.99      5000\n",
      "          11       0.96      0.97      0.96      5000\n",
      "          12       0.96      0.96      0.96      5000\n",
      "          13       0.89      0.91      0.90      5000\n",
      "\n",
      "    accuracy                           0.95     70000\n",
      "   macro avg       0.95      0.95      0.95     70000\n",
      "weighted avg       0.95      0.95      0.95     70000\n",
      "\n",
      "confusion matrix:\n",
      "[[4466   38   67   14   34  111   68   10    1    5    4   31   27  124]\n",
      " [  47 4832   18    7   28    3   47    4    1    0    1    0    2   10]\n",
      " [  62   19 4328   12   81    1   18    2    0    2    4  141   62  268]\n",
      " [   5    4   46 4909   15    6    4    1    0    6    0    1    1    2]\n",
      " [  29   35   87   18 4781   12   15    1    0    1    0    1    1   19]\n",
      " [ 101    1    4    3    7 4857   13    5    0    2    0    1    3    3]\n",
      " [  73   57   17    3   27   25 4739   41    4    2    0    0    5    7]\n",
      " [   6    4    6    1    5    9   39 4912    9    4    3    0    0    2]\n",
      " [   2    2    3    0    5    1   31   30 4921    2    1    0    0    2]\n",
      " [   3    0    3    7    2    1    1   10    0 4915   54    0    1    3]\n",
      " [  14    0    4    0    0    2    1    8    0   40 4928    0    0    3]\n",
      " [  13    0   91    1    0    1    1    0    0    1    2 4854   24   12]\n",
      " [  13    2   48    7    3    6    1    1    0    2    0   24 4795   98]\n",
      " [  91   17  179   17   34   11   13    5    0    2    3   15   72 4541]]\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "# compute the performance measures\n",
    "\n",
    "y_pred = [numpy.argmax(line) for line in y_pred]#printing the predictions\n",
    "\n",
    "score1 = metrics.accuracy_score(y_test, y_pred)\n",
    "print(\"accuracy:   %0.3f\" % score1)\n",
    "\n",
    "print(metrics.classification_report(y_test, y_pred, target_names=[str(x) for x in range(14)]))\n",
    "\n",
    "print(\"confusion matrix:\")\n",
    "print(metrics.confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print('------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59a3505",
   "metadata": {},
   "source": [
    "### half dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b5c2a82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train time: 1734.894s\n",
      "test time:  10.544s\n",
      "accuracy:   0.952\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.89      0.90      5000\n",
      "           1       0.96      0.96      0.96      5000\n",
      "           2       0.88      0.86      0.87      5000\n",
      "           3       0.98      0.98      0.98      5000\n",
      "           4       0.95      0.95      0.95      5000\n",
      "           5       0.96      0.97      0.96      5000\n",
      "           6       0.95      0.94      0.95      5000\n",
      "           7       0.98      0.98      0.98      5000\n",
      "           8       1.00      0.98      0.99      5000\n",
      "           9       0.98      0.98      0.98      5000\n",
      "          10       0.98      0.98      0.98      5000\n",
      "          11       0.95      0.97      0.96      5000\n",
      "          12       0.96      0.96      0.96      5000\n",
      "          13       0.89      0.91      0.90      5000\n",
      "\n",
      "    accuracy                           0.95     70000\n",
      "   macro avg       0.95      0.95      0.95     70000\n",
      "weighted avg       0.95      0.95      0.95     70000\n",
      "\n",
      "confusion matrix:\n",
      "[[4447   42   62   18   31  119   63   11    1    5    5   34   35  127]\n",
      " [  51 4812   17   10   33    2   48    6    4    1    1    0    3   12]\n",
      " [  68   21 4308   16   75    1   17    2    0    4    2  158   59  269]\n",
      " [   6    3   45 4903   17   10    3    1    0    6    0    1    2    3]\n",
      " [  31   44   95   17 4753   21   15    2    0    1    0    0    3   18]\n",
      " [ 106    2    4    4    7 4843   16    4    0    5    0    1    1    7]\n",
      " [  79   63   19    5   28   30 4719   38    6    2    0    1    2    8]\n",
      " [   3    3    7    0    8    7   41 4911   10    4    4    0    1    1]\n",
      " [   5    3    2    0    5    0   33   33 4914    0    1    0    0    4]\n",
      " [   4    0    6    7    2    2    0    8    0 4910   59    0    0    2]\n",
      " [  15    0    4    0    1    4    1    5    0   47 4920    1    0    2]\n",
      " [  11    0   99    1    3    1    3    0    0    1    2 4842   21   16]\n",
      " [  10    2   49    9    2    4    1    1    0    3    1   24 4803   91]\n",
      " [  94   19  182   16   36    8   10    5    1    4    5   19   74 4527]]\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "y_train, _, x_train = pickle.load(open(\"dbpedia_csv/\" + \"/train_lem_preprocessed_vectorized.pickle\", \"rb\"))\n",
    "#_, _, x_valid = pickle.load(open(paths.data + \"/validation_preprocessed.pickle\", \"rb\"))\n",
    "y_test, _, x_test = pickle.load(open(\"dbpedia_csv/\" + \"/test_lem_preprocessed_vectorized.pickle\", \"rb\"))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "y_train = y_train[slice(0, len(y_train), 2)]\n",
    "x_train = x_train[slice(0, len(x_train), 2)]\n",
    "\n",
    "y_train = [y-1 for y in y_train]\n",
    "y_test = [y-1 for y in y_test]\n",
    "\n",
    "\n",
    "d_train=lgb.Dataset(x_train, label=y_train)\n",
    "params={}\n",
    "params['learning_rate']=0.03\n",
    "params['boosting_type']='gbdt' #GradientBoostingDecisionTree\n",
    "params['objective']='multiclass' #Multi-class target feature\n",
    "params['metric']='multi_logloss' #metric for multi-class\n",
    "params['max_depth']=10\n",
    "params['num_leaves'] = 1100\n",
    "params['verbose'] = -1\n",
    "params['num_class']= 14 #no.of unique values in the target class not inclusive of the end value\n",
    "params['device'] = 'gpu'\n",
    "t = time()\n",
    "clf=lgb.train(params,d_train,100)\n",
    "training_time = time() - t\n",
    "print(\"train time: %0.3fs\" % training_time)\n",
    "\n",
    "\n",
    "t = time()\n",
    "y_pred=clf.predict(x_test)\n",
    "\n",
    "test_time = time() - t\n",
    "print(\"test time:  %0.3fs\" % test_time)\n",
    "\n",
    "\n",
    "# compute the performance measures\n",
    "y_pred = [numpy.argmax(line) for line in y_pred]#printing the predictions\n",
    "\n",
    "score1 = metrics.accuracy_score(y_test, y_pred)\n",
    "print(\"accuracy:   %0.3f\" % score1)\n",
    "\n",
    "print(metrics.classification_report(y_test, y_pred, target_names=[str(x) for x in range(14)]))\n",
    "\n",
    "print(\"confusion matrix:\")\n",
    "print(metrics.confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print('------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d25aca",
   "metadata": {},
   "source": [
    "### fourth of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ee04093",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train time: 1823.707s\n",
      "test time:  15.396s\n",
      "accuracy:   0.948\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.88      0.89      5000\n",
      "           1       0.96      0.96      0.96      5000\n",
      "           2       0.87      0.85      0.86      5000\n",
      "           3       0.98      0.98      0.98      5000\n",
      "           4       0.94      0.95      0.95      5000\n",
      "           5       0.95      0.97      0.96      5000\n",
      "           6       0.95      0.94      0.95      5000\n",
      "           7       0.98      0.98      0.98      5000\n",
      "           8       1.00      0.98      0.99      5000\n",
      "           9       0.98      0.98      0.98      5000\n",
      "          10       0.98      0.98      0.98      5000\n",
      "          11       0.95      0.97      0.96      5000\n",
      "          12       0.96      0.96      0.96      5000\n",
      "          13       0.88      0.89      0.89      5000\n",
      "\n",
      "    accuracy                           0.95     70000\n",
      "   macro avg       0.95      0.95      0.95     70000\n",
      "weighted avg       0.95      0.95      0.95     70000\n",
      "\n",
      "confusion matrix:\n",
      "[[4397   40   73   23   39  127   63   11    0    5    6   42   31  143]\n",
      " [  45 4809   25   10   35    2   45    3    5    0    1    0    4   16]\n",
      " [  77   19 4260   16   90    4   12    1    0    2    4  156   66  293]\n",
      " [   9    5   45 4885   19   16    3    3    0    7    0    0    4    4]\n",
      " [  28   44   93   19 4749   19   21    1    0    1    0    1    3   21]\n",
      " [ 120    4    2    3    7 4829   15    4    0    5    0    1    3    7]\n",
      " [  76   69   22    5   29   33 4708   43    3    2    0    0    3    7]\n",
      " [   4    3    4    0    6   12   35 4914   11    5    4    0    1    1]\n",
      " [   4    3    2    0    7    0   32   35 4912    2    1    0    0    2]\n",
      " [   3    0    8    6    2    3    2    9    0 4905   59    0    0    3]\n",
      " [  13    0    8    0    0    3    2    7    0   51 4914    1    0    1]\n",
      " [  15    0   94    1    0    0    2    1    0    2    2 4844   23   16]\n",
      " [  11    1   48   10    6    5    4    1    0    1    0   27 4794   92]\n",
      " [ 111   17  201   18   38   17   10    6    0    7    4   18   83 4470]]\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "y_train, _, x_train = pickle.load(open(\"dbpedia_csv/\" + \"/train_lem_preprocessed_vectorized.pickle\", \"rb\"))\n",
    "#_, _, x_valid = pickle.load(open(paths.data + \"/validation_preprocessed.pickle\", \"rb\"))\n",
    "y_test, _, x_test = pickle.load(open(\"dbpedia_csv/\" + \"/test_lem_preprocessed_vectorized.pickle\", \"rb\"))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "y_train = y_train[slice(0, len(y_train), 4)]\n",
    "x_train = x_train[slice(0, len(x_train), 4)]\n",
    "\n",
    "y_train = [y-1 for y in y_train]\n",
    "y_test = [y-1 for y in y_test]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "d_train=lgb.Dataset(x_train, label=y_train)\n",
    "params={}\n",
    "params['learning_rate']=0.03\n",
    "params['boosting_type']='gbdt' #GradientBoostingDecisionTree\n",
    "params['objective']='multiclass' #Multi-class target feature\n",
    "params['metric']='multi_logloss' #metric for multi-class\n",
    "params['max_depth']=10\n",
    "params['num_leaves'] = 800\n",
    "params['verbose'] = -1\n",
    "params['num_class']= 14 #no.of unique values in the target class not inclusive of the end value\n",
    "params['device'] = 'gpu'\n",
    "t = time()\n",
    "clf=lgb.train(params,d_train,100)\n",
    "training_time = time() - t\n",
    "print(\"train time: %0.3fs\" % training_time)\n",
    "\n",
    "\n",
    "t = time()\n",
    "y_pred=clf.predict(x_test)\n",
    "\n",
    "test_time = time() - t\n",
    "print(\"test time:  %0.3fs\" % test_time)\n",
    "\n",
    "\n",
    "# compute the performance measures\n",
    "y_pred = [numpy.argmax(line) for line in y_pred]#printing the predictions\n",
    "\n",
    "score1 = metrics.accuracy_score(y_test, y_pred)\n",
    "print(\"accuracy:   %0.3f\" % score1)\n",
    "\n",
    "print(metrics.classification_report(y_test, y_pred, target_names=[str(x) for x in range(14)]))\n",
    "\n",
    "print(\"confusion matrix:\")\n",
    "print(metrics.confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print('------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a48b062",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4efd9b28",
   "metadata": {},
   "source": [
    "### full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55e0fffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to extract features from training data : 14.589975 seconds\n",
      "n_samples: 560000, n_features: 698699\n",
      "Time taken to extract features from test data : 1.523001 seconds\n",
      "n_samples: 70000, n_features: 698699\n",
      "train time: 700.722s\n",
      "test time:  3.266s\n",
      "accuracy:   0.951\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.90      0.91      5000\n",
      "           1       0.96      0.97      0.97      5000\n",
      "           2       0.85      0.87      0.86      5000\n",
      "           3       0.97      0.97      0.97      5000\n",
      "           4       0.96      0.94      0.95      5000\n",
      "           5       0.95      0.95      0.95      5000\n",
      "           6       0.95      0.93      0.94      5000\n",
      "           7       0.97      0.97      0.97      5000\n",
      "           8       0.99      0.99      0.99      5000\n",
      "           9       0.95      0.97      0.96      5000\n",
      "          10       0.98      0.95      0.96      5000\n",
      "          11       0.97      0.98      0.97      5000\n",
      "          12       0.97      0.98      0.97      5000\n",
      "          13       0.93      0.95      0.94      5000\n",
      "\n",
      "    accuracy                           0.95     70000\n",
      "   macro avg       0.95      0.95      0.95     70000\n",
      "weighted avg       0.95      0.95      0.95     70000\n",
      "\n",
      "confusion matrix:\n",
      "[[4488   60   87   33   27   92   72   10    5    9    5   25   31   56]\n",
      " [  33 4874   19    5    8    6   39    1    3    1    1    0    7    3]\n",
      " [  58   42 4358   48   80   12   11    6    0    7    1  113   61  203]\n",
      " [   6    5  117 4828   14   11    4    4    0    4    1    0    3    3]\n",
      " [  37   40  163   24 4686   14    8    4    3    4    1    1    2   13]\n",
      " [ 113    0   50    4   19 4764   21   13    0    3    1    0    4    8]\n",
      " [  84   57   36    0   28   61 4634   59   17    6    2    2    7    7]\n",
      " [  13    2   29    0    7    4   43 4870   18    9    3    0    0    2]\n",
      " [   3    3    3    0    0    0   15   10 4963    1    0    1    0    1]\n",
      " [   5    0   30    0    1    8    1   14    0 4851   88    0    1    1]\n",
      " [  11    2   21    0    0    6    0    8    0  213 4737    0    1    1]\n",
      " [   7    0   57    3    1    2    0    0    0    0    0 4894   22   14]\n",
      " [   7    3   27    8    3    7    0    2    2    2    0   21 4886   32]\n",
      " [  45   10  108   14   18    7    9    0    1    4    2    9   36 4737]]\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "y_train, _, x_train = pickle.load(open(\"dbpedia_csv/\" + \"/train_preprocessed.pickle\", \"rb\"))\n",
    "#_, _, x_valid = pickle.load(open(paths.data + \"/validation_preprocessed.pickle\", \"rb\"))\n",
    "y_test, _, x_test = pickle.load(open(\"dbpedia_csv/\" + \"/test_preprocessed.pickle\", \"rb\"))\n",
    "\n",
    "y_train = [y-1 for y in y_train]\n",
    "y_test = [y-1 for y in y_test]\n",
    "\n",
    "t = time()  # not compulsory\n",
    "\n",
    "# loading CountVectorizer\n",
    "tf_vectorizer = TfidfVectorizer() # or term frequency\n",
    "\n",
    "x_train = tf_vectorizer.fit_transform(x_train)\n",
    "\n",
    "duration = time() - t\n",
    "print(\"Time taken to extract features from training data : %f seconds\" % (duration))\n",
    "print(\"n_samples: %d, n_features: %d\" % x_train.shape)\n",
    "\n",
    "t = time()\n",
    "x_test = tf_vectorizer.transform(x_test)\n",
    "\n",
    "duration = time() - t\n",
    "print(\"Time taken to extract features from test data : %f seconds\" % (duration))\n",
    "print(\"n_samples: %d, n_features: %d\" % x_test.shape)\n",
    "\n",
    "\n",
    "\n",
    "d_train=lgb.Dataset(x_train, label=y_train)\n",
    "\n",
    "\n",
    "import gc\n",
    "import sys\n",
    "del x_train\n",
    "del tf_vectorizer\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "params={}\n",
    "params['learning_rate']=0.03\n",
    "params['boosting_type']='gbdt' #GradientBoostingDecisionTree\n",
    "params['objective']='multiclass' #Multi-class target feature\n",
    "params['metric']='multi_logloss' #metric for multi-class\n",
    "params['max_depth']=10\n",
    "params['num_leaves'] = 800\n",
    "params['verbose'] = -1\n",
    "params['num_class']= 14 #no.of unique values in the target class not inclusive of the end value\n",
    "params['device'] = 'gpu'\n",
    "t = time()\n",
    "clf=lgb.train(params,d_train,100)\n",
    "training_time = time() - t\n",
    "print(\"train time: %0.3fs\" % training_time)\n",
    "\n",
    "\n",
    "t = time()\n",
    "y_pred=clf.predict(x_test)\n",
    "\n",
    "test_time = time() - t\n",
    "print(\"test time:  %0.3fs\" % test_time)\n",
    "\n",
    "\n",
    "# compute the performance measures\n",
    "y_pred = [numpy.argmax(line) for line in y_pred]#printing the predictions\n",
    "\n",
    "score1 = metrics.accuracy_score(y_test, y_pred)\n",
    "print(\"accuracy:   %0.3f\" % score1)\n",
    "\n",
    "print(metrics.classification_report(y_test, y_pred, target_names=[str(x) for x in range(14)]))\n",
    "\n",
    "print(\"confusion matrix:\")\n",
    "print(metrics.confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print('------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7500618e",
   "metadata": {},
   "source": [
    "## half of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0388801",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to extract features from training data : 6.953000 seconds\n",
      "n_samples: 280000, n_features: 445084\n",
      "Time taken to extract features from test data : 1.486998 seconds\n",
      "n_samples: 70000, n_features: 445084\n",
      "train time: 429.631s\n",
      "test time:  2.819s\n",
      "accuracy:   0.949\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.89      0.90      5000\n",
      "           1       0.95      0.97      0.96      5000\n",
      "           2       0.85      0.86      0.85      5000\n",
      "           3       0.97      0.97      0.97      5000\n",
      "           4       0.96      0.93      0.95      5000\n",
      "           5       0.95      0.95      0.95      5000\n",
      "           6       0.95      0.92      0.94      5000\n",
      "           7       0.97      0.97      0.97      5000\n",
      "           8       0.99      0.99      0.99      5000\n",
      "           9       0.95      0.97      0.96      5000\n",
      "          10       0.98      0.94      0.96      5000\n",
      "          11       0.96      0.98      0.97      5000\n",
      "          12       0.97      0.97      0.97      5000\n",
      "          13       0.93      0.94      0.94      5000\n",
      "\n",
      "    accuracy                           0.95     70000\n",
      "   macro avg       0.95      0.95      0.95     70000\n",
      "weighted avg       0.95      0.95      0.95     70000\n",
      "\n",
      "confusion matrix:\n",
      "[[4473   60   94   34   24   85   78   13    6   10    6   28   30   59]\n",
      " [  34 4864   20    6   10   11   40    3    2    0    1    0    5    4]\n",
      " [  56   49 4311   45   79   12   18    6    0    9    1  134   60  220]\n",
      " [   6    4  108 4837   17   12    3    4    0    4    0    0    3    2]\n",
      " [  37   49  163   26 4672   16    8    5    2    3    1    1    4   13]\n",
      " [ 105    0   58    7   16 4761   24   13    0    2    3    0    4    7]\n",
      " [  86   57   41    1   30   64 4621   58   19    5    1    2    7    8]\n",
      " [   8    1   26    0    8    7   46 4863   24   10    6    0    0    1]\n",
      " [   2    5    7    0    1    1   16   12 4954    1    0    0    0    1]\n",
      " [   5    0   35    0    1    6    1   15    0 4838   97    0    1    1]\n",
      " [  11    2   26    0    0    6    0    7    0  224 4722    0    0    2]\n",
      " [   4    0   57    3    1    3    0    0    0    1    0 4897   21   13]\n",
      " [  12    3   35    7    2    8    0    2    1    1    0   21 4874   34]\n",
      " [  51   11  111   13   17    9    8    1    1    6    2   11   37 4722]]\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "y_train, _, x_train = pickle.load(open(\"dbpedia_csv/\" + \"/train_preprocessed.pickle\", \"rb\"))\n",
    "#_, _, x_valid = pickle.load(open(paths.data + \"/validation_preprocessed.pickle\", \"rb\"))\n",
    "y_test, _, x_test = pickle.load(open(\"dbpedia_csv/\" + \"/test_preprocessed.pickle\", \"rb\"))\n",
    "\n",
    "y_train = y_train[slice(0, len(y_train), 2)]\n",
    "x_train = x_train[slice(0, len(x_train), 2)]\n",
    "\n",
    "y_train = [y-1 for y in y_train]\n",
    "y_test = [y-1 for y in y_test]\n",
    "\n",
    "t = time()  # not compulsory\n",
    "\n",
    "# loading CountVectorizer\n",
    "tf_vectorizer = TfidfVectorizer() # or term frequency\n",
    "\n",
    "x_train = tf_vectorizer.fit_transform(x_train)\n",
    "\n",
    "duration = time() - t\n",
    "print(\"Time taken to extract features from training data : %f seconds\" % (duration))\n",
    "print(\"n_samples: %d, n_features: %d\" % x_train.shape)\n",
    "\n",
    "t = time()\n",
    "x_test = tf_vectorizer.transform(x_test)\n",
    "\n",
    "duration = time() - t\n",
    "print(\"Time taken to extract features from test data : %f seconds\" % (duration))\n",
    "print(\"n_samples: %d, n_features: %d\" % x_test.shape)\n",
    "\n",
    "\n",
    "\n",
    "d_train=lgb.Dataset(x_train, label=y_train)\n",
    "\n",
    "\n",
    "import gc\n",
    "import sys\n",
    "del x_train\n",
    "del tf_vectorizer\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "params={}\n",
    "params['learning_rate']=0.03\n",
    "params['boosting_type']='gbdt' #GradientBoostingDecisionTree\n",
    "params['objective']='multiclass' #Multi-class target feature\n",
    "params['metric']='multi_logloss' #metric for multi-class\n",
    "params['max_depth']=10\n",
    "params['num_leaves'] = 800\n",
    "params['verbose'] = -1\n",
    "params['num_class']= 14 #no.of unique values in the target class not inclusive of the end value\n",
    "params['device'] = 'gpu'\n",
    "t = time()\n",
    "clf=lgb.train(params,d_train,100)\n",
    "training_time = time() - t\n",
    "print(\"train time: %0.3fs\" % training_time)\n",
    "\n",
    "\n",
    "t = time()\n",
    "y_pred=clf.predict(x_test)\n",
    "\n",
    "test_time = time() - t\n",
    "print(\"test time:  %0.3fs\" % test_time)\n",
    "\n",
    "\n",
    "# compute the performance measures\n",
    "y_pred = [numpy.argmax(line) for line in y_pred]#printing the predictions\n",
    "\n",
    "score1 = metrics.accuracy_score(y_test, y_pred)\n",
    "print(\"accuracy:   %0.3f\" % score1)\n",
    "\n",
    "print(metrics.classification_report(y_test, y_pred, target_names=[str(x) for x in range(14)]))\n",
    "\n",
    "print(\"confusion matrix:\")\n",
    "print(metrics.confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print('------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0704883f",
   "metadata": {},
   "source": [
    "### fourth of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c0d98a5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to extract features from training data : 3.361711 seconds\n",
      "n_samples: 140000, n_features: 282474\n",
      "Time taken to extract features from test data : 1.366998 seconds\n",
      "n_samples: 70000, n_features: 282474\n",
      "train time: 230.059s\n",
      "test time:  2.764s\n",
      "accuracy:   0.946\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.89      0.90      5000\n",
      "           1       0.95      0.97      0.96      5000\n",
      "           2       0.84      0.85      0.85      5000\n",
      "           3       0.97      0.96      0.97      5000\n",
      "           4       0.96      0.93      0.95      5000\n",
      "           5       0.95      0.95      0.95      5000\n",
      "           6       0.95      0.92      0.93      5000\n",
      "           7       0.97      0.97      0.97      5000\n",
      "           8       0.99      0.99      0.99      5000\n",
      "           9       0.94      0.96      0.95      5000\n",
      "          10       0.97      0.94      0.96      5000\n",
      "          11       0.96      0.98      0.97      5000\n",
      "          12       0.96      0.97      0.97      5000\n",
      "          13       0.92      0.94      0.93      5000\n",
      "\n",
      "    accuracy                           0.95     70000\n",
      "   macro avg       0.95      0.95      0.95     70000\n",
      "weighted avg       0.95      0.95      0.95     70000\n",
      "\n",
      "confusion matrix:\n",
      "[[4459   61   96   33   23   91   82   12    5   11    6   29   27   65]\n",
      " [  46 4851   21    9    9   10   41    1    1    0    1    0    7    3]\n",
      " [  72   53 4270   52   79   12   16    8    2    6    2  144   62  222]\n",
      " [   6    6  116 4823   16   10    6    4    0    6    1    0    1    5]\n",
      " [  40   50  161   25 4669   17    6    6    3    3    1    1    4   14]\n",
      " [ 117    0   55    5   15 4748   27   13    0    4    2    0    4   10]\n",
      " [  85   66   35    1   29   72 4604   64   19    6    1    2    8    8]\n",
      " [  14    1   29    0    8    9   39 4864   22   11    2    0    0    1]\n",
      " [   2    5    7    0    1    1   17   13 4952    1    0    0    0    1]\n",
      " [   5    0   39    2    1    7    1   17    0 4803  122    0    1    2]\n",
      " [  11    1   26    0    0    6    1    8    0  238 4705    0    1    3]\n",
      " [   5    0   63    3    0    3    0    0    0    1    0 4885   27   13]\n",
      " [  11    6   35    9    3   10    1    2    1    2    0   23 4862   35]\n",
      " [  53   12  127   11   19    9    8    1    1    5    3   11   46 4694]]\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "y_train, _, x_train = pickle.load(open(\"dbpedia_csv/\" + \"/train_preprocessed.pickle\", \"rb\"))\n",
    "#_, _, x_valid = pickle.load(open(paths.data + \"/validation_preprocessed.pickle\", \"rb\"))\n",
    "y_test, _, x_test = pickle.load(open(\"dbpedia_csv/\" + \"/test_preprocessed.pickle\", \"rb\"))\n",
    "\n",
    "\n",
    "y_train = y_train[slice(0, len(y_train), 4)]\n",
    "x_train = x_train[slice(0, len(x_train), 4)]\n",
    "\n",
    "\n",
    "y_train = [y-1 for y in y_train]\n",
    "y_test = [y-1 for y in y_test]\n",
    "\n",
    "t = time()  # not compulsory\n",
    "\n",
    "# loading CountVectorizer\n",
    "tf_vectorizer = TfidfVectorizer() # or term frequency\n",
    "\n",
    "x_train = tf_vectorizer.fit_transform(x_train)\n",
    "\n",
    "duration = time() - t\n",
    "print(\"Time taken to extract features from training data : %f seconds\" % (duration))\n",
    "print(\"n_samples: %d, n_features: %d\" % x_train.shape)\n",
    "\n",
    "t = time()\n",
    "x_test = tf_vectorizer.transform(x_test)\n",
    "\n",
    "duration = time() - t\n",
    "print(\"Time taken to extract features from test data : %f seconds\" % (duration))\n",
    "print(\"n_samples: %d, n_features: %d\" % x_test.shape)\n",
    "\n",
    "\n",
    "\n",
    "d_train=lgb.Dataset(x_train, label=y_train)\n",
    "\n",
    "\n",
    "import gc\n",
    "import sys\n",
    "del x_train\n",
    "del tf_vectorizer\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "params={}\n",
    "params['learning_rate']=0.03\n",
    "params['boosting_type']='gbdt' #GradientBoostingDecisionTree\n",
    "params['objective']='multiclass' #Multi-class target feature\n",
    "params['metric']='multi_logloss' #metric for multi-class\n",
    "params['max_depth']=10\n",
    "params['num_leaves'] = 800\n",
    "params['verbose'] = -1\n",
    "params['num_class']= 14 #no.of unique values in the target class not inclusive of the end value\n",
    "params['device'] = 'gpu'\n",
    "t = time()\n",
    "clf=lgb.train(params,d_train,100)\n",
    "training_time = time() - t\n",
    "print(\"train time: %0.3fs\" % training_time)\n",
    "\n",
    "\n",
    "t = time()\n",
    "y_pred=clf.predict(x_test)\n",
    "\n",
    "test_time = time() - t\n",
    "print(\"test time:  %0.3fs\" % test_time)\n",
    "\n",
    "\n",
    "# compute the performance measures\n",
    "y_pred = [numpy.argmax(line) for line in y_pred]#printing the predictions\n",
    "\n",
    "score1 = metrics.accuracy_score(y_test, y_pred)\n",
    "print(\"accuracy:   %0.3f\" % score1)\n",
    "\n",
    "print(metrics.classification_report(y_test, y_pred, target_names=[str(x) for x in range(14)]))\n",
    "\n",
    "print(\"confusion matrix:\")\n",
    "print(metrics.confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print('------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192f1968",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
