{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da7b11f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "import pandas\n",
    "import pickle\n",
    "import numpy\n",
    "\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "from time import time\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759da62c",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba227a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring = ['accuracy', 'precision', 'recall', 'f1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f72dd71",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train, _, x_train = pickle.load(open(\"dbpedia_csv/\" + \"/train_preprocessed.pickle\", \"rb\"))\n",
    "#_, _, x_valid = pickle.load(open(paths.data + \"/validation_preprocessed.pickle\", \"rb\"))\n",
    "y_test, _, x_test = pickle.load(open(\"dbpedia_csv/\" + \"/test_preprocessed.pickle\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3bb09b04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to extract features from training data : 23.517927 seconds\n",
      "n_samples: 560000, n_features: 698699\n"
     ]
    }
   ],
   "source": [
    "t = time()  # not compulsory\n",
    "\n",
    "# loading CountVectorizer\n",
    "tf_vectorizer = TfidfVectorizer() # or term frequency\n",
    "\n",
    "x_train = tf_vectorizer.fit_transform(x_train)\n",
    "\n",
    "duration = time() - t\n",
    "print(\"Time taken to extract features from training data : %f seconds\" % (duration))\n",
    "print(\"n_samples: %d, n_features: %d\" % x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "53008fb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to extract features from test data : 2.694394 seconds\n",
      "n_samples: 70000, n_features: 698699\n"
     ]
    }
   ],
   "source": [
    "t = time()\n",
    "x_test = tf_vectorizer.transform(x_test)\n",
    "\n",
    "duration = time() - t\n",
    "print(\"Time taken to extract features from test data : %f seconds\" % (duration))\n",
    "print(\"n_samples: %d, n_features: %d\" % x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "459a7914",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "640"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import gc \n",
    "del X_train_tf\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "33ffdecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train time: 1.146s\n"
     ]
    }
   ],
   "source": [
    "t = time()\n",
    "\n",
    "naive_bayes_classifier = MultinomialNB()\n",
    "# naive_bayes_classifier.fit(X_train_tf, y_train)\n",
    "\n",
    "naive_bayes_classifier.fit(x_train, y_train)\n",
    "\n",
    "training_time = time() - t\n",
    "print(\"train time: %0.3fs\" % training_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a1e12462",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test time:  0.204s\n",
      "accuracy:   0.947\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.86      0.89      5000\n",
      "           1       0.94      0.97      0.96      5000\n",
      "           2       0.94      0.77      0.85      5000\n",
      "           3       0.98      0.99      0.99      5000\n",
      "           4       0.96      0.97      0.96      5000\n",
      "           5       0.96      0.99      0.97      5000\n",
      "           6       0.94      0.95      0.95      5000\n",
      "           7       0.98      0.98      0.98      5000\n",
      "           8       1.00      0.95      0.97      5000\n",
      "           9       1.00      0.98      0.99      5000\n",
      "          10       0.99      0.99      0.99      5000\n",
      "          11       0.88      0.99      0.93      5000\n",
      "          12       0.93      0.96      0.95      5000\n",
      "          13       0.86      0.92      0.89      5000\n",
      "\n",
      "    accuracy                           0.95     70000\n",
      "   macro avg       0.95      0.95      0.95     70000\n",
      "weighted avg       0.95      0.95      0.95     70000\n",
      "\n",
      "confusion matrix:\n",
      "[[4276   56   47   12   19  142   58    3    0    0    0  190   42  155]\n",
      " [  47 4862   12    3   22    5   34    1    3    0    0    0    1   10]\n",
      " [  46   20 3850   12   92    2   18    1    1    1    2  421  116  418]\n",
      " [   2    0   24 4939   14    8    3    0    0    0    0    2    7    1]\n",
      " [  23   25   42   15 4831   16   18    0    0    0    0    1    9   20]\n",
      " [  39    1    1    0    1 4942    9    1    0    0    0    0    2    4]\n",
      " [  64  100   10    2   16   21 4748   25    2    1    0    1    1    9]\n",
      " [   6    6    0    0    4    8   74 4886    7    2    3    1    0    3]\n",
      " [   4   53    2    0   10    1   73   86 4768    0    0    0    0    3]\n",
      " [   6    0    6   25    1    1    3    7    0 4885   57    1    3    5]\n",
      " [  14    1    4    0    0    1    4    1    0   16 4957    0    1    1]\n",
      " [   3    0   28    1    0    0    0    0    0    0    0 4940   21    7]\n",
      " [  10    1   24    7    4    8    1    0    0    0    0   46 4794  105]\n",
      " [  97   22   57    9   23    3    5    0    0    1    3   25  148 4607]]\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "t = time()\n",
    "y_pred = naive_bayes_classifier.predict(x_test)\n",
    "\n",
    "test_time = time() - t\n",
    "print(\"test time:  %0.3fs\" % test_time)\n",
    "\n",
    "# compute the performance measures\n",
    "score1 = metrics.accuracy_score(y_test, y_pred)\n",
    "print(\"accuracy:   %0.3f\" % score1)\n",
    "\n",
    "print(metrics.classification_report(y_test, y_pred, target_names=[str(x) for x in range(14)]))\n",
    "\n",
    "print(\"confusion matrix:\")\n",
    "print(metrics.confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print('------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335b3c82",
   "metadata": {},
   "source": [
    "### half of the dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "19289b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train, _, x_train = pickle.load(open(\"dbpedia_csv/\" + \"/train_preprocessed.pickle\", \"rb\"))\n",
    "#_, _, x_valid = pickle.load(open(paths.data + \"/validation_preprocessed.pickle\", \"rb\"))\n",
    "y_test, _, x_test = pickle.load(open(\"dbpedia_csv/\" + \"/test_preprocessed.pickle\", \"rb\"))\n",
    "\n",
    "y_train = y_train[slice(0, len(y_train), 2)]\n",
    "x_train = x_train[slice(0, len(x_train), 2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "db1c3723",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to extract features from training data : 12.984325 seconds\n",
      "n_samples: 280000, n_features: 445084\n",
      "Time taken to extract features from test data : 3.610619 seconds\n",
      "n_samples: 70000, n_features: 445084\n"
     ]
    }
   ],
   "source": [
    "t = time()  # not compulsory\n",
    "\n",
    "# loading CountVectorizer\n",
    "tf_vectorizer = TfidfVectorizer() # or term frequency\n",
    "\n",
    "x_train = tf_vectorizer.fit_transform(x_train)\n",
    "\n",
    "duration = time() - t\n",
    "print(\"Time taken to extract features from training data : %f seconds\" % (duration))\n",
    "print(\"n_samples: %d, n_features: %d\" % x_train.shape)\n",
    "\n",
    "t = time()\n",
    "x_test = tf_vectorizer.transform(x_test)\n",
    "\n",
    "duration = time() - t\n",
    "print(\"Time taken to extract features from test data : %f seconds\" % (duration))\n",
    "print(\"n_samples: %d, n_features: %d\" % x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9c3c2556",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train time: 0.692s\n"
     ]
    }
   ],
   "source": [
    "t = time()\n",
    "\n",
    "naive_bayes_classifier = MultinomialNB()\n",
    "# naive_bayes_classifier.fit(X_train_tf, y_train)\n",
    "\n",
    "naive_bayes_classifier.fit(x_train, y_train)\n",
    "\n",
    "training_time = time() - t\n",
    "print(\"train time: %0.3fs\" % training_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c37a1651",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test time:  0.256s\n",
      "accuracy:   0.947\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.86      0.89      5000\n",
      "           1       0.94      0.97      0.96      5000\n",
      "           2       0.94      0.78      0.85      5000\n",
      "           3       0.98      0.99      0.98      5000\n",
      "           4       0.96      0.97      0.96      5000\n",
      "           5       0.96      0.99      0.97      5000\n",
      "           6       0.94      0.95      0.95      5000\n",
      "           7       0.98      0.98      0.98      5000\n",
      "           8       1.00      0.96      0.98      5000\n",
      "           9       1.00      0.97      0.98      5000\n",
      "          10       0.98      0.99      0.99      5000\n",
      "          11       0.88      0.99      0.93      5000\n",
      "          12       0.94      0.96      0.95      5000\n",
      "          13       0.86      0.92      0.89      5000\n",
      "\n",
      "    accuracy                           0.95     70000\n",
      "   macro avg       0.95      0.95      0.95     70000\n",
      "weighted avg       0.95      0.95      0.95     70000\n",
      "\n",
      "confusion matrix:\n",
      "[[4278   59   49   12   18  148   58    5    0    0    0  180   37  156]\n",
      " [  46 4871   12    5   16    6   32    2    2    0    0    0    0    8]\n",
      " [  43   19 3886   13  102    2   19    1    1    1    2  404  101  406]\n",
      " [   2    0   26 4937   13    8    4    0    0    0    0    1    8    1]\n",
      " [  20   24   36   16 4844   17   17    0    0    0    0    1    9   16]\n",
      " [  39    2    1    0    0 4938   14    2    0    0    0    0    2    2]\n",
      " [  62  106    8    1   17   19 4748   25    2    1    0    2    1    8]\n",
      " [   6    6    0    0    3    9   61 4903    6    1    2    1    0    2]\n",
      " [   4   45    2    1   10    0   77   80 4778    0    0    0    0    3]\n",
      " [   4    0    5   33    2    1    3    8    0 4842   90    1    4    7]\n",
      " [  15    2    3    0    0    1    5    0    0   17 4954    0    1    2]\n",
      " [   3    0   27    1    0    0    0    0    0    0    0 4946   19    4]\n",
      " [  10    1   24    7    5   10    1    0    0    0    0   46 4791  105]\n",
      " [  94   26   57    8   27    3    5    0    0    1    2   27  144 4606]]\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "t = time()\n",
    "y_pred = naive_bayes_classifier.predict(x_test)\n",
    "\n",
    "test_time = time() - t\n",
    "print(\"test time:  %0.3fs\" % test_time)\n",
    "\n",
    "# compute the performance measures\n",
    "score1 = metrics.accuracy_score(y_test, y_pred)\n",
    "print(\"accuracy:   %0.3f\" % score1)\n",
    "\n",
    "print(metrics.classification_report(y_test, y_pred, target_names=[str(x) for x in range(14)]))\n",
    "\n",
    "print(\"confusion matrix:\")\n",
    "print(metrics.confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print('------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684aa827",
   "metadata": {},
   "source": [
    "### one fourth of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fde04a66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to extract features from training data : 5.715026 seconds\n",
      "n_samples: 140000, n_features: 282474\n",
      "Time taken to extract features from test data : 2.271709 seconds\n",
      "n_samples: 70000, n_features: 282474\n",
      "train time: 0.285s\n",
      "test time:  0.061s\n",
      "accuracy:   0.947\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.86      0.89      5000\n",
      "           1       0.94      0.98      0.96      5000\n",
      "           2       0.94      0.77      0.85      5000\n",
      "           3       0.98      0.99      0.98      5000\n",
      "           4       0.96      0.97      0.96      5000\n",
      "           5       0.96      0.99      0.97      5000\n",
      "           6       0.94      0.95      0.95      5000\n",
      "           7       0.97      0.98      0.98      5000\n",
      "           8       1.00      0.96      0.98      5000\n",
      "           9       1.00      0.96      0.98      5000\n",
      "          10       0.97      0.99      0.98      5000\n",
      "          11       0.88      0.99      0.93      5000\n",
      "          12       0.94      0.96      0.95      5000\n",
      "          13       0.87      0.92      0.89      5000\n",
      "\n",
      "    accuracy                           0.95     70000\n",
      "   macro avg       0.95      0.95      0.95     70000\n",
      "weighted avg       0.95      0.95      0.95     70000\n",
      "\n",
      "confusion matrix:\n",
      "[[4277   62   54   14   19  143   57    5    0    0    0  181   38  150]\n",
      " [  39 4883   10    6   13    6   33    1    1    0    0    0    0    8]\n",
      " [  45   22 3860   15  106    2   22    1    1    1    1  412  106  406]\n",
      " [   3    1   30 4938   11    7    4    0    0    0    0    0    5    1]\n",
      " [  22   30   35   19 4837   16   19    0    0    0    0    1    7   14]\n",
      " [  42    2    2    1    0 4932   14    2    0    0    0    1    2    2]\n",
      " [  57  103   10    2   18   19 4754   28    2    1    0    1    1    4]\n",
      " [   6    6    1    0    3    9   58 4906    6    1    2    1    0    1]\n",
      " [   4   37    2    0    9    0   72   83 4791    0    0    0    0    2]\n",
      " [   4    0    5   37    2    2    3   10    0 4803  122    2    3    7]\n",
      " [  15    2    3    0    0    1    5    1    0   19 4950    0    1    3]\n",
      " [   3    0   26    1    0    0    1    0    0    0    0 4947   17    5]\n",
      " [  10    1   21    8    4   10    1    0    0    1    0   46 4799   99]\n",
      " [ 100   23   58   10   28    4    7    0    0    1    2   23  141 4603]]\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "y_train, _, x_train = pickle.load(open(\"dbpedia_csv/\" + \"/train_preprocessed.pickle\", \"rb\"))\n",
    "#_, _, x_valid = pickle.load(open(paths.data + \"/validation_preprocessed.pickle\", \"rb\"))\n",
    "y_test, _, x_test = pickle.load(open(\"dbpedia_csv/\" + \"/test_preprocessed.pickle\", \"rb\"))\n",
    "\n",
    "y_train = y_train[slice(0, len(y_train), 4)]\n",
    "x_train = x_train[slice(0, len(x_train), 4)]\n",
    "\n",
    "\n",
    "## TFIDF VECTORIZATION\n",
    "\n",
    "t = time()  # not compulsory\n",
    "\n",
    "# loading CountVectorizer\n",
    "tf_vectorizer = TfidfVectorizer() # or term frequency\n",
    "\n",
    "x_train = tf_vectorizer.fit_transform(x_train)\n",
    "\n",
    "duration = time() - t\n",
    "print(\"Time taken to extract features from training data : %f seconds\" % (duration))\n",
    "print(\"n_samples: %d, n_features: %d\" % x_train.shape)\n",
    "\n",
    "t = time()\n",
    "x_test = tf_vectorizer.transform(x_test)\n",
    "\n",
    "duration = time() - t\n",
    "print(\"Time taken to extract features from test data : %f seconds\" % (duration))\n",
    "print(\"n_samples: %d, n_features: %d\" % x_test.shape)\n",
    "\n",
    "\n",
    "## TRAIN AND FIT CLASSIFIER\n",
    "\n",
    "t = time()\n",
    "\n",
    "naive_bayes_classifier = MultinomialNB()\n",
    "# naive_bayes_classifier.fit(X_train_tf, y_train)\n",
    "\n",
    "naive_bayes_classifier.fit(x_train, y_train)\n",
    "\n",
    "training_time = time() - t\n",
    "print(\"train time: %0.3fs\" % training_time)\n",
    "\n",
    "\n",
    "t = time()\n",
    "y_pred = naive_bayes_classifier.predict(x_test)\n",
    "\n",
    "test_time = time() - t\n",
    "print(\"test time:  %0.3fs\" % test_time)\n",
    "\n",
    "# compute the performance measures\n",
    "score1 = metrics.accuracy_score(y_test, y_pred)\n",
    "print(\"accuracy:   %0.3f\" % score1)\n",
    "\n",
    "print(metrics.classification_report(y_test, y_pred, target_names=[str(x) for x in range(14)]))\n",
    "\n",
    "print(\"confusion matrix:\")\n",
    "print(metrics.confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print('------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff507dd",
   "metadata": {},
   "source": [
    "## Word2Vec using pretrained model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59fa3701",
   "metadata": {},
   "source": [
    "### full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4978457b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train time: 1.547s\n",
      "test time:  0.249s\n",
      "accuracy:   0.876\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.82      0.79      5000\n",
      "           1       0.93      0.93      0.93      5000\n",
      "           2       0.81      0.72      0.76      5000\n",
      "           3       0.99      0.94      0.96      5000\n",
      "           4       0.95      0.93      0.94      5000\n",
      "           5       0.86      0.93      0.89      5000\n",
      "           6       0.83      0.90      0.86      5000\n",
      "           7       0.97      0.89      0.93      5000\n",
      "           8       0.98      0.96      0.97      5000\n",
      "           9       0.97      0.59      0.73      5000\n",
      "          10       0.76      0.92      0.83      5000\n",
      "          11       0.88      0.97      0.92      5000\n",
      "          12       0.94      0.88      0.91      5000\n",
      "          13       0.73      0.90      0.80      5000\n",
      "\n",
      "    accuracy                           0.88     70000\n",
      "   macro avg       0.88      0.88      0.87     70000\n",
      "weighted avg       0.88      0.88      0.87     70000\n",
      "\n",
      "confusion matrix:\n",
      "[[4080   35  109    9   23  180   84    3    1    0    0  226   40  210]\n",
      " [ 130 4663   26    5   35    9   72    4   40    0    0    0    4   12]\n",
      " [  70   45 3583   10   69    4   25    0    0    0    2  396  126  670]\n",
      " [  16   10  142 4707   24   64    4    0    0    0    0    6    0   27]\n",
      " [  50   87  107   10 4636   31   22    1    8    0    0    1    4   43]\n",
      " [ 323    0    2    1    0 4635   18    2    0    0    0    2    0   17]\n",
      " [ 205  123   30    3   35   46 4485   19   25    0    0    0    3   26]\n",
      " [  28    1    8    0    9   79  388 4440   34    0    0    0    0   13]\n",
      " [   6    7    2    0    8    4  111   33 4819    0    0    0    0   10]\n",
      " [  41    0   65    9    7  249   83   57    0 2929 1427    0    0  133]\n",
      " [ 107    1   15    0    1   28   64   31    0   84 4601    1    1   66]\n",
      " [   8    0   79    1    0    1    1    0    0    0    0 4841   19   50]\n",
      " [   4    5  125    6    3   17    1    1    0    0    0   33 4412  393]\n",
      " [ 208   19  127   10   32   14   21    1    0    0    2   13   69 4484]]\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "y_train, _, x_train = pickle.load(open(\"dbpedia_csv/\" + \"/train_lem_preprocessed_vectorized.pickle\", \"rb\"))\n",
    "#_, _, x_valid = pickle.load(open(paths.data + \"/validation_preprocessed.pickle\", \"rb\"))\n",
    "y_test, _, x_test = pickle.load(open(\"dbpedia_csv/\" + \"/test_lem_preprocessed_vectorized.pickle\", \"rb\"))\n",
    "\n",
    "\n",
    "\n",
    "# Choose either standardization or Normalization\n",
    "# On this data Min Max Normalization is used because we need to fit Naive Bayes\n",
    "\n",
    "# Choose between standardization and MinMAx normalization\n",
    "#PredictorScaler=StandardScaler()\n",
    "PredictorScaler=MinMaxScaler()\n",
    "\n",
    "# Storing the fit object for later reference\n",
    "PredictorScalerFit=PredictorScaler.fit(x_train)\n",
    "\n",
    "\n",
    "x_train = PredictorScalerFit.transform(x_train)\n",
    "x_test = PredictorScalerFit.transform(x_test)\n",
    "\n",
    "\n",
    "## TRAIN AND FIT CLASSIFIER\n",
    "\n",
    "t = time()\n",
    "\n",
    "naive_bayes_classifier = MultinomialNB()\n",
    "# naive_bayes_classifier.fit(X_train_tf, y_train)\n",
    "\n",
    "naive_bayes_classifier.fit(x_train, y_train)\n",
    "\n",
    "training_time = time() - t\n",
    "print(\"train time: %0.3fs\" % training_time)\n",
    "\n",
    "\n",
    "t = time()\n",
    "y_pred = naive_bayes_classifier.predict(x_test)\n",
    "\n",
    "test_time = time() - t\n",
    "print(\"test time:  %0.3fs\" % test_time)\n",
    "\n",
    "# compute the performance measures\n",
    "score1 = metrics.accuracy_score(y_test, y_pred)\n",
    "print(\"accuracy:   %0.3f\" % score1)\n",
    "\n",
    "print(metrics.classification_report(y_test, y_pred, target_names=[str(x) for x in range(14)]))\n",
    "\n",
    "print(\"confusion matrix:\")\n",
    "print(metrics.confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print('------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fcce4dc",
   "metadata": {},
   "source": [
    "### half dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77af6261",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train time: 0.700s\n",
      "test time:  0.257s\n",
      "accuracy:   0.877\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.82      0.79      5000\n",
      "           1       0.93      0.93      0.93      5000\n",
      "           2       0.81      0.71      0.76      5000\n",
      "           3       0.99      0.94      0.96      5000\n",
      "           4       0.95      0.93      0.94      5000\n",
      "           5       0.87      0.93      0.90      5000\n",
      "           6       0.84      0.90      0.87      5000\n",
      "           7       0.97      0.89      0.93      5000\n",
      "           8       0.98      0.96      0.97      5000\n",
      "           9       0.97      0.59      0.74      5000\n",
      "          10       0.77      0.92      0.84      5000\n",
      "          11       0.88      0.97      0.92      5000\n",
      "          12       0.94      0.88      0.91      5000\n",
      "          13       0.73      0.90      0.80      5000\n",
      "\n",
      "    accuracy                           0.88     70000\n",
      "   macro avg       0.89      0.88      0.88     70000\n",
      "weighted avg       0.89      0.88      0.88     70000\n",
      "\n",
      "confusion matrix:\n",
      "[[4080   36  110   10   23  178   80    3    1    0    0  225   41  213]\n",
      " [ 124 4671   24    4   37    9   66    4   42    0    0    0    4   15]\n",
      " [  68   43 3575   10   73    4   24    0    0    0    2  392  125  684]\n",
      " [  17    9  139 4720   23   61    5    0    0    0    0    6    0   20]\n",
      " [  49   87  105   10 4644   31   21    1    8    0    0    1    4   39]\n",
      " [ 319    0    3    1    0 4637   20    2    0    0    0    2    0   16]\n",
      " [ 207  126   28    3   34   46 4481   20   26    0    0    0    3   26]\n",
      " [  32    1   11    0    9   69  367 4463   34    0    0    0    0   14]\n",
      " [   6    8    3    0    8    3  113   33 4816    0    0    0    0   10]\n",
      " [  54    0   66    7    7  246   84   60    0 2965 1379    0    0  132]\n",
      " [ 113    0   16    0    1   25   65   35    0   79 4599    0    1   66]\n",
      " [   8    0   81    1    0    1    1    0    0    0    0 4840   19   49]\n",
      " [   4    5  122    6    3   17    1    1    0    0    0   29 4410  402]\n",
      " [ 205   21  126   11   33   14   22    1    0    0    2   11   69 4485]]\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "y_train, _, x_train = pickle.load(open(\"dbpedia_csv/\" + \"/train_lem_preprocessed_vectorized.pickle\", \"rb\"))\n",
    "#_, _, x_valid = pickle.load(open(paths.data + \"/validation_preprocessed.pickle\", \"rb\"))\n",
    "y_test, _, x_test = pickle.load(open(\"dbpedia_csv/\" + \"/test_lem_preprocessed_vectorized.pickle\", \"rb\"))\n",
    "\n",
    "\n",
    "y_train = y_train[slice(0, len(y_train), 2)]\n",
    "x_train = x_train[slice(0, len(x_train), 2)]\n",
    "\n",
    "\n",
    "# Choose either standardization or Normalization\n",
    "# On this data Min Max Normalization is used because we need to fit Naive Bayes\n",
    "\n",
    "# Choose between standardization and MinMAx normalization\n",
    "#PredictorScaler=StandardScaler()\n",
    "PredictorScaler=MinMaxScaler()\n",
    "\n",
    "# Storing the fit object for later reference\n",
    "PredictorScalerFit=PredictorScaler.fit(x_train)\n",
    "\n",
    "\n",
    "x_train = PredictorScalerFit.transform(x_train)\n",
    "x_test = PredictorScalerFit.transform(x_test)\n",
    "\n",
    "\n",
    "## TRAIN AND FIT CLASSIFIER\n",
    "\n",
    "t = time()\n",
    "\n",
    "naive_bayes_classifier = MultinomialNB()\n",
    "# naive_bayes_classifier.fit(X_train_tf, y_train)\n",
    "\n",
    "naive_bayes_classifier.fit(x_train, y_train)\n",
    "\n",
    "training_time = time() - t\n",
    "print(\"train time: %0.3fs\" % training_time)\n",
    "\n",
    "\n",
    "t = time()\n",
    "y_pred = naive_bayes_classifier.predict(x_test)\n",
    "\n",
    "test_time = time() - t\n",
    "print(\"test time:  %0.3fs\" % test_time)\n",
    "\n",
    "# compute the performance measures\n",
    "score1 = metrics.accuracy_score(y_test, y_pred)\n",
    "print(\"accuracy:   %0.3f\" % score1)\n",
    "\n",
    "print(metrics.classification_report(y_test, y_pred, target_names=[str(x) for x in range(14)]))\n",
    "\n",
    "print(\"confusion matrix:\")\n",
    "print(metrics.confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print('------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f509f0d9",
   "metadata": {},
   "source": [
    "### one fourth of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2c8d020",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train time: 0.374s\n",
      "test time:  0.267s\n",
      "accuracy:   0.877\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.82      0.79      5000\n",
      "           1       0.93      0.93      0.93      5000\n",
      "           2       0.82      0.72      0.76      5000\n",
      "           3       0.99      0.95      0.97      5000\n",
      "           4       0.95      0.93      0.94      5000\n",
      "           5       0.87      0.93      0.90      5000\n",
      "           6       0.83      0.90      0.86      5000\n",
      "           7       0.96      0.89      0.93      5000\n",
      "           8       0.98      0.96      0.97      5000\n",
      "           9       0.97      0.60      0.74      5000\n",
      "          10       0.77      0.92      0.84      5000\n",
      "          11       0.88      0.97      0.92      5000\n",
      "          12       0.94      0.89      0.91      5000\n",
      "          13       0.73      0.90      0.80      5000\n",
      "\n",
      "    accuracy                           0.88     70000\n",
      "   macro avg       0.89      0.88      0.88     70000\n",
      "weighted avg       0.89      0.88      0.88     70000\n",
      "\n",
      "confusion matrix:\n",
      "[[4079   36  108   10   24  182   79    3    1    0    0  227   42  209]\n",
      " [ 121 4672   24    4   36    9   66    4   42    0    0    0    4   18]\n",
      " [  67   41 3582    9   73    6   25    0    0    0    2  391  125  679]\n",
      " [  18    8  130 4727   23   59    4    0    0    0    0    6    0   25]\n",
      " [  49   83  104   12 4649   32   22    1    7    0    0    1    4   36]\n",
      " [ 322    0    3    1    0 4634   20    2    0    0    0    2    0   16]\n",
      " [ 203  122   25    3   37   49 4484   20   26    0    0    0    3   28]\n",
      " [  31    1    8    0   12   70  381 4449   34    0    0    0    0   14]\n",
      " [  14    8    2    0    9    3  111   34 4817    0    0    0    0    2]\n",
      " [  60    0   60   10    9  233   91   60    0 2987 1358    0    0  132]\n",
      " [ 112    0   15    0    2   24   71   36    0   78 4594    0    1   67]\n",
      " [   8    0   87    1    0    1    1    0    0    0    0 4834   19   49]\n",
      " [   5    5  117    6    3   17    1    1    0    0    0   34 4430  381]\n",
      " [ 212   22  116   11   33   12   25    1    0    0    2   13   70 4483]]\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "y_train, _, x_train = pickle.load(open(\"dbpedia_csv/\" + \"/train_lem_preprocessed_vectorized.pickle\", \"rb\"))\n",
    "#_, _, x_valid = pickle.load(open(paths.data + \"/validation_preprocessed.pickle\", \"rb\"))\n",
    "y_test, _, x_test = pickle.load(open(\"dbpedia_csv/\" + \"/test_lem_preprocessed_vectorized.pickle\", \"rb\"))\n",
    "\n",
    "\n",
    "y_train = y_train[slice(0, len(y_train), 4)]\n",
    "x_train = x_train[slice(0, len(x_train), 4)]\n",
    "\n",
    "\n",
    "# Choose either standardization or Normalization\n",
    "# On this data Min Max Normalization is used because we need to fit Naive Bayes\n",
    "\n",
    "# Choose between standardization and MinMAx normalization\n",
    "#PredictorScaler=StandardScaler()\n",
    "PredictorScaler=MinMaxScaler()\n",
    "\n",
    "# Storing the fit object for later reference\n",
    "PredictorScalerFit=PredictorScaler.fit(x_train)\n",
    "\n",
    "\n",
    "x_train = PredictorScalerFit.transform(x_train)\n",
    "x_test = PredictorScalerFit.transform(x_test)\n",
    "\n",
    "\n",
    "## TRAIN AND FIT CLASSIFIER\n",
    "\n",
    "t = time()\n",
    "\n",
    "naive_bayes_classifier = MultinomialNB()\n",
    "# naive_bayes_classifier.fit(X_train_tf, y_train)\n",
    "\n",
    "naive_bayes_classifier.fit(x_train, y_train)\n",
    "\n",
    "training_time = time() - t\n",
    "print(\"train time: %0.3fs\" % training_time)\n",
    "\n",
    "\n",
    "t = time()\n",
    "y_pred = naive_bayes_classifier.predict(x_test)\n",
    "\n",
    "test_time = time() - t\n",
    "print(\"test time:  %0.3fs\" % test_time)\n",
    "\n",
    "# compute the performance measures\n",
    "score1 = metrics.accuracy_score(y_test, y_pred)\n",
    "print(\"accuracy:   %0.3f\" % score1)\n",
    "\n",
    "print(metrics.classification_report(y_test, y_pred, target_names=[str(x) for x in range(14)]))\n",
    "\n",
    "print(\"confusion matrix:\")\n",
    "print(metrics.confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print('------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504575e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
